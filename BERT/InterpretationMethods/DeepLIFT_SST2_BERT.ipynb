{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0379fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, pickle\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '../../Utils')\n",
    "from global_constants import gpu_device\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from BERT_models import BERT_SST2_MODEL\n",
    "\n",
    "from _utils import sample_random_glue_sst2, get_continuation_mapping, \\\n",
    "                    get_continuous_attributions, get_continuous_raw_inputs, \\\n",
    "                    collect_info_for_metric, save_info, download_HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd7ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3079c41d43ba4425a4a2277d3c64f452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3b24abff24d1d8c0.arrow\n",
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5960909ab3834668.arrow\n"
     ]
    }
   ],
   "source": [
    "sst2_data_raw, targets, idxs = sample_random_glue_sst2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cc7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_SST2_MODEL()\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e778996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_list, raw_attr_list, conti_attr_list, raw_input_list = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cece0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from captum.attr import DeepLift\n",
    "from captum.attr import visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9937c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift = DeepLift(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6c1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_record(raw_review, target):\n",
    "    #tokenizer operations\n",
    "    tokenized = tokenizer(raw_review, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokenized['offset_mapping']\n",
    "    conti_map = get_continuation_mapping(offset_mapping)\n",
    "    input_ids = torch.tensor(tokenized['input_ids']).unsqueeze(0).to(gpu_device)\n",
    "    detokenized = [t.replace('#', '') for t in tokenizer.convert_ids_to_tokens(input_ids[0])]\n",
    "    print(f'detokenized: {detokenized}')\n",
    "    \n",
    "    #feeding input forward \n",
    "    input_emb = model.get_embeddings(input_ids)\n",
    "    pred_prob = model(input_emb).item()\n",
    "    \n",
    "    #categorizing results\n",
    "    pred_class = 'Pos' if pred_prob > 0.5 else 'Neg' \n",
    "    true_class = 'Pos' if target > 0.5 else 'Neg' \n",
    "    \n",
    "    #attribution algorithm working\n",
    "    attribution, delta = deep_lift.attribute(input_emb, return_convergence_delta=True)\n",
    "    print(f'attribution.size {attribution.size()}')\n",
    "    word_attributions = attribution.squeeze(0).sum(dim=1)\n",
    "    word_attributions /= torch.norm(word_attributions)\n",
    "    attr_score = torch.sum(word_attributions)\n",
    "    attr_class = 'Pos' if attr_score > 0.5 else 'Neg'\n",
    "    convergence_score = delta\n",
    "    \n",
    "    \n",
    "    #re-organizing tensors and arrays because words get split down\n",
    "    conti_attr = get_continuous_attributions(conti_map, word_attributions)\n",
    "    raw_input = get_continuous_raw_inputs(conti_map, detokenized)\n",
    "    assert (conti_attr.shape[0] == len(raw_input))\n",
    "    print(f'conti_attri_size: {conti_attr.size()}, raw_input_len:  {len(raw_input)}')\n",
    "    \n",
    "    print(f'word attributions {word_attributions}')\n",
    "    print(f'pred_prob {pred_prob}')\n",
    "    print(f'pred_class {pred_class}')\n",
    "    print(f'true_class {true_class}')\n",
    "    print(f'attribution {attribution}')\n",
    "    print(f'attr_class {attr_class}')\n",
    "    print(f'attr_score {attr_score}')\n",
    "    print(f'raw_input {raw_input}')\n",
    "\n",
    "        \n",
    "#     collect info for metrics later\n",
    "    collect_info_for_metric(model_out_list, pred_prob, raw_attr_list, attribution, conti_attr_list, conti_attr, raw_input_list, raw_input)\n",
    "        \n",
    "    \n",
    "    visual_record = visualization.VisualizationDataRecord(word_attributions=conti_attr,\n",
    "                                                         pred_prob=pred_prob,\n",
    "                                                         pred_class=pred_class,\n",
    "                                                         true_class=true_class,\n",
    "                                                         attr_class=attr_class,\n",
    "                                                         attr_score=attr_score,\n",
    "                                                         raw_input_ids=raw_input,\n",
    "                                                         convergence_score=convergence_score)\n",
    "        \n",
    "        \n",
    "    return visual_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6941070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review: its oscar nomination \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'its', 'oscar', 'nomination', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([-0.0169,  0.9381,  0.0499, -0.0706, -0.3349], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0169, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.9381, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0499, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0706, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3349, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'its', 'oscar', 'nomination', '[SEP]']\n",
      "len conti_raw 5\n",
      "conti_raw ['[CLS]', 'its', 'oscar', 'nomination', '[SEP]']\n",
      "conti_attri_size: torch.Size([5]), raw_input_len:  5\n",
      "word attributions tensor([-0.0169,  0.9381,  0.0499, -0.0706, -0.3349], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9578894972801208\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[ 5.6118e-05,  2.9305e-04,  9.7753e-05,  ...,  2.9469e-06,\n",
      "           1.9243e-06,  1.2387e-04],\n",
      "         [-4.7869e-05, -1.5525e-05, -5.5945e-05,  ...,  1.0678e-04,\n",
      "          -2.7966e-04,  7.9963e-05],\n",
      "         [-7.4345e-05, -1.2982e-05,  5.1472e-05,  ...,  1.2174e-04,\n",
      "           2.2254e-05, -6.7197e-04],\n",
      "         [-1.2129e-04, -9.9070e-05, -2.7507e-04,  ...,  4.9241e-06,\n",
      "           9.1832e-05,  6.7943e-05],\n",
      "         [ 3.6586e-05,  3.4733e-05,  1.5339e-05,  ...,  2.1184e-06,\n",
      "          -2.2236e-05, -1.2717e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Pos\n",
      "attr_score 0.5657088756561279\n",
      "raw_input ['[CLS]', 'its', 'oscar', 'nomination', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/metric/lib/python3.9/site-packages/captum/attr/_core/deep_lift.py:336: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.96)</b></text></td><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.57</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 54%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> oscar                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nomination                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: shenanigans and slapstick \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'shen', 'ani', 'gan', 's', 'and', 'slap', 'stick', '[SEP]']\n",
      "attribution.size torch.Size([1, 9, 768])\n",
      "word attr tensor([ 0.2709, -0.3590,  0.3535,  0.1895, -0.0343, -0.3118, -0.5368, -0.3094,\n",
      "         0.3932], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2709, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0296, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.3118, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4231, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.3932, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'shen', 'ani', 'gan', 's', 'and', 'slap', 'stick', '[SEP]']\n",
      "len conti_raw 5\n",
      "conti_raw ['[CLS]', 'shenanigans', 'and', 'slapstick', '[SEP]']\n",
      "conti_attri_size: torch.Size([5]), raw_input_len:  5\n",
      "word attributions tensor([ 0.2709, -0.3590,  0.3535,  0.1895, -0.0343, -0.3118, -0.5368, -0.3094,\n",
      "         0.3932], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.02607199177145958\n",
      "pred_class Neg\n",
      "true_class Pos\n",
      "attribution tensor([[[-2.2240e-05,  2.0466e-05, -1.5911e-05,  ...,  1.2393e-06,\n",
      "          -7.1767e-06,  2.2254e-05],\n",
      "         [ 3.2862e-04,  1.0194e-05, -1.1568e-04,  ..., -4.8323e-05,\n",
      "          -3.8453e-05,  3.3143e-05],\n",
      "         [ 1.5021e-05, -4.2352e-05,  4.4267e-07,  ..., -1.2696e-05,\n",
      "          -9.0889e-05,  1.5951e-05],\n",
      "         ...,\n",
      "         [-8.4689e-05,  8.7179e-05, -1.0657e-06,  ..., -4.5986e-05,\n",
      "           3.8754e-05, -7.3308e-05],\n",
      "         [ 2.5701e-05,  5.6159e-04,  3.1380e-05,  ..., -4.4703e-05,\n",
      "           5.0181e-05, -1.2103e-04],\n",
      "         [ 9.5672e-06,  3.0635e-06, -1.3325e-06,  ..., -2.0004e-05,\n",
      "          -2.1287e-05, -7.0118e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.344232439994812\n",
      "raw_input ['[CLS]', 'shenanigans', 'and', 'slapstick', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.03)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.34</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> shenanigans                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> slapstick                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: an unsettling sight , \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'an', 'un', 'sett', 'ling', 'sight', ',', '[SEP]']\n",
      "attribution.size torch.Size([1, 8, 768])\n",
      "word attr tensor([ 0.1475, -0.2139, -0.6607, -0.1604,  0.5041,  0.4351, -0.0191, -0.1624],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1475, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2139, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.4351, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0191, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1624, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'an', 'un', 'sett', 'ling', 'sight', ',', '[SEP]']\n",
      "len conti_raw 6\n",
      "conti_raw ['[CLS]', 'an', 'unsettling', 'sight', ',', '[SEP]']\n",
      "conti_attri_size: torch.Size([6]), raw_input_len:  6\n",
      "word attributions tensor([ 0.1475, -0.2139, -0.6607, -0.1604,  0.5041,  0.4351, -0.0191, -0.1624],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9280797243118286\n",
      "pred_class Pos\n",
      "true_class Neg\n",
      "attribution tensor([[[-8.4471e-04, -1.3056e-03,  2.1097e-03,  ...,  8.1015e-06,\n",
      "           1.1388e-04, -1.2068e-03],\n",
      "         [-1.4752e-04,  4.7157e-04,  1.4792e-03,  ...,  1.4924e-04,\n",
      "          -2.6515e-05, -9.7217e-05],\n",
      "         [-5.9664e-03,  1.8475e-04, -1.1587e-03,  ...,  4.5340e-04,\n",
      "           6.3575e-04, -3.3946e-03],\n",
      "         ...,\n",
      "         [ 3.9218e-04,  5.4181e-03,  7.5541e-04,  ...,  1.6092e-03,\n",
      "           2.4072e-03,  1.9684e-03],\n",
      "         [-9.4711e-04, -2.0052e-04,  7.9903e-04,  ..., -1.5353e-04,\n",
      "          -2.1151e-04, -3.7344e-04],\n",
      "         [-7.0907e-04,  3.5432e-04,  3.9687e-05,  ...,  1.6068e-04,\n",
      "           4.8841e-04,  1.8676e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.1298295259475708\n",
      "raw_input ['[CLS]', 'an', 'unsettling', 'sight', ',', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.93)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.13</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> unsettling                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sight                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: the climactic hourlong cricket match \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'the', 'cl', 'ima', 'ctic', 'hour', 'long', 'cricket', 'match', '[SEP]']\n",
      "attribution.size torch.Size([1, 10, 768])\n",
      "word attr tensor([ 0.0127,  0.1172, -0.1191, -0.5673, -0.7051,  0.2658,  0.0901,  0.2001,\n",
      "         0.1508,  0.1066], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0127, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1172, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.5242, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1779, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2001, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1508, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1066, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'the', 'cl', 'ima', 'ctic', 'hour', 'long', 'cricket', 'match', '[SEP]']\n",
      "len conti_raw 7\n",
      "conti_raw ['[CLS]', 'the', 'climactic', 'hourlong', 'cricket', 'match', '[SEP]']\n",
      "conti_attri_size: torch.Size([7]), raw_input_len:  7\n",
      "word attributions tensor([ 0.0127,  0.1172, -0.1191, -0.5673, -0.7051,  0.2658,  0.0901,  0.2001,\n",
      "         0.1508,  0.1066], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.6026754379272461\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-2.7535e-03, -3.6461e-03,  3.6146e-03,  ...,  6.4074e-04,\n",
      "          -5.1886e-04, -3.7664e-04],\n",
      "         [ 9.2471e-04,  2.5147e-03,  1.2014e-04,  ...,  1.3683e-04,\n",
      "           2.0381e-03,  2.9042e-03],\n",
      "         [-2.2307e-03, -1.1027e-03,  3.9112e-04,  ...,  2.0170e-03,\n",
      "           3.3976e-03, -5.1761e-04],\n",
      "         ...,\n",
      "         [-3.9418e-03, -4.1627e-04,  6.5526e-03,  ...,  2.2180e-04,\n",
      "          -9.0398e-04,  3.1244e-05],\n",
      "         [-4.0247e-03, -9.3806e-04, -1.0850e-03,  ..., -9.0199e-04,\n",
      "          -5.1297e-06, -3.0710e-03],\n",
      "         [ 5.7123e-05,  6.6015e-05,  5.2154e-04,  ..., -1.8634e-04,\n",
      "           3.5000e-04, -2.6765e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.4483766555786133\n",
      "raw_input ['[CLS]', 'the', 'climactic', 'hourlong', 'cricket', 'match', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.60)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.45</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> climactic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hourlong                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cricket                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> match                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: alternating between facetious comic parody and pulp melodrama , this smart-aleck movie ... tosses around some intriguing questions about the difference between human and android life \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'alternating', 'between', 'face', 'tious', 'comic', 'parody', 'and', 'pulp', 'mel', 'od', 'rama', ',', 'this', 'smart', '-', 'alec', 'k', 'movie', '.', '.', '.', 'toss', 'es', 'around', 'some', 'intriguing', 'questions', 'about', 'the', 'difference', 'between', 'human', 'and', 'android', 'life', '[SEP]']\n",
      "attribution.size torch.Size([1, 37, 768])\n",
      "word attr tensor([ 0.1010,  0.0262, -0.1032, -0.1007, -0.2120,  0.3789, -0.1731,  0.1659,\n",
      "         0.0044,  0.0729, -0.0268,  0.0270,  0.0562,  0.0447, -0.1489, -0.1333,\n",
      "         0.0205,  0.0685,  0.1400,  0.0282,  0.0212, -0.0973,  0.1020, -0.6188,\n",
      "         0.3163, -0.0946,  0.2628, -0.0248,  0.0237,  0.0427, -0.0866,  0.0038,\n",
      "        -0.1079,  0.0833,  0.0182,  0.1446,  0.1154], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1010, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0262, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1032, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1563, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.3789, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1731, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1659, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0044, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0250, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0562, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0447, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0041, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1400, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0363, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.2584, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.3163, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0946, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2628, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0248, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0237, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0427, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0866, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0038, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1079, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0833, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0182, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1446, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1154, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'alternating', 'between', 'face', 'tious', 'comic', 'parody', 'and', 'pulp', 'mel', 'od', 'rama', ',', 'this', 'smart', '-', 'alec', 'k', 'movie', '.', '.', '.', 'toss', 'es', 'around', 'some', 'intriguing', 'questions', 'about', 'the', 'difference', 'between', 'human', 'and', 'android', 'life', '[SEP]']\n",
      "len conti_raw 28\n",
      "conti_raw ['[CLS]', 'alternating', 'between', 'facetious', 'comic', 'parody', 'and', 'pulp', 'melodrama', ',', 'this', 'smart-aleck', 'movie', '...', 'tosses', 'around', 'some', 'intriguing', 'questions', 'about', 'the', 'difference', 'between', 'human', 'and', 'android', 'life', '[SEP]']\n",
      "conti_attri_size: torch.Size([28]), raw_input_len:  28\n",
      "word attributions tensor([ 0.1010,  0.0262, -0.1032, -0.1007, -0.2120,  0.3789, -0.1731,  0.1659,\n",
      "         0.0044,  0.0729, -0.0268,  0.0270,  0.0562,  0.0447, -0.1489, -0.1333,\n",
      "         0.0205,  0.0685,  0.1400,  0.0282,  0.0212, -0.0973,  0.1020, -0.6188,\n",
      "         0.3163, -0.0946,  0.2628, -0.0248,  0.0237,  0.0427, -0.0866,  0.0038,\n",
      "        -0.1079,  0.0833,  0.0182,  0.1446,  0.1154], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9272036552429199\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-5.7046e-05, -4.1725e-04,  4.6891e-04,  ...,  5.0591e-06,\n",
      "           3.4942e-05, -1.0506e-04],\n",
      "         [ 9.9722e-05, -1.1713e-05, -9.5881e-05,  ...,  1.8912e-04,\n",
      "           3.7043e-06,  2.8597e-04],\n",
      "         [-4.4453e-05,  1.1189e-04,  1.2184e-04,  ..., -3.9734e-05,\n",
      "           4.4952e-05,  7.2982e-05],\n",
      "         ...,\n",
      "         [-1.9071e-04,  1.8042e-04,  2.0304e-05,  ...,  5.9772e-05,\n",
      "           2.1915e-05,  1.9642e-05],\n",
      "         [-8.3036e-05,  7.6641e-05, -2.0500e-05,  ..., -2.1299e-05,\n",
      "          -4.8015e-06, -2.1186e-05],\n",
      "         [-4.1505e-05, -6.7389e-06, -3.0532e-06,  ..., -1.2476e-04,\n",
      "          -3.3247e-05, -5.3039e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.34019508957862854\n",
      "raw_input ['[CLS]', 'alternating', 'between', 'facetious', 'comic', 'parody', 'and', 'pulp', 'melodrama', ',', 'this', 'smart-aleck', 'movie', '...', 'tosses', 'around', 'some', 'intriguing', 'questions', 'about', 'the', 'difference', 'between', 'human', 'and', 'android', 'life', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.93)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.34</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alternating                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> facetious                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> comic                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> parody                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pulp                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> melodrama                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> smart-aleck                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ...                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tosses                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> around                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> intriguing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> questions                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> difference                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> human                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> android                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> life                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: to be a part of that elusive adult world \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'to', 'be', 'a', 'part', 'of', 'that', 'elusive', 'adult', 'world', '[SEP]']\n",
      "attribution.size torch.Size([1, 11, 768])\n",
      "word attr tensor([-0.2217, -0.1169, -0.2887, -0.0235,  0.0704,  0.5797,  0.3952,  0.4382,\n",
      "        -0.3514, -0.1747,  0.1001], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.2217, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1169, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2887, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0235, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0704, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.5797, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3952, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4382, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3514, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1747, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1001, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'to', 'be', 'a', 'part', 'of', 'that', 'elusive', 'adult', 'world', '[SEP]']\n",
      "len conti_raw 11\n",
      "conti_raw ['[CLS]', 'to', 'be', 'a', 'part', 'of', 'that', 'elusive', 'adult', 'world', '[SEP]']\n",
      "conti_attri_size: torch.Size([11]), raw_input_len:  11\n",
      "word attributions tensor([-0.2217, -0.1169, -0.2887, -0.0235,  0.0704,  0.5797,  0.3952,  0.4382,\n",
      "        -0.3514, -0.1747,  0.1001], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9597240090370178\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-5.7106e-06, -4.4951e-05, -7.3287e-05,  ...,  6.0580e-06,\n",
      "           1.1504e-05, -5.0293e-05],\n",
      "         [-7.9074e-06,  2.7797e-05, -1.0174e-05,  ..., -5.5681e-05,\n",
      "           3.0717e-05,  6.9963e-05],\n",
      "         [-1.8169e-05,  1.9898e-05,  1.1885e-05,  ..., -1.3127e-04,\n",
      "          -1.0561e-05,  3.6324e-05],\n",
      "         ...,\n",
      "         [-1.1675e-04,  2.5093e-05, -1.2660e-04,  ..., -5.1119e-04,\n",
      "          -6.3490e-04, -2.7509e-04],\n",
      "         [-8.1122e-06, -1.4693e-04,  3.5224e-06,  ...,  5.1901e-05,\n",
      "          -8.1771e-06, -4.2823e-05],\n",
      "         [-1.7240e-06, -3.2325e-08,  2.1431e-07,  ...,  2.5354e-05,\n",
      "          -5.6306e-07,  9.8437e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.4067387580871582\n",
      "raw_input ['[CLS]', 'to', 'be', 'a', 'part', 'of', 'that', 'elusive', 'adult', 'world', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.96)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.41</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> part                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> elusive                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> adult                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> world                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: emotional power \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'emotional', 'power', '[SEP]']\n",
      "attribution.size torch.Size([1, 4, 768])\n",
      "word attr tensor([ 0.2289,  0.8345, -0.2999, -0.4016], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2289, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.8345, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2999, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4016, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'emotional', 'power', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'emotional', 'power', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([ 0.2289,  0.8345, -0.2999, -0.4016], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9770727157592773\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[ 1.3127e-05,  2.2723e-05,  6.3689e-05,  ...,  1.0705e-07,\n",
      "           7.7197e-06,  4.7968e-06],\n",
      "         [-3.9285e-05, -3.1470e-06,  3.7734e-05,  ..., -1.2771e-05,\n",
      "          -1.1550e-04,  2.2015e-05],\n",
      "         [-2.2206e-05, -4.9624e-06, -9.6476e-06,  ..., -5.5834e-05,\n",
      "          -4.7645e-05, -3.9261e-06],\n",
      "         [-7.3419e-06,  2.1784e-05, -1.6599e-06,  ...,  1.6574e-05,\n",
      "           6.0319e-07,  1.9300e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.3618888258934021\n",
      "raw_input ['[CLS]', 'emotional', 'power', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.98)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.36</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> emotional                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> power                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: reminds you of why animation is such a perfect medium for children , because of the way it allows the mind to enter and accept another world \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'reminds', 'you', 'of', 'why', 'animation', 'is', 'such', 'a', 'perfect', 'medium', 'for', 'children', ',', 'because', 'of', 'the', 'way', 'it', 'allows', 'the', 'mind', 'to', 'enter', 'and', 'accept', 'another', 'world', '[SEP]']\n",
      "attribution.size torch.Size([1, 29, 768])\n",
      "word attr tensor([-1.7282e-01, -4.3502e-01,  3.5896e-01, -1.2356e-02,  4.0488e-02,\n",
      "        -3.9486e-01,  1.5669e-01,  4.2826e-03,  8.9566e-02, -4.2371e-02,\n",
      "         2.0590e-02,  1.5962e-01,  1.8793e-01,  5.2300e-01, -5.7915e-03,\n",
      "         1.5629e-01, -5.9820e-03, -1.4820e-02,  1.2546e-01, -5.2364e-02,\n",
      "        -1.1015e-01, -6.7547e-03,  1.1125e-01, -1.4747e-01,  1.3142e-01,\n",
      "        -1.3559e-04, -2.3657e-02,  1.2558e-01, -4.5692e-02], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1728, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4350, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3590, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0124, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0405, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3949, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1567, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0043, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0896, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0424, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0206, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1596, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1879, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.5230, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0058, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1563, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0060, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0148, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1255, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0524, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1101, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0068, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1113, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1475, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1314, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0001, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0237, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1256, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0457, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'reminds', 'you', 'of', 'why', 'animation', 'is', 'such', 'a', 'perfect', 'medium', 'for', 'children', ',', 'because', 'of', 'the', 'way', 'it', 'allows', 'the', 'mind', 'to', 'enter', 'and', 'accept', 'another', 'world', '[SEP]']\n",
      "len conti_raw 29\n",
      "conti_raw ['[CLS]', 'reminds', 'you', 'of', 'why', 'animation', 'is', 'such', 'a', 'perfect', 'medium', 'for', 'children', ',', 'because', 'of', 'the', 'way', 'it', 'allows', 'the', 'mind', 'to', 'enter', 'and', 'accept', 'another', 'world', '[SEP]']\n",
      "conti_attri_size: torch.Size([29]), raw_input_len:  29\n",
      "word attributions tensor([-1.7282e-01, -4.3502e-01,  3.5896e-01, -1.2356e-02,  4.0488e-02,\n",
      "        -3.9486e-01,  1.5669e-01,  4.2826e-03,  8.9566e-02, -4.2371e-02,\n",
      "         2.0590e-02,  1.5962e-01,  1.8793e-01,  5.2300e-01, -5.7915e-03,\n",
      "         1.5629e-01, -5.9820e-03, -1.4820e-02,  1.2546e-01, -5.2364e-02,\n",
      "        -1.1015e-01, -6.7547e-03,  1.1125e-01, -1.4747e-01,  1.3142e-01,\n",
      "        -1.3559e-04, -2.3657e-02,  1.2558e-01, -4.5692e-02], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9707044959068298\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-1.9493e-06, -2.7472e-05,  4.5581e-05,  ..., -3.2066e-07,\n",
      "           3.6805e-07,  2.7350e-06],\n",
      "         [-5.9369e-05,  1.8699e-04,  1.8960e-05,  ...,  4.1680e-05,\n",
      "          -6.3430e-05,  3.3588e-05],\n",
      "         [ 4.5409e-05,  1.2603e-05,  8.8290e-06,  ...,  2.4557e-05,\n",
      "           1.7124e-05, -1.4570e-05],\n",
      "         ...,\n",
      "         [ 3.0550e-08,  1.3725e-05, -1.7679e-07,  ...,  7.0417e-07,\n",
      "          -1.4703e-06, -9.0486e-07],\n",
      "         [-1.4697e-08,  8.3785e-06, -7.1520e-06,  ...,  1.9073e-05,\n",
      "          -7.8026e-06, -4.6081e-06],\n",
      "         [ 1.9716e-06,  2.6898e-06, -6.0718e-06,  ...,  1.8756e-05,\n",
      "           7.7612e-07, -2.2300e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Pos\n",
      "attr_score 0.7208849191665649\n",
      "raw_input ['[CLS]', 'reminds', 'you', 'of', 'why', 'animation', 'is', 'such', 'a', 'perfect', 'medium', 'for', 'children', ',', 'because', 'of', 'the', 'way', 'it', 'allows', 'the', 'mind', 'to', 'enter', 'and', 'accept', 'another', 'world', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.72</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reminds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> why                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> animation                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> such                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> perfect                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> medium                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> children                    </font></mark><mark style=\"background-color: hsl(120, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> because                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> way                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> allows                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mind                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enter                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> accept                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> another                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> world                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: unparalleled proportions , writer-director parker \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'un', 'para', 'lle', 'led', 'proportions', ',', 'writer', '-', 'director', 'parker', '[SEP]']\n",
      "attribution.size torch.Size([1, 12, 768])\n",
      "word attr tensor([-0.0780, -0.0742,  0.5437,  0.1833,  0.5613, -0.3191, -0.3551,  0.0134,\n",
      "        -0.2385, -0.0444, -0.2347, -0.0453], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0780, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3852, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.3191, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3551, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0785, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.2347, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0453, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'un', 'para', 'lle', 'led', 'proportions', ',', 'writer', '-', 'director', 'parker', '[SEP]']\n",
      "len conti_raw 7\n",
      "conti_raw ['[CLS]', 'unparalleled', 'proportions', ',', 'writer-director', 'parker', '[SEP]']\n",
      "conti_attri_size: torch.Size([7]), raw_input_len:  7\n",
      "word attributions tensor([-0.0780, -0.0742,  0.5437,  0.1833,  0.5613, -0.3191, -0.3551,  0.0134,\n",
      "        -0.2385, -0.0444, -0.2347, -0.0453], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.8834286332130432\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-1.3690e-03,  5.3666e-04,  4.6986e-03,  ...,  3.6927e-04,\n",
      "          -1.1285e-04,  9.6212e-04],\n",
      "         [-4.8043e-03, -6.6548e-04,  3.5503e-04,  ..., -1.0213e-03,\n",
      "           5.8423e-04, -1.0551e-03],\n",
      "         [ 2.8534e-04, -1.2939e-02,  1.8908e-03,  ..., -2.5307e-03,\n",
      "           2.1004e-03, -8.0169e-04],\n",
      "         ...,\n",
      "         [ 2.5952e-03,  1.6008e-03,  1.3781e-03,  ...,  2.0294e-03,\n",
      "          -1.6675e-03, -3.8248e-04],\n",
      "         [ 2.6019e-03, -2.0525e-04, -7.4006e-03,  ...,  1.0706e-03,\n",
      "           1.6927e-03,  8.1553e-04],\n",
      "         [ 5.2381e-05, -2.3889e-04,  9.8754e-05,  ...,  6.4149e-04,\n",
      "           1.6421e-03,  1.8162e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.08753636479377747\n",
      "raw_input ['[CLS]', 'unparalleled', 'proportions', ',', 'writer-director', 'parker', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.88)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.09</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> unparalleled                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> proportions                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> writer-director                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> parker                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: this surprisingly decent flick \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'this', 'surprisingly', 'decent', 'flick', '[SEP]']\n",
      "attribution.size torch.Size([1, 6, 768])\n",
      "word attr tensor([-0.0320,  0.1857, -0.0830, -0.8073,  0.5528, -0.0162], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0320, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1857, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0830, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.8073, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.5528, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0162, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'this', 'surprisingly', 'decent', 'flick', '[SEP]']\n",
      "len conti_raw 6\n",
      "conti_raw ['[CLS]', 'this', 'surprisingly', 'decent', 'flick', '[SEP]']\n",
      "conti_attri_size: torch.Size([6]), raw_input_len:  6\n",
      "word attributions tensor([-0.0320,  0.1857, -0.0830, -0.8073,  0.5528, -0.0162], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9658218622207642\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-3.7908e-05, -4.9183e-05,  8.6487e-06,  ..., -4.1470e-06,\n",
      "           1.8032e-06, -4.7871e-05],\n",
      "         [ 6.3360e-05,  1.7524e-05, -8.9799e-07,  ..., -1.2517e-05,\n",
      "           4.8621e-06, -2.0737e-05],\n",
      "         [-2.1350e-05, -9.5027e-05,  1.6444e-04,  ..., -1.3762e-05,\n",
      "          -1.1767e-06, -6.2087e-05],\n",
      "         [-9.8799e-05, -7.9502e-05,  7.8448e-05,  ...,  1.1757e-05,\n",
      "           1.7321e-05,  5.1947e-05],\n",
      "         [ 5.6605e-05,  3.0655e-05, -2.2095e-06,  ...,  8.8350e-05,\n",
      "           7.4597e-05,  1.1686e-04],\n",
      "         [-5.1702e-05,  8.0042e-06, -1.3448e-05,  ..., -5.1371e-05,\n",
      "          -1.6773e-06,  8.5062e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.19993233680725098\n",
      "raw_input ['[CLS]', 'this', 'surprisingly', 'decent', 'flick', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.20</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> surprisingly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decent                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> flick                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: about the best thing you could say about narc is that it 's a rock-solid little genre picture . \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'about', 'the', 'best', 'thing', 'you', 'could', 'say', 'about', 'na', 'rc', 'is', 'that', 'it', \"'\", 's', 'a', 'rock', '-', 'solid', 'little', 'genre', 'picture', '.', '[SEP]']\n",
      "attribution.size torch.Size([1, 25, 768])\n",
      "word attr tensor([-1.0571e-01,  1.0276e-02,  1.3271e-02, -1.8174e-01, -2.4987e-01,\n",
      "        -1.6005e-01, -3.9558e-02, -2.3003e-01,  3.4982e-04,  4.9282e-01,\n",
      "        -3.2087e-02, -1.6324e-01,  7.6906e-02, -7.4246e-02,  5.3206e-02,\n",
      "        -2.7259e-02,  4.5712e-01,  2.7482e-01,  1.6931e-02,  5.5243e-02,\n",
      "         2.7484e-02,  3.1365e-01, -3.6857e-01,  6.6165e-02, -1.6048e-02],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1057, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0103, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0133, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1817, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2499, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1600, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0396, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2300, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0003, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2304, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.1632, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0769, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0742, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0130, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.4571, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1006, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0275, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3136, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3686, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0662, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0160, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'about', 'the', 'best', 'thing', 'you', 'could', 'say', 'about', 'na', 'rc', 'is', 'that', 'it', \"'\", 's', 'a', 'rock', '-', 'solid', 'little', 'genre', 'picture', '.', '[SEP]']\n",
      "len conti_raw 21\n",
      "conti_raw ['[CLS]', 'about', 'the', 'best', 'thing', 'you', 'could', 'say', 'about', 'narc', 'is', 'that', 'it', \"'s\", 'a', 'rock-solid', 'little', 'genre', 'picture', '.', '[SEP]']\n",
      "conti_attri_size: torch.Size([21]), raw_input_len:  21\n",
      "word attributions tensor([-1.0571e-01,  1.0276e-02,  1.3271e-02, -1.8174e-01, -2.4987e-01,\n",
      "        -1.6005e-01, -3.9558e-02, -2.3003e-01,  3.4982e-04,  4.9282e-01,\n",
      "        -3.2087e-02, -1.6324e-01,  7.6906e-02, -7.4246e-02,  5.3206e-02,\n",
      "        -2.7259e-02,  4.5712e-01,  2.7482e-01,  1.6931e-02,  5.5243e-02,\n",
      "         2.7484e-02,  3.1365e-01, -3.6857e-01,  6.6165e-02, -1.6048e-02],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9529748558998108\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-8.3879e-05, -2.4201e-05,  9.9320e-05,  ...,  1.1370e-05,\n",
      "          -2.4173e-05, -5.1113e-05],\n",
      "         [ 3.5297e-08,  5.2696e-06,  4.7844e-07,  ...,  7.8000e-05,\n",
      "           1.1606e-05, -2.0265e-05],\n",
      "         [ 1.8848e-04,  6.0568e-05,  1.0493e-05,  ..., -4.3904e-06,\n",
      "          -1.2509e-05, -3.3004e-05],\n",
      "         ...,\n",
      "         [-2.2781e-04,  1.4775e-04,  2.5136e-04,  ...,  7.3135e-06,\n",
      "          -6.2341e-04, -1.0640e-04],\n",
      "         [ 3.1001e-05,  1.3900e-04,  3.8195e-05,  ..., -6.9541e-05,\n",
      "           8.3273e-05,  2.9944e-05],\n",
      "         [ 1.3619e-05,  1.0677e-05,  3.2851e-05,  ..., -2.3965e-06,\n",
      "          -2.2746e-05, -2.3708e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.20983901619911194\n",
      "raw_input ['[CLS]', 'about', 'the', 'best', 'thing', 'you', 'could', 'say', 'about', 'narc', 'is', 'that', 'it', \"'s\", 'a', 'rock-solid', 'little', 'genre', 'picture', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.95)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.21</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> could                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> say                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> narc                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rock-solid                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> little                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> genre                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> picture                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: the very best \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'the', 'very', 'best', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.4469,  0.0911, -0.6789, -0.3612,  0.4479], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4469, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0911, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6789, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3612, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4479, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'the', 'very', 'best', '[SEP]']\n",
      "len conti_raw 5\n",
      "conti_raw ['[CLS]', 'the', 'very', 'best', '[SEP]']\n",
      "conti_attri_size: torch.Size([5]), raw_input_len:  5\n",
      "word attributions tensor([ 0.4469,  0.0911, -0.6789, -0.3612,  0.4479], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9749705791473389\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-1.5726e-06, -3.7741e-06, -2.2028e-06,  ...,  1.3993e-07,\n",
      "          -5.0455e-07, -3.4728e-06],\n",
      "         [ 3.8678e-06, -9.3996e-06,  1.4354e-07,  ..., -4.6208e-07,\n",
      "           2.3996e-05,  3.6043e-08],\n",
      "         [-3.9884e-05, -1.8896e-05,  2.6419e-05,  ..., -8.6467e-06,\n",
      "           6.5382e-05, -4.8945e-05],\n",
      "         [ 5.1369e-06, -1.7922e-05, -9.6659e-05,  ...,  2.8248e-05,\n",
      "           2.3627e-05, -1.0502e-05],\n",
      "         [ 2.1263e-05, -1.7121e-06,  1.1478e-06,  ...,  5.0158e-06,\n",
      "           2.4289e-06, -3.1984e-07]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.05411481857299805\n",
      "raw_input ['[CLS]', 'the', 'very', 'best', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.05</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> very                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: been modeled on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'been', 'modeled', 'on', 'the', 'worst', 'revenge', '-', 'of', '-', 'the', '-', 'ne', 'rds', 'cl', 'iche', 's', 'the', 'filmmakers', 'could', 'dr', 'edge', 'up', '[SEP]']\n",
      "attribution.size torch.Size([1, 24, 768])\n",
      "word attr tensor([ 0.1239, -0.1179, -0.2977,  0.0219, -0.0534,  0.3971,  0.1547,  0.0668,\n",
      "        -0.2050, -0.1495,  0.0268, -0.1968, -0.0445, -0.1046,  0.4782,  0.0224,\n",
      "        -0.0217, -0.3645, -0.2225,  0.2042,  0.2663, -0.0989, -0.0466, -0.1986],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1239, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1179, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2977, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0219, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0534, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3971, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0925, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1143, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.3645, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2225, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2042, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0837, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0466, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1986, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'been', 'modeled', 'on', 'the', 'worst', 'revenge', '-', 'of', '-', 'the', '-', 'ne', 'rds', 'cl', 'iche', 's', 'the', 'filmmakers', 'could', 'dr', 'edge', 'up', '[SEP]']\n",
      "len conti_raw 14\n",
      "conti_raw ['[CLS]', 'been', 'modeled', 'on', 'the', 'worst', 'revenge-of-the-nerds', 'cliches', 'the', 'filmmakers', 'could', 'dredge', 'up', '[SEP]']\n",
      "conti_attri_size: torch.Size([14]), raw_input_len:  14\n",
      "word attributions tensor([ 0.1239, -0.1179, -0.2977,  0.0219, -0.0534,  0.3971,  0.1547,  0.0668,\n",
      "        -0.2050, -0.1495,  0.0268, -0.1968, -0.0445, -0.1046,  0.4782,  0.0224,\n",
      "        -0.0217, -0.3645, -0.2225,  0.2042,  0.2663, -0.0989, -0.0466, -0.1986],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.030051304027438164\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 1.4502e-05,  1.4741e-05, -2.5708e-05,  ..., -1.5564e-06,\n",
      "          -2.4454e-07, -9.3092e-06],\n",
      "         [-1.6708e-05, -6.6752e-06, -4.2105e-06,  ..., -1.6519e-05,\n",
      "           1.8434e-05,  5.7428e-05],\n",
      "         [ 1.1637e-05, -3.4137e-05, -4.6630e-05,  ..., -7.7274e-06,\n",
      "           2.0469e-05,  2.5676e-06],\n",
      "         ...,\n",
      "         [ 3.6232e-06,  1.7431e-05, -2.7441e-05,  ..., -2.1750e-05,\n",
      "           1.9033e-06, -7.8760e-06],\n",
      "         [ 5.3451e-06, -1.0707e-05, -6.2580e-06,  ..., -2.4123e-06,\n",
      "          -3.3773e-05, -7.6056e-07],\n",
      "         [-3.7619e-06, -1.7280e-05, -7.1128e-08,  ..., -9.6150e-06,\n",
      "          -4.1897e-06, -5.5655e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.3598250150680542\n",
      "raw_input ['[CLS]', 'been', 'modeled', 'on', 'the', 'worst', 'revenge-of-the-nerds', 'cliches', 'the', 'filmmakers', 'could', 'dredge', 'up', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.03)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.36</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> modeled                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> worst                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> revenge-of-the-nerds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cliches                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> filmmakers                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> could                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dredge                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> up                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: tell you \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'tell', 'you', '[SEP]']\n",
      "attribution.size torch.Size([1, 4, 768])\n",
      "word attr tensor([-0.1917,  0.8105,  0.2391, -0.4991], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1917, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.8105, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2391, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4991, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'tell', 'you', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'tell', 'you', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([-0.1917,  0.8105,  0.2391, -0.4991], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.8954260945320129\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[ 1.6948e-04,  7.5237e-05,  7.2884e-04,  ...,  2.4884e-05,\n",
      "           5.3599e-05, -2.7698e-04],\n",
      "         [-5.3324e-05, -4.0641e-04,  9.7193e-04,  ...,  1.7618e-03,\n",
      "          -5.5861e-03, -3.4724e-03],\n",
      "         [ 3.2000e-04,  3.1976e-04, -4.9410e-04,  ...,  4.9872e-04,\n",
      "           7.4494e-04,  3.8955e-05],\n",
      "         [ 1.3637e-04,  1.3046e-04, -2.0151e-04,  ...,  7.2926e-05,\n",
      "           1.5687e-06,  1.2726e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.35883164405822754\n",
      "raw_input ['[CLS]', 'tell', 'you', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.90)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.36</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tell                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: utterly absorbing \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'utterly', 'absorbing', '[SEP]']\n",
      "attribution.size torch.Size([1, 4, 768])\n",
      "word attr tensor([ 0.7587,  0.2889, -0.5804,  0.0639], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.7587, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2889, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.5804, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0639, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'utterly', 'absorbing', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'utterly', 'absorbing', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([ 0.7587,  0.2889, -0.5804,  0.0639], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.974568247795105\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-5.8755e-06, -7.5190e-06,  3.2281e-06,  ..., -7.2082e-07,\n",
      "           2.9925e-06,  2.7231e-06],\n",
      "         [-3.4200e-05, -6.0501e-06, -2.4908e-07,  ..., -3.4590e-05,\n",
      "          -1.6954e-06, -7.4547e-06],\n",
      "         [-3.9505e-05, -1.3914e-05,  1.7442e-05,  ..., -3.8390e-05,\n",
      "          -5.0091e-05, -6.1136e-06],\n",
      "         [ 1.8123e-05, -3.6713e-06,  1.2399e-06,  ...,  1.1440e-05,\n",
      "          -1.8561e-07,  5.5115e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Pos\n",
      "attr_score 0.5310527086257935\n",
      "raw_input ['[CLS]', 'utterly', 'absorbing', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.53</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> utterly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> absorbing                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: restate \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'rest', 'ate', '[SEP]']\n",
      "attribution.size torch.Size([1, 4, 768])\n",
      "word attr tensor([-0.6817, -0.0015,  0.0604,  0.7291], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.6817, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0294, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7291, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'rest', 'ate', '[SEP]']\n",
      "len conti_raw 3\n",
      "conti_raw ['[CLS]', 'restate', '[SEP]']\n",
      "conti_attri_size: torch.Size([3]), raw_input_len:  3\n",
      "word attributions tensor([-0.6817, -0.0015,  0.0604,  0.7291], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.5530857443809509\n",
      "pred_class Pos\n",
      "true_class Neg\n",
      "attribution tensor([[[ 0.0011, -0.0052, -0.0098,  ...,  0.0008, -0.0006, -0.0057],\n",
      "         [-0.0025, -0.0028,  0.0131,  ..., -0.0083, -0.0075, -0.0011],\n",
      "         [ 0.0056,  0.0008, -0.0446,  ..., -0.0038, -0.0153,  0.0231],\n",
      "         [-0.0013, -0.0002, -0.0063,  ..., -0.0035, -0.0009,  0.0033]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.10627228021621704\n",
      "raw_input ['[CLS]', 'restate', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.55)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.11</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> restate                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: bears about as much resemblance to the experiences of most battered women as spider-man \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'bears', 'about', 'as', 'much', 'resemblance', 'to', 'the', 'experiences', 'of', 'most', 'battered', 'women', 'as', 'spider', '-', 'man', '[SEP]']\n",
      "attribution.size torch.Size([1, 18, 768])\n",
      "word attr tensor([-0.1064, -0.3416, -0.0236,  0.1678, -0.3179, -0.0244,  0.1094, -0.2324,\n",
      "         0.7117,  0.0610, -0.1641,  0.0336,  0.3203, -0.0346, -0.0900,  0.0219,\n",
      "        -0.1502, -0.0487], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1064, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3416, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0236, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1678, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3179, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0244, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1094, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2324, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.7117, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0610, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1641, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0336, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3203, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0346, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0921, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0487, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'bears', 'about', 'as', 'much', 'resemblance', 'to', 'the', 'experiences', 'of', 'most', 'battered', 'women', 'as', 'spider', '-', 'man', '[SEP]']\n",
      "len conti_raw 16\n",
      "conti_raw ['[CLS]', 'bears', 'about', 'as', 'much', 'resemblance', 'to', 'the', 'experiences', 'of', 'most', 'battered', 'women', 'as', 'spider-man', '[SEP]']\n",
      "conti_attri_size: torch.Size([16]), raw_input_len:  16\n",
      "word attributions tensor([-0.1064, -0.3416, -0.0236,  0.1678, -0.3179, -0.0244,  0.1094, -0.2324,\n",
      "         0.7117,  0.0610, -0.1641,  0.0336,  0.3203, -0.0346, -0.0900,  0.0219,\n",
      "        -0.1502, -0.0487], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.7086556553840637\n",
      "pred_class Pos\n",
      "true_class Neg\n",
      "attribution tensor([[[-2.0609e-04, -9.4074e-04,  1.0023e-03,  ...,  7.6250e-05,\n",
      "           1.4261e-04, -1.1315e-03],\n",
      "         [ 2.9596e-03,  1.2237e-03,  9.3196e-04,  ..., -1.9112e-03,\n",
      "          -2.4170e-03, -2.9512e-03],\n",
      "         [-1.3850e-03, -8.1027e-05,  4.2842e-04,  ..., -1.8960e-03,\n",
      "          -1.6938e-04, -7.4819e-04],\n",
      "         ...,\n",
      "         [ 3.3389e-05,  2.4678e-04, -5.1011e-04,  ...,  9.3583e-05,\n",
      "           2.5418e-04, -9.0653e-05],\n",
      "         [-1.8092e-05, -5.9786e-04,  1.5406e-05,  ...,  7.5957e-04,\n",
      "          -1.8389e-03,  1.5642e-03],\n",
      "         [ 1.2571e-04, -3.6306e-04, -1.1855e-04,  ...,  6.3281e-05,\n",
      "           2.3516e-04,  3.7622e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.10832953453063965\n",
      "raw_input ['[CLS]', 'bears', 'about', 'as', 'much', 'resemblance', 'to', 'the', 'experiences', 'of', 'most', 'battered', 'women', 'as', 'spider-man', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.71)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.11</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bears                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> much                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> resemblance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> experiences                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> battered                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> women                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spider-man                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: expressively performed \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'expressive', 'ly', 'performed', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.7281,  0.0105,  0.4696, -0.4946, -0.0677], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.7281, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2400, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.4946, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0677, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'expressive', 'ly', 'performed', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'expressively', 'performed', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([ 0.7281,  0.0105,  0.4696, -0.4946, -0.0677], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9764208197593689\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[ 2.2622e-05, -2.6069e-05,  6.6887e-06,  ...,  3.9094e-07,\n",
      "          -2.8721e-06, -1.8889e-05],\n",
      "         [-1.0269e-04, -1.3553e-04,  5.5502e-06,  ...,  1.0555e-05,\n",
      "           3.9240e-07,  7.4170e-05],\n",
      "         [-6.6972e-06, -4.4028e-06,  3.1622e-05,  ...,  7.8215e-08,\n",
      "          -7.0880e-05, -1.3707e-05],\n",
      "         [-4.6956e-06, -3.7383e-06, -1.5216e-04,  ...,  3.3071e-05,\n",
      "           9.7264e-06, -1.3651e-05],\n",
      "         [-1.9373e-05, -3.0151e-06,  2.7378e-06,  ...,  5.2237e-06,\n",
      "          -1.5071e-06,  9.2984e-07]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Pos\n",
      "attr_score 0.6457632780075073\n",
      "raw_input ['[CLS]', 'expressively', 'performed', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.98)</b></text></td><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.65</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> expressively                    </font></mark><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: the acting is amateurish , the cinematography is atrocious \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'the', 'acting', 'is', 'amateur', 'ish', ',', 'the', 'cinematography', 'is', 'at', 'ro', 'cious', '[SEP]']\n",
      "attribution.size torch.Size([1, 14, 768])\n",
      "word attr tensor([ 0.0726, -0.0022,  0.2605, -0.0500,  0.4875, -0.3496,  0.0793,  0.0374,\n",
      "         0.1258, -0.2427,  0.3035, -0.5150, -0.0937, -0.3405], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0726, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0022, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2605, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0500, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0690, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0793, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0374, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1258, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2427, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0997, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.3405, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'the', 'acting', 'is', 'amateur', 'ish', ',', 'the', 'cinematography', 'is', 'at', 'ro', 'cious', '[SEP]']\n",
      "len conti_raw 11\n",
      "conti_raw ['[CLS]', 'the', 'acting', 'is', 'amateurish', ',', 'the', 'cinematography', 'is', 'atrocious', '[SEP]']\n",
      "conti_attri_size: torch.Size([11]), raw_input_len:  11\n",
      "word attributions tensor([ 0.0726, -0.0022,  0.2605, -0.0500,  0.4875, -0.3496,  0.0793,  0.0374,\n",
      "         0.1258, -0.2427,  0.3035, -0.5150, -0.0937, -0.3405], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.031884290277957916\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 3.1152e-05,  9.8489e-07,  1.8055e-04,  ...,  2.1063e-05,\n",
      "          -8.8610e-06,  7.6682e-05],\n",
      "         [ 9.9634e-06,  1.1925e-05, -7.2186e-07,  ...,  4.5815e-06,\n",
      "          -1.0859e-05, -2.5873e-05],\n",
      "         [ 9.5554e-07, -3.9607e-06,  2.3549e-06,  ...,  1.7172e-04,\n",
      "           8.1369e-05, -2.7915e-06],\n",
      "         ...,\n",
      "         [ 2.2013e-04,  1.7475e-05,  7.7590e-05,  ...,  1.9327e-05,\n",
      "          -1.7467e-04,  7.3584e-05],\n",
      "         [ 1.1327e-04, -1.0497e-05,  3.6865e-05,  ..., -9.3987e-04,\n",
      "           1.0718e-04, -3.6612e-05],\n",
      "         [-4.6008e-05,  1.2201e-05, -4.0564e-05,  ...,  7.2045e-05,\n",
      "          -1.4340e-05,  7.4914e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.2268892079591751\n",
      "raw_input ['[CLS]', 'the', 'acting', 'is', 'amateurish', ',', 'the', 'cinematography', 'is', 'atrocious', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.03)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.23</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> amateurish                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cinematography                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> atrocious                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: solidly constructed \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'solid', 'ly', 'constructed', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.1286,  0.5019,  0.1010, -0.8465, -0.0695], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1286, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3014, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.8465, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0695, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'solid', 'ly', 'constructed', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'solidly', 'constructed', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([ 0.1286,  0.5019,  0.1010, -0.8465, -0.0695], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9670499563217163\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[ 9.0524e-06, -4.3318e-05,  2.5908e-05,  ...,  1.1114e-05,\n",
      "           1.8061e-05,  4.2793e-05],\n",
      "         [ 3.0117e-04, -7.9200e-06, -1.7391e-04,  ..., -1.3817e-06,\n",
      "           4.6694e-04, -4.1211e-06],\n",
      "         [-2.8881e-05, -2.3238e-05, -1.9858e-05,  ..., -3.1670e-06,\n",
      "           1.1579e-04,  3.3367e-05],\n",
      "         [-6.5006e-05, -6.1647e-04, -4.3587e-04,  ..., -3.9856e-05,\n",
      "           6.3274e-05,  7.9873e-05],\n",
      "         [ 5.3489e-06,  9.9481e-06,  1.3974e-05,  ...,  3.6162e-05,\n",
      "           3.0708e-05, -8.5965e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.18467366695404053\n",
      "raw_input ['[CLS]', 'solidly', 'constructed', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.18</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> solidly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> constructed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: are undermined by the movie 's presentation , which is way too stagy \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'are', 'undermine', 'd', 'by', 'the', 'movie', \"'\", 's', 'presentation', ',', 'which', 'is', 'way', 'too', 'st', 'ag', 'y', '[SEP]']\n",
      "attribution.size torch.Size([1, 19, 768])\n",
      "word attr tensor([-0.0901,  0.1143, -0.6174,  0.3966,  0.0376,  0.0031,  0.2809, -0.0100,\n",
      "         0.0622, -0.0248, -0.4176,  0.1376, -0.1121, -0.0952, -0.0285,  0.3559,\n",
      "        -0.0393, -0.0197,  0.1051], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0901, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1143, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1104, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0376, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0031, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2809, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0261, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0248, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4176, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1376, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1121, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0952, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0285, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0693, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1051, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'are', 'undermine', 'd', 'by', 'the', 'movie', \"'\", 's', 'presentation', ',', 'which', 'is', 'way', 'too', 'st', 'ag', 'y', '[SEP]']\n",
      "len conti_raw 15\n",
      "conti_raw ['[CLS]', 'are', 'undermined', 'by', 'the', 'movie', \"'s\", 'presentation', ',', 'which', 'is', 'way', 'too', 'stagy', '[SEP]']\n",
      "conti_attri_size: torch.Size([15]), raw_input_len:  15\n",
      "word attributions tensor([-0.0901,  0.1143, -0.6174,  0.3966,  0.0376,  0.0031,  0.2809, -0.0100,\n",
      "         0.0622, -0.0248, -0.4176,  0.1376, -0.1121, -0.0952, -0.0285,  0.3559,\n",
      "        -0.0393, -0.0197,  0.1051], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.04456028714776039\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 2.1709e-05, -1.0990e-05, -9.1821e-06,  ..., -2.3799e-06,\n",
      "          -2.3174e-06,  4.0010e-07],\n",
      "         [-6.3746e-06, -6.2289e-07, -3.1407e-05,  ..., -7.3458e-06,\n",
      "           2.1751e-06,  6.7365e-06],\n",
      "         [-9.8520e-05, -4.7012e-05, -7.6850e-06,  ...,  9.8600e-07,\n",
      "          -6.0562e-05,  3.2081e-05],\n",
      "         ...,\n",
      "         [-4.6194e-05,  5.7704e-05, -2.2622e-05,  ..., -1.4948e-06,\n",
      "          -4.6414e-05,  3.4469e-06],\n",
      "         [ 1.5426e-05,  7.0727e-07,  9.2346e-06,  ..., -1.2389e-05,\n",
      "          -1.9147e-07,  7.3344e-06],\n",
      "         [ 2.2232e-06,  5.4812e-07, -6.1788e-08,  ...,  2.4420e-07,\n",
      "          -1.1138e-06, -3.8960e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.038522496819496155\n",
      "raw_input ['[CLS]', 'are', 'undermined', 'by', 'the', 'movie', \"'s\", 'presentation', ',', 'which', 'is', 'way', 'too', 'stagy', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.04)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.04</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> undermined                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> presentation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> way                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> too                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stagy                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: a great film \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'a', 'great', 'film', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.4263,  0.0523, -0.6309,  0.4843, -0.4278], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4263, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0523, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6309, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4843, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4278, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'a', 'great', 'film', '[SEP]']\n",
      "len conti_raw 5\n",
      "conti_raw ['[CLS]', 'a', 'great', 'film', '[SEP]']\n",
      "conti_attri_size: torch.Size([5]), raw_input_len:  5\n",
      "word attributions tensor([ 0.4263,  0.0523, -0.6309,  0.4843, -0.4278], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9733861684799194\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[ 3.8071e-06,  6.5181e-06,  3.8087e-05,  ...,  2.5980e-07,\n",
      "           9.8095e-07,  1.6948e-06],\n",
      "         [ 8.0576e-06, -3.0133e-05, -5.5306e-07,  ...,  1.5439e-05,\n",
      "           1.6651e-05, -2.1615e-05],\n",
      "         [ 3.0059e-06, -7.2396e-06, -4.0301e-05,  ..., -4.0350e-05,\n",
      "          -4.8268e-05,  7.1154e-06],\n",
      "         [-4.3089e-05,  3.7098e-07,  3.2425e-05,  ...,  1.5705e-05,\n",
      "           8.5213e-06,  8.4914e-06],\n",
      "         [-3.3712e-06,  2.8807e-06, -2.7770e-06,  ...,  3.2585e-06,\n",
      "           2.8002e-06, -6.0028e-07]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.09574230015277863\n",
      "raw_input ['[CLS]', 'a', 'great', 'film', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.10</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: charm \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'charm', '[SEP]']\n",
      "attribution.size torch.Size([1, 3, 768])\n",
      "word attr tensor([ 0.4231,  0.3307, -0.8436], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4231, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3307, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.8436, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'charm', '[SEP]']\n",
      "len conti_raw 3\n",
      "conti_raw ['[CLS]', 'charm', '[SEP]']\n",
      "conti_attri_size: torch.Size([3]), raw_input_len:  3\n",
      "word attributions tensor([ 0.4231,  0.3307, -0.8436], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9737695455551147\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-3.7756e-05, -5.4807e-05,  2.9673e-05,  ...,  2.5451e-06,\n",
      "          -7.0754e-06, -8.8289e-05],\n",
      "         [ 1.6365e-05,  1.9021e-04, -2.0888e-05,  ..., -6.9746e-05,\n",
      "           1.6556e-04, -1.2634e-04],\n",
      "         [ 1.1341e-05, -1.4848e-06, -3.5010e-08,  ..., -2.7444e-05,\n",
      "           5.8132e-06,  1.0974e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.08973601460456848\n",
      "raw_input ['[CLS]', 'charm', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.09</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> charm                    </font></mark><mark style=\"background-color: hsl(0, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: this new jangle of noise , mayhem and stupidity \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'this', 'new', 'jang', 'le', 'of', 'noise', ',', 'mayhem', 'and', 'stupidity', '[SEP]']\n",
      "attribution.size torch.Size([1, 12, 768])\n",
      "word attr tensor([ 0.2218, -0.0386,  0.0836,  0.3184, -0.1584,  0.2109, -0.4157, -0.0611,\n",
      "         0.4498,  0.0442, -0.5193, -0.3478], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2218, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0386, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0836, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0800, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2109, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4157, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0611, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4498, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0442, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.5193, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3478, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'this', 'new', 'jang', 'le', 'of', 'noise', ',', 'mayhem', 'and', 'stupidity', '[SEP]']\n",
      "len conti_raw 11\n",
      "conti_raw ['[CLS]', 'this', 'new', 'jangle', 'of', 'noise', ',', 'mayhem', 'and', 'stupidity', '[SEP]']\n",
      "conti_attri_size: torch.Size([11]), raw_input_len:  11\n",
      "word attributions tensor([ 0.2218, -0.0386,  0.0836,  0.3184, -0.1584,  0.2109, -0.4157, -0.0611,\n",
      "         0.4498,  0.0442, -0.5193, -0.3478], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.09326992928981781\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-7.9858e-04, -5.7894e-05,  1.3077e-03,  ...,  1.9946e-06,\n",
      "           1.4976e-04,  4.8682e-05],\n",
      "         [ 1.4561e-04, -2.7210e-04,  9.1617e-05,  ..., -4.0648e-04,\n",
      "          -1.6151e-04, -1.6426e-04],\n",
      "         [ 2.4620e-04, -2.5611e-05, -8.1231e-04,  ...,  1.7174e-03,\n",
      "          -6.0632e-04,  3.4388e-04],\n",
      "         ...,\n",
      "         [ 2.0533e-04,  1.3401e-04,  2.9562e-04,  ...,  9.9795e-04,\n",
      "          -7.1221e-04,  6.7601e-04],\n",
      "         [ 1.1342e-03, -4.0498e-04, -8.5521e-05,  ..., -7.6541e-04,\n",
      "           1.8276e-03, -3.0406e-03],\n",
      "         [-4.3591e-04,  7.6237e-05,  9.3653e-05,  ..., -5.7256e-04,\n",
      "          -2.4154e-04,  5.4355e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.21217358112335205\n",
      "raw_input ['[CLS]', 'this', 'new', 'jangle', 'of', 'noise', ',', 'mayhem', 'and', 'stupidity', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.09)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.21</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> new                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> jangle                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> noise                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mayhem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stupidity                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: sustains it \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'sustain', 's', 'it', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.1955,  0.7376,  0.1932, -0.5361, -0.3050], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1955, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4654, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.5361, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3050, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'sustain', 's', 'it', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'sustains', 'it', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([ 0.1955,  0.7376,  0.1932, -0.5361, -0.3050], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9708950519561768\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-1.2169e-05, -6.1126e-05,  1.1738e-04,  ...,  1.0516e-06,\n",
      "          -2.8821e-06, -2.1278e-05],\n",
      "         [ 2.9901e-05, -6.7084e-05, -2.1465e-05,  ..., -1.0447e-05,\n",
      "          -1.3921e-04,  5.2941e-05],\n",
      "         [-1.5009e-05,  6.7858e-07, -6.8856e-05,  ...,  5.2170e-06,\n",
      "           1.1680e-06,  3.3056e-06],\n",
      "         [-2.1204e-05,  1.1838e-05, -8.5127e-06,  ..., -5.4897e-05,\n",
      "           2.1695e-05, -8.9943e-06],\n",
      "         [-1.1647e-05,  7.5310e-07, -1.4874e-06,  ...,  2.0025e-05,\n",
      "           6.9576e-07, -1.7007e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.2851855158805847\n",
      "raw_input ['[CLS]', 'sustains', 'it', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.29</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sustains                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: is so deadly dull that watching the proverbial paint dry would be a welcome improvement . \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'is', 'so', 'deadly', 'dull', 'that', 'watching', 'the', 'prove', 'rb', 'ial', 'paint', 'dry', 'would', 'be', 'a', 'welcome', 'improvement', '.', '[SEP]']\n",
      "attribution.size torch.Size([1, 20, 768])\n",
      "word attr tensor([-0.3513, -0.0648,  0.1475,  0.6808, -0.1779,  0.1738,  0.0773, -0.0202,\n",
      "         0.3626,  0.0458, -0.1733, -0.1180, -0.1224, -0.0651, -0.1918,  0.1865,\n",
      "         0.0703, -0.0817,  0.1946, -0.0343], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.3513, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0648, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1475, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.6808, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1779, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1738, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0773, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0202, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0155, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.1180, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1224, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0651, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1918, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1865, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0703, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0817, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1946, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0343, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'is', 'so', 'deadly', 'dull', 'that', 'watching', 'the', 'prove', 'rb', 'ial', 'paint', 'dry', 'would', 'be', 'a', 'welcome', 'improvement', '.', '[SEP]']\n",
      "len conti_raw 18\n",
      "conti_raw ['[CLS]', 'is', 'so', 'deadly', 'dull', 'that', 'watching', 'the', 'proverbial', 'paint', 'dry', 'would', 'be', 'a', 'welcome', 'improvement', '.', '[SEP]']\n",
      "conti_attri_size: torch.Size([18]), raw_input_len:  18\n",
      "word attributions tensor([-0.3513, -0.0648,  0.1475,  0.6808, -0.1779,  0.1738,  0.0773, -0.0202,\n",
      "         0.3626,  0.0458, -0.1733, -0.1180, -0.1224, -0.0651, -0.1918,  0.1865,\n",
      "         0.0703, -0.0817,  0.1946, -0.0343], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.41628754138946533\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-2.3901e-03, -6.5693e-04,  2.8960e-03,  ...,  2.8596e-04,\n",
      "           1.5143e-04, -2.0218e-03],\n",
      "         [-1.6023e-04,  2.2168e-05,  9.1537e-04,  ..., -2.9368e-03,\n",
      "          -5.8927e-04, -5.2006e-04],\n",
      "         [ 2.2618e-03,  1.5420e-03,  2.7514e-04,  ...,  6.0882e-04,\n",
      "           1.5734e-03,  1.7694e-03],\n",
      "         ...,\n",
      "         [ 8.2648e-04,  1.1560e-02, -1.5523e-03,  ..., -2.0432e-03,\n",
      "          -9.6284e-03, -4.1791e-03],\n",
      "         [ 1.9582e-03,  2.7271e-03,  4.2241e-05,  ...,  6.6640e-04,\n",
      "           2.3050e-03,  2.3111e-03],\n",
      "         [ 1.0196e-03,  4.0439e-05,  6.6492e-04,  ..., -8.5574e-04,\n",
      "           1.8582e-05, -1.2017e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Pos\n",
      "attr_score 0.5384165644645691\n",
      "raw_input ['[CLS]', 'is', 'so', 'deadly', 'dull', 'that', 'watching', 'the', 'proverbial', 'paint', 'dry', 'would', 'be', 'a', 'welcome', 'improvement', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.42)</b></text></td><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.54</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> so                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> deadly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dull                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watching                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> proverbial                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paint                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dry                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> welcome                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> improvement                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: to accomplish \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'to', 'accomplish', '[SEP]']\n",
      "attribution.size torch.Size([1, 4, 768])\n",
      "word attr tensor([-0.2798,  0.3581,  0.5793, -0.6766], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.2798, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3581, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.5793, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6766, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'to', 'accomplish', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'to', 'accomplish', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([-0.2798,  0.3581,  0.5793, -0.6766], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9736232161521912\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-2.3343e-05, -2.3361e-05,  1.1454e-04,  ...,  8.9868e-07,\n",
      "           1.2645e-05, -1.7938e-05],\n",
      "         [ 6.1118e-05,  6.1600e-05,  3.6646e-05,  ..., -1.2065e-04,\n",
      "           8.2937e-06,  1.4947e-05],\n",
      "         [ 1.4476e-04,  2.0582e-04, -5.2648e-05,  ..., -1.3833e-05,\n",
      "           5.1991e-05, -7.5593e-06],\n",
      "         [-7.8072e-05,  1.3316e-05, -8.9833e-06,  ...,  2.0717e-05,\n",
      "          -7.5088e-06, -2.2836e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.01900845766067505\n",
      "raw_input ['[CLS]', 'to', 'accomplish', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.02</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> accomplish                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: does n't work . \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'does', 'n', \"'\", 't', 'work', '.', '[SEP]']\n",
      "attribution.size torch.Size([1, 8, 768])\n",
      "word attr tensor([-0.1869, -0.0428,  0.8030, -0.1265, -0.5281, -0.0029,  0.1533, -0.0024],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1869, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0428, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0949, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0029, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1533, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0024, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'does', 'n', \"'\", 't', 'work', '.', '[SEP]']\n",
      "len conti_raw 6\n",
      "conti_raw ['[CLS]', 'does', \"n't\", 'work', '.', '[SEP]']\n",
      "conti_attri_size: torch.Size([6]), raw_input_len:  6\n",
      "word attributions tensor([-0.1869, -0.0428,  0.8030, -0.1265, -0.5281, -0.0029,  0.1533, -0.0024],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.04155701398849487\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 2.1942e-05,  1.2470e-05, -1.5882e-06,  ..., -1.5837e-06,\n",
      "           2.9576e-06, -1.3241e-06],\n",
      "         [ 7.2940e-05, -1.6574e-05,  2.2394e-05,  ...,  2.1936e-06,\n",
      "          -1.1238e-05,  1.0915e-05],\n",
      "         [ 1.0305e-06,  2.0906e-05, -7.3598e-05,  ..., -5.0954e-06,\n",
      "          -4.7961e-06, -4.0087e-05],\n",
      "         ...,\n",
      "         [-5.4147e-05,  1.8609e-05,  9.3747e-07,  ...,  1.6975e-05,\n",
      "           4.0806e-07,  2.1564e-06],\n",
      "         [-7.7567e-06, -2.0813e-06,  5.2366e-06,  ...,  6.0409e-06,\n",
      "          -6.0292e-06,  8.4612e-06],\n",
      "         [-4.4959e-06,  7.2653e-06, -2.4047e-06,  ..., -3.2687e-05,\n",
      "          -9.0924e-07, -3.0917e-07]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.06671741604804993\n",
      "raw_input ['[CLS]', 'does', \"n't\", 'work', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.04)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.07</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> does                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> n't                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> work                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: given it a one-star rating \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'given', 'it', 'a', 'one', '-', 'star', 'rating', '[SEP]']\n",
      "attribution.size torch.Size([1, 9, 768])\n",
      "word attr tensor([ 0.4724, -0.3102, -0.1172,  0.0596, -0.0975,  0.4457, -0.6419, -0.0911,\n",
      "        -0.1865], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4724, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3102, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1172, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0596, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2339, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0911, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1865, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'given', 'it', 'a', 'one', '-', 'star', 'rating', '[SEP]']\n",
      "len conti_raw 7\n",
      "conti_raw ['[CLS]', 'given', 'it', 'a', 'one-star', 'rating', '[SEP]']\n",
      "conti_attri_size: torch.Size([7]), raw_input_len:  7\n",
      "word attributions tensor([ 0.4724, -0.3102, -0.1172,  0.0596, -0.0975,  0.4457, -0.6419, -0.0911,\n",
      "        -0.1865], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.7330113053321838\n",
      "pred_class Pos\n",
      "true_class Neg\n",
      "attribution tensor([[[ 9.9116e-04, -2.7847e-03,  4.7868e-03,  ..., -3.3443e-04,\n",
      "          -7.1248e-05, -2.9241e-04],\n",
      "         [-7.5598e-03, -5.8167e-05, -1.5867e-03,  ..., -1.0491e-02,\n",
      "          -6.8996e-04,  1.2664e-02],\n",
      "         [ 5.5208e-03, -1.4954e-04,  1.7701e-04,  ..., -1.2888e-03,\n",
      "           4.0579e-03,  4.5639e-04],\n",
      "         ...,\n",
      "         [ 5.9983e-03,  5.8060e-03,  8.1891e-03,  ..., -9.0070e-05,\n",
      "           1.1040e-02, -6.5972e-03],\n",
      "         [ 1.8468e-02, -2.0957e-02,  3.5347e-03,  ..., -2.8132e-05,\n",
      "          -8.3085e-03,  2.1958e-02],\n",
      "         [-1.4205e-03,  1.7160e-03, -1.7241e-04,  ..., -5.1928e-03,\n",
      "           1.4986e-03, -1.5349e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.4667588770389557\n",
      "raw_input ['[CLS]', 'given', 'it', 'a', 'one-star', 'rating', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.73)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.47</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> given                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one-star                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rating                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: ice cube is n't quite out of ripe screwball ideas , but friday after next spreads them pretty thin \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'ice', 'cube', 'is', 'n', \"'\", 't', 'quite', 'out', 'of', 'ripe', 'screw', 'ball', 'ideas', ',', 'but', 'friday', 'after', 'next', 'spreads', 'them', 'pretty', 'thin', '[SEP]']\n",
      "attribution.size torch.Size([1, 24, 768])\n",
      "word attr tensor([ 0.1420,  0.1307, -0.1961, -0.0852,  0.0801, -0.1060, -0.0087, -0.0021,\n",
      "        -0.1367,  0.0706,  0.2590,  0.0152, -0.1089, -0.5723, -0.5195,  0.1086,\n",
      "         0.2158, -0.0009,  0.0508, -0.2730,  0.1639, -0.1255,  0.1350, -0.0512],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1420, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1307, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1961, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0852, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0109, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0021, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1367, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0706, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2590, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0469, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.5723, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.5195, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1086, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2158, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0009, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0508, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2730, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1639, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1255, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1350, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0512, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'ice', 'cube', 'is', 'n', \"'\", 't', 'quite', 'out', 'of', 'ripe', 'screw', 'ball', 'ideas', ',', 'but', 'friday', 'after', 'next', 'spreads', 'them', 'pretty', 'thin', '[SEP]']\n",
      "len conti_raw 21\n",
      "conti_raw ['[CLS]', 'ice', 'cube', 'is', \"n't\", 'quite', 'out', 'of', 'ripe', 'screwball', 'ideas', ',', 'but', 'friday', 'after', 'next', 'spreads', 'them', 'pretty', 'thin', '[SEP]']\n",
      "conti_attri_size: torch.Size([21]), raw_input_len:  21\n",
      "word attributions tensor([ 0.1420,  0.1307, -0.1961, -0.0852,  0.0801, -0.1060, -0.0087, -0.0021,\n",
      "        -0.1367,  0.0706,  0.2590,  0.0152, -0.1089, -0.5723, -0.5195,  0.1086,\n",
      "         0.2158, -0.0009,  0.0508, -0.2730,  0.1639, -0.1255,  0.1350, -0.0512],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.04169812798500061\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 2.6635e-05,  1.1284e-04, -1.1036e-04,  ...,  6.3797e-07,\n",
      "          -9.5204e-06, -1.1014e-05],\n",
      "         [ 2.8110e-06, -8.1474e-07, -2.4583e-05,  ..., -7.2258e-06,\n",
      "          -1.0113e-04,  1.5770e-04],\n",
      "         [-3.4061e-05, -7.3432e-05,  2.1656e-04,  ...,  1.9766e-05,\n",
      "           6.5011e-06, -7.1494e-06],\n",
      "         ...,\n",
      "         [ 4.4248e-06, -3.1386e-05,  3.3340e-05,  ..., -1.1634e-05,\n",
      "          -1.4394e-04, -2.0230e-05],\n",
      "         [-1.2557e-04, -1.4315e-04, -2.6913e-06,  ...,  6.3474e-05,\n",
      "          -1.6742e-04,  2.4762e-05],\n",
      "         [-3.3104e-06,  1.0262e-05,  2.3399e-07,  ...,  1.7745e-07,\n",
      "           9.1472e-06, -6.5062e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.8145967125892639\n",
      "raw_input ['[CLS]', 'ice', 'cube', 'is', \"n't\", 'quite', 'out', 'of', 'ripe', 'screwball', 'ideas', ',', 'but', 'friday', 'after', 'next', 'spreads', 'them', 'pretty', 'thin', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.04)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.81</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ice                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cube                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> n't                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> quite                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> out                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ripe                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> screwball                    </font></mark><mark style=\"background-color: hsl(0, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ideas                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> friday                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> after                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> next                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spreads                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> them                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pretty                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thin                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: been trying to forget \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'been', 'trying', 'to', 'forget', '[SEP]']\n",
      "attribution.size torch.Size([1, 6, 768])\n",
      "word attr tensor([ 0.2060, -0.4474, -0.0405,  0.3503,  0.4659, -0.6450], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2060, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4474, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0405, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3503, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4659, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6450, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'been', 'trying', 'to', 'forget', '[SEP]']\n",
      "len conti_raw 6\n",
      "conti_raw ['[CLS]', 'been', 'trying', 'to', 'forget', '[SEP]']\n",
      "conti_attri_size: torch.Size([6]), raw_input_len:  6\n",
      "word attributions tensor([ 0.2060, -0.4474, -0.0405,  0.3503,  0.4659, -0.6450], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.06650674343109131\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-8.3760e-05, -3.2577e-07, -7.2443e-05,  ..., -6.9045e-06,\n",
      "          -1.1266e-04,  2.3770e-04],\n",
      "         [ 3.5900e-04,  1.0539e-03,  7.4832e-05,  ..., -2.6654e-04,\n",
      "           8.5969e-04, -4.2134e-04],\n",
      "         [ 4.8394e-04,  4.7084e-04, -3.5098e-04,  ...,  1.6913e-04,\n",
      "           2.4788e-04,  8.9316e-04],\n",
      "         [-1.6766e-04, -1.0352e-04,  1.3098e-06,  ...,  4.4922e-05,\n",
      "          -7.7971e-05, -1.3330e-04],\n",
      "         [-7.0681e-04,  2.6462e-03,  7.4123e-05,  ..., -2.4295e-05,\n",
      "           7.6408e-04, -4.0252e-05],\n",
      "         [-1.6396e-04,  2.5793e-04, -1.2306e-04,  ..., -6.0981e-05,\n",
      "          -3.6281e-04,  1.6424e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.11065071821212769\n",
      "raw_input ['[CLS]', 'been', 'trying', 'to', 'forget', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.07)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.11</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> trying                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> forget                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: big stars and \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'big', 'stars', 'and', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.4711, -0.6442,  0.1502, -0.5694,  0.1276], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4711, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6442, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1502, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.5694, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1276, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'big', 'stars', 'and', '[SEP]']\n",
      "len conti_raw 5\n",
      "conti_raw ['[CLS]', 'big', 'stars', 'and', '[SEP]']\n",
      "conti_attri_size: torch.Size([5]), raw_input_len:  5\n",
      "word attributions tensor([ 0.4711, -0.6442,  0.1502, -0.5694,  0.1276], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9741768836975098\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-3.2368e-05, -4.2201e-05,  1.1938e-06,  ..., -2.3166e-06,\n",
      "          -1.9747e-06,  8.5891e-07],\n",
      "         [-4.9124e-06, -1.2696e-04, -5.6553e-05,  ...,  3.9120e-06,\n",
      "          -1.8003e-04, -1.0751e-05],\n",
      "         [ 6.1404e-05, -5.1238e-05, -1.8792e-04,  ..., -1.8766e-06,\n",
      "          -2.9393e-05, -1.9601e-05],\n",
      "         [ 8.4486e-06, -2.7757e-05, -7.4137e-06,  ..., -4.1251e-05,\n",
      "          -1.0682e-05,  2.1416e-05],\n",
      "         [ 5.0507e-05,  1.2932e-06,  9.9915e-06,  ...,  8.1489e-06,\n",
      "           2.1811e-05, -4.6258e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.4647141695022583\n",
      "raw_input ['[CLS]', 'big', 'stars', 'and', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.46</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> big                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stars                    </font></mark><mark style=\"background-color: hsl(0, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: of cheesy dialogue \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'of', 'che', 'es', 'y', 'dialogue', '[SEP]']\n",
      "attribution.size torch.Size([1, 7, 768])\n",
      "word attr tensor([-0.3926,  0.0644,  0.2808, -0.3034,  0.2048, -0.7918, -0.0452],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.3926, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0644, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0967, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.7918, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0452, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'of', 'che', 'es', 'y', 'dialogue', '[SEP]']\n",
      "len conti_raw 5\n",
      "conti_raw ['[CLS]', 'of', 'cheesy', 'dialogue', '[SEP]']\n",
      "conti_attri_size: torch.Size([5]), raw_input_len:  5\n",
      "word attributions tensor([-0.3926,  0.0644,  0.2808, -0.3034,  0.2048, -0.7918, -0.0452],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.02389264479279518\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-1.9801e-05, -9.0758e-06,  1.0570e-05,  ...,  5.7816e-06,\n",
      "          -4.7012e-06,  1.9117e-05],\n",
      "         [-6.2349e-06, -2.2102e-06, -1.2051e-06,  ...,  3.0041e-05,\n",
      "           4.1574e-05, -3.1511e-06],\n",
      "         [ 3.9155e-05, -1.3769e-05,  3.3695e-06,  ..., -5.4160e-05,\n",
      "          -2.3346e-06, -1.2024e-05],\n",
      "         ...,\n",
      "         [-4.1446e-06,  5.7195e-06,  2.3660e-05,  ..., -1.9267e-06,\n",
      "           1.4023e-06, -2.8339e-05],\n",
      "         [ 4.9730e-05, -1.6036e-05, -1.1638e-05,  ..., -1.9036e-05,\n",
      "           1.3651e-05, -2.9961e-05],\n",
      "         [-6.0888e-07, -1.2886e-06, -6.4118e-06,  ..., -1.1765e-05,\n",
      "           3.0122e-06, -3.2444e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.9829576015472412\n",
      "raw_input ['[CLS]', 'of', 'cheesy', 'dialogue', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.02)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.98</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cheesy                    </font></mark><mark style=\"background-color: hsl(0, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dialogue                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: the château is never quite able to overcome the cultural moat surrounding its ludicrous and contrived plot . ' \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'the', 'chateau', 'is', 'never', 'quite', 'able', 'to', 'overcome', 'the', 'cultural', 'moat', 'surrounding', 'its', 'lu', 'dic', 'rous', 'and', 'con', 'tri', 'ved', 'plot', '.', \"'\", '[SEP]']\n",
      "attribution.size torch.Size([1, 25, 768])\n",
      "word attr tensor([ 0.0122,  0.0127, -0.1748,  0.1025,  0.2031, -0.1094,  0.0943, -0.0688,\n",
      "         0.0498,  0.0519, -0.0563,  0.0028, -0.0648, -0.0082,  0.1927, -0.4046,\n",
      "        -0.2579,  0.1419, -0.3607, -0.0593,  0.4121, -0.1031, -0.3353,  0.2412,\n",
      "        -0.3273], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0122, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0127, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1748, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1025, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2031, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1094, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0943, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0688, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0498, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0519, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0563, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0028, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0648, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0082, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1819, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1419, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1011, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.1031, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3353, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2412, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3273, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'the', 'chateau', 'is', 'never', 'quite', 'able', 'to', 'overcome', 'the', 'cultural', 'moat', 'surrounding', 'its', 'lu', 'dic', 'rous', 'and', 'con', 'tri', 'ved', 'plot', '.', \"'\", '[SEP]']\n",
      "len conti_raw 21\n",
      "conti_raw ['[CLS]', 'the', 'chateau', 'is', 'never', 'quite', 'able', 'to', 'overcome', 'the', 'cultural', 'moat', 'surrounding', 'its', 'ludicrous', 'and', 'contrived', 'plot', '.', \"'\", '[SEP]']\n",
      "conti_attri_size: torch.Size([21]), raw_input_len:  21\n",
      "word attributions tensor([ 0.0122,  0.0127, -0.1748,  0.1025,  0.2031, -0.1094,  0.0943, -0.0688,\n",
      "         0.0498,  0.0519, -0.0563,  0.0028, -0.0648, -0.0082,  0.1927, -0.4046,\n",
      "        -0.2579,  0.1419, -0.3607, -0.0593,  0.4121, -0.1031, -0.3353,  0.2412,\n",
      "        -0.3273], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.060439176857471466\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-5.9042e-05,  1.8278e-04,  8.5474e-05,  ...,  1.0801e-05,\n",
      "           3.8661e-06, -4.0562e-05],\n",
      "         [ 9.7180e-05,  5.7750e-05,  3.2696e-06,  ..., -2.9894e-06,\n",
      "          -1.2729e-05,  6.8346e-06],\n",
      "         [ 1.7774e-05, -2.1695e-05,  2.5749e-05,  ...,  3.3581e-05,\n",
      "          -7.5368e-05,  2.8521e-04],\n",
      "         ...,\n",
      "         [ 1.6308e-05, -5.2560e-05,  8.2617e-05,  ...,  3.0120e-05,\n",
      "           8.8284e-05,  9.9966e-05],\n",
      "         [ 5.7471e-05, -1.9445e-04, -6.3393e-05,  ..., -2.6926e-04,\n",
      "           6.7456e-05,  5.6434e-04],\n",
      "         [ 2.3326e-05, -4.8799e-05, -2.7280e-05,  ...,  3.6866e-05,\n",
      "           3.0678e-05,  1.4843e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.8130491971969604\n",
      "raw_input ['[CLS]', 'the', 'chateau', 'is', 'never', 'quite', 'able', 'to', 'overcome', 'the', 'cultural', 'moat', 'surrounding', 'its', 'ludicrous', 'and', 'contrived', 'plot', '.', \"'\", '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.06)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.81</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chateau                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> never                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> quite                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> able                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> overcome                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cultural                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> moat                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> surrounding                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ludicrous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> contrived                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plot                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: principled \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'principle', 'd', '[SEP]']\n",
      "attribution.size torch.Size([1, 4, 768])\n",
      "word attr tensor([ 0.8245, -0.3863,  0.3266, -0.2536], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.8245, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0298, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.2536, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'principle', 'd', '[SEP]']\n",
      "len conti_raw 3\n",
      "conti_raw ['[CLS]', 'principled', '[SEP]']\n",
      "conti_attri_size: torch.Size([3]), raw_input_len:  3\n",
      "word attributions tensor([ 0.8245, -0.3863,  0.3266, -0.2536], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9714932441711426\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-8.6671e-07, -2.2054e-05,  1.6083e-05,  ...,  3.0221e-06,\n",
      "          -1.0206e-06,  3.5013e-05],\n",
      "         [-7.6407e-05,  2.2034e-05, -1.0459e-04,  ...,  1.1570e-05,\n",
      "          -2.7344e-06,  4.1473e-05],\n",
      "         [ 1.3018e-05, -2.7149e-05, -4.5132e-05,  ...,  9.2963e-05,\n",
      "          -2.3813e-05,  1.9574e-05],\n",
      "         [ 1.4112e-05, -1.8912e-05, -5.7022e-06,  ...,  2.3022e-05,\n",
      "          -8.0425e-08, -2.2106e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Pos\n",
      "attr_score 0.5112422704696655\n",
      "raw_input ['[CLS]', 'principled', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.51</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> principled                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: end it all by stuffing himself into an electric pencil sharpener \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'end', 'it', 'all', 'by', 'stuffing', 'himself', 'into', 'an', 'electric', 'pencil', 'sharpe', 'ner', '[SEP]']\n",
      "attribution.size torch.Size([1, 14, 768])\n",
      "word attr tensor([ 0.5746, -0.0817, -0.3704, -0.0077, -0.5246,  0.0094,  0.0275, -0.1248,\n",
      "        -0.3190, -0.0629, -0.0019, -0.2555,  0.2359, -0.0876], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.5746, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0817, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3704, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0077, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.5246, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0094, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0275, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1248, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3190, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0629, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0019, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0098, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0876, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'end', 'it', 'all', 'by', 'stuffing', 'himself', 'into', 'an', 'electric', 'pencil', 'sharpe', 'ner', '[SEP]']\n",
      "len conti_raw 13\n",
      "conti_raw ['[CLS]', 'end', 'it', 'all', 'by', 'stuffing', 'himself', 'into', 'an', 'electric', 'pencil', 'sharpener', '[SEP]']\n",
      "conti_attri_size: torch.Size([13]), raw_input_len:  13\n",
      "word attributions tensor([ 0.5746, -0.0817, -0.3704, -0.0077, -0.5246,  0.0094,  0.0275, -0.1248,\n",
      "        -0.3190, -0.0629, -0.0019, -0.2555,  0.2359, -0.0876], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.06462432444095612\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-2.9817e-04,  2.3735e-05, -1.7401e-04,  ..., -3.8237e-07,\n",
      "          -1.6468e-05, -1.2356e-04],\n",
      "         [-1.3887e-05, -2.2484e-04, -5.7021e-05,  ...,  4.7934e-05,\n",
      "           7.7219e-06,  1.0408e-04],\n",
      "         [ 6.2319e-05, -1.0129e-05,  1.2506e-06,  ..., -1.7780e-04,\n",
      "           3.1702e-05, -3.1370e-05],\n",
      "         ...,\n",
      "         [ 8.8288e-06, -4.9111e-05, -1.3808e-04,  ..., -2.1250e-04,\n",
      "          -4.8968e-06, -1.3757e-05],\n",
      "         [-6.3140e-05, -1.0999e-06,  8.1773e-05,  ..., -3.4984e-05,\n",
      "           1.0493e-05, -2.7084e-05],\n",
      "         [ 1.9229e-04,  7.5351e-07,  1.3233e-05,  ..., -1.8215e-05,\n",
      "           7.2056e-06,  1.0230e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.9888148307800293\n",
      "raw_input ['[CLS]', 'end', 'it', 'all', 'by', 'stuffing', 'himself', 'into', 'an', 'electric', 'pencil', 'sharpener', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.06)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.99</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> end                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stuffing                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> himself                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> into                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> electric                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pencil                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sharpener                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: funniest idea \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'fun', 'nies', 't', 'idea', '[SEP]']\n",
      "attribution.size torch.Size([1, 6, 768])\n",
      "word attr tensor([-0.1961, -0.0024, -0.2971, -0.5678, -0.3823,  0.6362], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1961, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3588, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.3823, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.6362, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'fun', 'nies', 't', 'idea', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'funniest', 'idea', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([-0.1961, -0.0024, -0.2971, -0.5678, -0.3823,  0.6362], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9628397226333618\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-6.6441e-05, -8.3544e-05,  1.7784e-04,  ...,  1.0857e-05,\n",
      "          -8.9894e-06,  1.0729e-06],\n",
      "         [-3.5605e-04,  3.8828e-06, -1.7129e-05,  ..., -2.2378e-05,\n",
      "          -5.0690e-05,  4.4922e-07],\n",
      "         [ 8.6835e-06,  1.0296e-04, -2.8976e-04,  ...,  7.8794e-05,\n",
      "          -9.1170e-05,  8.8553e-05],\n",
      "         [ 2.4934e-04, -5.6335e-06, -1.2534e-04,  ..., -9.2085e-05,\n",
      "           1.1307e-05, -1.3900e-05],\n",
      "         [-5.5930e-05, -8.4604e-05,  8.2236e-05,  ...,  1.5878e-06,\n",
      "           2.3311e-05,  1.6142e-04],\n",
      "         [ 1.4131e-04, -7.2575e-06,  2.5924e-05,  ...,  6.2854e-05,\n",
      "           9.5706e-05, -2.9407e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.8095016479492188\n",
      "raw_input ['[CLS]', 'funniest', 'idea', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.96)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.81</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> funniest                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> idea                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: silly and tedious \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'silly', 'and', 'ted', 'ious', '[SEP]']\n",
      "attribution.size torch.Size([1, 6, 768])\n",
      "word attr tensor([-0.2907,  0.4455,  0.0885, -0.6108,  0.4324, -0.3862], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.2907, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4455, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0885, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0892, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.3862, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'silly', 'and', 'ted', 'ious', '[SEP]']\n",
      "len conti_raw 5\n",
      "conti_raw ['[CLS]', 'silly', 'and', 'tedious', '[SEP]']\n",
      "conti_attri_size: torch.Size([5]), raw_input_len:  5\n",
      "word attributions tensor([-0.2907,  0.4455,  0.0885, -0.6108,  0.4324, -0.3862], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.035474635660648346\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-2.0472e-04, -1.2286e-04,  1.5446e-05,  ...,  1.4780e-05,\n",
      "          -4.2645e-06,  7.6638e-05],\n",
      "         [ 8.4665e-05, -1.1964e-04,  2.3985e-04,  ..., -2.8419e-04,\n",
      "           1.0989e-04,  7.3927e-05],\n",
      "         [ 2.1969e-04,  1.5050e-05,  1.1233e-06,  ...,  2.8086e-06,\n",
      "          -8.9091e-05,  8.1865e-05],\n",
      "         [-3.9292e-04,  5.7023e-04, -5.3597e-05,  ..., -8.6774e-05,\n",
      "          -2.1747e-04,  4.5271e-04],\n",
      "         [-6.8893e-04, -3.5115e-04,  3.6147e-05,  ..., -5.2622e-05,\n",
      "          -3.3489e-04, -1.5096e-04],\n",
      "         [ 8.4317e-05, -4.3053e-05, -2.0074e-05,  ...,  5.0146e-05,\n",
      "          -1.7643e-05, -2.8070e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.3212001919746399\n",
      "raw_input ['[CLS]', 'silly', 'and', 'tedious', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.04)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.32</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> silly                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tedious                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: two surefire , beloved genres \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'two', 'sure', 'fire', ',', 'beloved', 'genres', '[SEP]']\n",
      "attribution.size torch.Size([1, 8, 768])\n",
      "word attr tensor([-0.3083, -0.2355,  0.5674, -0.3321, -0.3376, -0.3643,  0.2075,  0.3572],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.3083, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2355, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1176, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.3376, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3643, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2075, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3572, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'two', 'sure', 'fire', ',', 'beloved', 'genres', '[SEP]']\n",
      "len conti_raw 7\n",
      "conti_raw ['[CLS]', 'two', 'surefire', ',', 'beloved', 'genres', '[SEP]']\n",
      "conti_attri_size: torch.Size([7]), raw_input_len:  7\n",
      "word attributions tensor([-0.3083, -0.2355,  0.5674, -0.3321, -0.3376, -0.3643,  0.2075,  0.3572],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9750030040740967\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-1.5841e-05, -4.0055e-05,  1.3962e-05,  ...,  1.5808e-06,\n",
      "          -6.9240e-07,  1.3099e-05],\n",
      "         [-2.4925e-06,  1.9772e-06, -2.3693e-08,  ..., -2.2529e-06,\n",
      "          -3.9616e-06,  1.5391e-05],\n",
      "         [ 3.3581e-05, -1.8613e-06,  6.1800e-06,  ...,  1.2109e-05,\n",
      "          -3.9117e-06, -4.8674e-06],\n",
      "         ...,\n",
      "         [ 1.0057e-04,  2.3435e-06, -1.6810e-05,  ...,  6.9540e-05,\n",
      "           4.6464e-05,  1.4025e-05],\n",
      "         [-5.0036e-05,  3.3869e-05, -9.0419e-08,  ...,  3.5055e-06,\n",
      "          -1.1748e-04, -2.5713e-05],\n",
      "         [ 1.5728e-05, -2.8351e-06, -4.4458e-06,  ...,  4.8946e-06,\n",
      "          -1.6714e-06, -4.1742e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.4457169473171234\n",
      "raw_input ['[CLS]', 'two', 'surefire', ',', 'beloved', 'genres', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.98)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.45</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> surefire                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> beloved                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> genres                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: most charmless \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'most', 'charm', 'less', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([-0.1707, -0.6284,  0.4402, -0.0503,  0.6161], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1707, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6284, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1950, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.6161, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'most', 'charm', 'less', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'most', 'charmless', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([-0.1707, -0.6284,  0.4402, -0.0503,  0.6161], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9686804413795471\n",
      "pred_class Pos\n",
      "true_class Neg\n",
      "attribution tensor([[[-5.7838e-05, -1.0294e-04,  8.6529e-05,  ...,  1.0215e-05,\n",
      "          -2.8571e-05, -5.1206e-05],\n",
      "         [-9.8080e-05, -4.1954e-04, -3.2463e-05,  ..., -3.3777e-04,\n",
      "          -8.4599e-05,  1.5167e-05],\n",
      "         [ 1.4036e-05,  5.5807e-04, -1.6762e-04,  ..., -1.0826e-03,\n",
      "           2.3765e-03, -8.7056e-04],\n",
      "         [ 1.0250e-04, -5.4617e-05, -1.9594e-04,  ...,  2.7668e-05,\n",
      "          -8.4031e-04,  3.0026e-04],\n",
      "         [ 9.2369e-07, -1.9258e-05,  2.9939e-05,  ..., -1.1834e-05,\n",
      "           3.0345e-05, -9.9724e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.2069040834903717\n",
      "raw_input ['[CLS]', 'most', 'charmless', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.21</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> charmless                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: offers us the sense that on some elemental level , lilia deeply wants to break free of her old life . \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'offers', 'us', 'the', 'sense', 'that', 'on', 'some', 'elemental', 'level', ',', 'lil', 'ia', 'deeply', 'wants', 'to', 'break', 'free', 'of', 'her', 'old', 'life', '.', '[SEP]']\n",
      "attribution.size torch.Size([1, 24, 768])\n",
      "word attr tensor([-0.1206,  0.1659, -0.1774,  0.4294,  0.2112,  0.1563, -0.1412, -0.2674,\n",
      "        -0.3360,  0.1312,  0.0867,  0.1658, -0.1641, -0.1360, -0.1888,  0.2256,\n",
      "         0.0395,  0.0324,  0.0661, -0.1398,  0.0301,  0.4908, -0.0743, -0.1034],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1206, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1659, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1774, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4294, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2112, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1563, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1412, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2674, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3360, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1312, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0867, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.1360, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1888, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2256, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0395, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0324, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0661, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1398, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0301, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4908, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0743, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1034, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'offers', 'us', 'the', 'sense', 'that', 'on', 'some', 'elemental', 'level', ',', 'lil', 'ia', 'deeply', 'wants', 'to', 'break', 'free', 'of', 'her', 'old', 'life', '.', '[SEP]']\n",
      "len conti_raw 23\n",
      "conti_raw ['[CLS]', 'offers', 'us', 'the', 'sense', 'that', 'on', 'some', 'elemental', 'level', ',', 'lilia', 'deeply', 'wants', 'to', 'break', 'free', 'of', 'her', 'old', 'life', '.', '[SEP]']\n",
      "conti_attri_size: torch.Size([23]), raw_input_len:  23\n",
      "word attributions tensor([-0.1206,  0.1659, -0.1774,  0.4294,  0.2112,  0.1563, -0.1412, -0.2674,\n",
      "        -0.3360,  0.1312,  0.0867,  0.1658, -0.1641, -0.1360, -0.1888,  0.2256,\n",
      "         0.0395,  0.0324,  0.0661, -0.1398,  0.0301,  0.4908, -0.0743, -0.1034],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9634529948234558\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[-1.8323e-05, -6.9529e-05,  5.0805e-05,  ...,  3.4530e-06,\n",
      "          -2.6187e-06,  1.0072e-05],\n",
      "         [ 2.9681e-05,  1.8740e-06,  4.4954e-05,  ..., -8.2763e-06,\n",
      "          -8.1420e-05, -1.4135e-05],\n",
      "         [ 1.9085e-05, -4.4207e-06,  4.9691e-05,  ..., -1.9821e-04,\n",
      "          -6.2178e-05, -9.3180e-05],\n",
      "         ...,\n",
      "         [-3.8878e-05,  1.0112e-04, -8.4732e-06,  ...,  9.2188e-06,\n",
      "          -1.0378e-05,  2.2695e-05],\n",
      "         [ 9.6987e-06,  3.3312e-06, -5.6509e-06,  ...,  3.3357e-05,\n",
      "           2.5269e-05,  3.7695e-05],\n",
      "         [ 1.9542e-05,  2.9475e-06, -4.5791e-07,  ..., -1.0965e-05,\n",
      "          -2.7434e-06, -3.0567e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.38221117854118347\n",
      "raw_input ['[CLS]', 'offers', 'us', 'the', 'sense', 'that', 'on', 'some', 'elemental', 'level', ',', 'lilia', 'deeply', 'wants', 'to', 'break', 'free', 'of', 'her', 'old', 'life', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.96)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> offers                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sense                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> elemental                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> level                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lilia                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> deeply                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> wants                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> break                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> free                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> old                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> life                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: might be best forgotten \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'might', 'be', 'best', 'forgotten', '[SEP]']\n",
      "attribution.size torch.Size([1, 6, 768])\n",
      "word attr tensor([ 0.1531, -0.1980, -0.6500, -0.6200,  0.0287,  0.3600], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1531, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1980, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6500, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.6200, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0287, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3600, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'might', 'be', 'best', 'forgotten', '[SEP]']\n",
      "len conti_raw 6\n",
      "conti_raw ['[CLS]', 'might', 'be', 'best', 'forgotten', '[SEP]']\n",
      "conti_attri_size: torch.Size([6]), raw_input_len:  6\n",
      "word attributions tensor([ 0.1531, -0.1980, -0.6500, -0.6200,  0.0287,  0.3600], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.06778600811958313\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-4.6103e-04, -3.0831e-04, -2.5143e-04,  ..., -4.3445e-05,\n",
      "          -1.6570e-04,  8.0589e-06],\n",
      "         [-1.7042e-04, -3.8339e-04, -4.1214e-04,  ...,  1.4887e-05,\n",
      "          -4.3873e-05,  2.0356e-03],\n",
      "         [ 1.6719e-05, -8.6997e-05,  1.6819e-04,  ..., -6.8126e-04,\n",
      "           1.5002e-04,  8.5864e-05],\n",
      "         [-6.9395e-05,  3.0101e-04,  6.4874e-04,  ..., -7.4524e-05,\n",
      "          -4.9351e-04,  1.2174e-04],\n",
      "         [-1.0115e-05,  6.6065e-04, -1.1478e-04,  ...,  3.1142e-05,\n",
      "          -1.4346e-04,  9.7698e-05],\n",
      "         [ 3.8689e-05,  3.0805e-05,  9.5421e-05,  ..., -2.5014e-04,\n",
      "          -5.3173e-04, -7.6428e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.9262599349021912\n",
      "raw_input ['[CLS]', 'might', 'be', 'best', 'forgotten', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.07)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.93</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> might                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> forgotten                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: a substantial arc of change that does n't produce any real transformation \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'a', 'substantial', 'arc', 'of', 'change', 'that', 'does', 'n', \"'\", 't', 'produce', 'any', 'real', 'transformation', '[SEP]']\n",
      "attribution.size torch.Size([1, 16, 768])\n",
      "word attr tensor([-0.0196,  0.0819, -0.1579, -0.0174, -0.0503, -0.0753,  0.0985,  0.0941,\n",
      "        -0.2184, -0.8397,  0.3600, -0.1111,  0.1855,  0.0874,  0.0531, -0.0363],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0196, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0819, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1579, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0174, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0503, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0753, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0985, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0941, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0846, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.1111, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1855, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0874, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0531, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0363, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'a', 'substantial', 'arc', 'of', 'change', 'that', 'does', 'n', \"'\", 't', 'produce', 'any', 'real', 'transformation', '[SEP]']\n",
      "len conti_raw 14\n",
      "conti_raw ['[CLS]', 'a', 'substantial', 'arc', 'of', 'change', 'that', 'does', \"n't\", 'produce', 'any', 'real', 'transformation', '[SEP]']\n",
      "conti_attri_size: torch.Size([14]), raw_input_len:  14\n",
      "word attributions tensor([-0.0196,  0.0819, -0.1579, -0.0174, -0.0503, -0.0753,  0.0985,  0.0941,\n",
      "        -0.2184, -0.8397,  0.3600, -0.1111,  0.1855,  0.0874,  0.0531, -0.0363],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.0436873659491539\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 2.7197e-05,  1.1296e-04, -8.5880e-05,  ...,  4.6198e-06,\n",
      "           9.6052e-06,  2.1354e-05],\n",
      "         [ 2.6142e-05,  2.4231e-05,  2.2442e-06,  ...,  1.9072e-05,\n",
      "           2.3877e-05, -2.1547e-05],\n",
      "         [ 2.5393e-06,  7.1075e-05, -3.2784e-05,  ..., -9.2138e-06,\n",
      "          -1.0792e-05, -2.0384e-05],\n",
      "         ...,\n",
      "         [ 8.6237e-08, -2.4571e-05, -1.1066e-05,  ...,  8.4661e-05,\n",
      "          -3.7210e-06, -9.6249e-07],\n",
      "         [-4.2932e-05, -1.1937e-04, -1.0635e-04,  ..., -1.1659e-04,\n",
      "          -5.3850e-06, -4.8406e-05],\n",
      "         [ 3.0086e-05,  1.3992e-05,  1.7960e-05,  ...,  2.4470e-06,\n",
      "          -9.3841e-06, -4.8605e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.5655394792556763\n",
      "raw_input ['[CLS]', 'a', 'substantial', 'arc', 'of', 'change', 'that', 'does', \"n't\", 'produce', 'any', 'real', 'transformation', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.04)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.57</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> substantial                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> arc                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> change                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> does                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> n't                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> produce                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> any                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> real                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> transformation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: unadorned \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'una', 'dor', 'ned', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([-0.2473,  0.3455, -0.7060,  0.5618, -0.0734], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.2473, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.1908, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0734, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'una', 'dor', 'ned', '[SEP]']\n",
      "len conti_raw 3\n",
      "conti_raw ['[CLS]', 'unadorned', '[SEP]']\n",
      "conti_attri_size: torch.Size([3]), raw_input_len:  3\n",
      "word attributions tensor([-0.2473,  0.3455, -0.7060,  0.5618, -0.0734], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.058859918266534805\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-6.5135e-04, -7.9913e-04,  5.6523e-04,  ...,  3.2998e-05,\n",
      "          -5.6369e-05,  2.1507e-04],\n",
      "         [-7.1676e-04, -7.2108e-04,  6.6194e-04,  ..., -3.9838e-05,\n",
      "          -3.8586e-04,  5.9887e-06],\n",
      "         [-8.3998e-04,  1.5451e-04, -1.9032e-04,  ..., -2.0753e-04,\n",
      "          -3.3329e-05, -2.6720e-03],\n",
      "         [ 5.6651e-04,  3.4404e-03, -2.5126e-04,  ..., -5.3545e-04,\n",
      "           7.0895e-04,  2.5596e-04],\n",
      "         [-6.4728e-04,  1.5006e-04, -6.9665e-05,  ...,  1.3534e-04,\n",
      "           2.9914e-05,  2.2283e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.1193678081035614\n",
      "raw_input ['[CLS]', 'unadorned', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.06)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.12</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> unadorned                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: this is the kind of movie that you only need to watch for about thirty seconds before you say to yourself , ` ah , yes , here we have a bad , bad , bad movie . ' \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'this', 'is', 'the', 'kind', 'of', 'movie', 'that', 'you', 'only', 'need', 'to', 'watch', 'for', 'about', 'thirty', 'seconds', 'before', 'you', 'say', 'to', 'yourself', ',', '`', 'ah', ',', 'yes', ',', 'here', 'we', 'have', 'a', 'bad', ',', 'bad', ',', 'bad', 'movie', '.', \"'\", '[SEP]']\n",
      "attribution.size torch.Size([1, 41, 768])\n",
      "word attr tensor([ 0.2111,  0.0282, -0.0291, -0.0099, -0.0414,  0.0400,  0.0463, -0.0331,\n",
      "        -0.0307,  0.0427,  0.0505, -0.0328, -0.1487,  0.0187,  0.0150,  0.0087,\n",
      "         0.0728,  0.0123, -0.1122, -0.0820, -0.1369, -0.0229, -0.0151,  0.0106,\n",
      "         0.0525, -0.1429, -0.0809,  0.0642, -0.0849, -0.1381, -0.3035,  0.0018,\n",
      "         0.2621,  0.0418,  0.5613, -0.2263,  0.0905, -0.2437, -0.4391,  0.0461,\n",
      "        -0.1487], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2111, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0282, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0291, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0099, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0414, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0400, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0463, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0331, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0307, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0427, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0505, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0328, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1487, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0187, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0150, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0087, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0728, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0123, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1122, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0820, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1369, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0229, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0151, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0106, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0525, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1429, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0809, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0642, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.0849, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1381, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.3035, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0018, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.2621, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0418, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.5613, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2263, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0905, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2437, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4391, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0461, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.1487, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'this', 'is', 'the', 'kind', 'of', 'movie', 'that', 'you', 'only', 'need', 'to', 'watch', 'for', 'about', 'thirty', 'seconds', 'before', 'you', 'say', 'to', 'yourself', ',', '`', 'ah', ',', 'yes', ',', 'here', 'we', 'have', 'a', 'bad', ',', 'bad', ',', 'bad', 'movie', '.', \"'\", '[SEP]']\n",
      "len conti_raw 41\n",
      "conti_raw ['[CLS]', 'this', 'is', 'the', 'kind', 'of', 'movie', 'that', 'you', 'only', 'need', 'to', 'watch', 'for', 'about', 'thirty', 'seconds', 'before', 'you', 'say', 'to', 'yourself', ',', '`', 'ah', ',', 'yes', ',', 'here', 'we', 'have', 'a', 'bad', ',', 'bad', ',', 'bad', 'movie', '.', \"'\", '[SEP]']\n",
      "conti_attri_size: torch.Size([41]), raw_input_len:  41\n",
      "word attributions tensor([ 0.2111,  0.0282, -0.0291, -0.0099, -0.0414,  0.0400,  0.0463, -0.0331,\n",
      "        -0.0307,  0.0427,  0.0505, -0.0328, -0.1487,  0.0187,  0.0150,  0.0087,\n",
      "         0.0728,  0.0123, -0.1122, -0.0820, -0.1369, -0.0229, -0.0151,  0.0106,\n",
      "         0.0525, -0.1429, -0.0809,  0.0642, -0.0849, -0.1381, -0.3035,  0.0018,\n",
      "         0.2621,  0.0418,  0.5613, -0.2263,  0.0905, -0.2437, -0.4391,  0.0461,\n",
      "        -0.1487], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.06825332343578339\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 1.6276e-05,  3.7201e-05,  4.0487e-06,  ..., -2.7480e-06,\n",
      "          -8.3599e-06, -2.9737e-05],\n",
      "         [-2.8363e-05,  7.4823e-06, -2.7392e-06,  ...,  8.3707e-06,\n",
      "           1.7607e-05, -6.3278e-06],\n",
      "         [-1.1679e-06,  2.8555e-06, -5.5088e-06,  ...,  1.9565e-06,\n",
      "           1.0683e-05,  1.9728e-05],\n",
      "         ...,\n",
      "         [-6.9168e-06, -4.7917e-06, -5.3040e-06,  ...,  1.1239e-05,\n",
      "          -4.6062e-06, -3.0246e-05],\n",
      "         [ 1.0028e-05, -7.2306e-06, -6.1426e-06,  ...,  1.5533e-05,\n",
      "           5.5870e-06, -8.0438e-06],\n",
      "         [-1.2198e-06,  5.7928e-07, -1.1956e-06,  ..., -6.3132e-05,\n",
      "           1.3834e-05,  9.4613e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.8254058361053467\n",
      "raw_input ['[CLS]', 'this', 'is', 'the', 'kind', 'of', 'movie', 'that', 'you', 'only', 'need', 'to', 'watch', 'for', 'about', 'thirty', 'seconds', 'before', 'you', 'say', 'to', 'yourself', ',', '`', 'ah', ',', 'yes', ',', 'here', 'we', 'have', 'a', 'bad', ',', 'bad', ',', 'bad', 'movie', '.', \"'\", '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.07)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.83</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kind                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> only                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> need                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watch                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thirty                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seconds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> before                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> you                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> say                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> yourself                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> `                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ah                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> yes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> we                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: feel sanitised and stagey \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'feel', 'san', 'itis', 'ed', 'and', 'stage', 'y', '[SEP]']\n",
      "attribution.size torch.Size([1, 9, 768])\n",
      "word attr tensor([-0.0517, -0.2406, -0.4975,  0.7850,  0.0476,  0.1674, -0.0586,  0.1676,\n",
      "        -0.1177], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0517, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2406, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0957, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1674, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.1177, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'feel', 'san', 'itis', 'ed', 'and', 'stage', 'y', '[SEP]']\n",
      "len conti_raw 6\n",
      "conti_raw ['[CLS]', 'feel', 'sanitised', 'and', 'stagey', '[SEP]']\n",
      "conti_attri_size: torch.Size([6]), raw_input_len:  6\n",
      "word attributions tensor([-0.0517, -0.2406, -0.4975,  0.7850,  0.0476,  0.1674, -0.0586,  0.1676,\n",
      "        -0.1177], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "pred_prob 0.026674073189496994\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 5.1744e-06,  8.4274e-06, -1.9106e-05,  ..., -1.3140e-06,\n",
      "           5.4614e-07,  5.0827e-06],\n",
      "         [ 2.4882e-05, -7.4711e-06,  8.9851e-06,  ..., -8.2522e-06,\n",
      "          -7.4577e-05,  2.1217e-06],\n",
      "         [ 1.5851e-06,  2.2482e-06,  6.5901e-06,  ...,  3.0072e-06,\n",
      "           2.2203e-05, -5.4184e-05],\n",
      "         ...,\n",
      "         [ 5.6668e-05, -2.2350e-05, -1.8512e-04,  ..., -4.5703e-05,\n",
      "          -2.2408e-05, -5.3592e-05],\n",
      "         [-2.8287e-07, -6.9934e-06,  5.6447e-06,  ...,  1.4806e-05,\n",
      "          -4.6104e-06, -8.6863e-06],\n",
      "         [-6.6935e-06,  2.1267e-08, -2.7334e-06,  ..., -9.8862e-06,\n",
      "          -1.5683e-05, -1.5170e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.20147943496704102\n",
      "raw_input ['[CLS]', 'feel', 'sanitised', 'and', 'stagey', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.03)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.20</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> feel                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sanitised                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stagey                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: replete with stereotypical familial quandaries \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'rep', 'lete', 'with', 'stereo', 'typical', 'fa', 'mi', 'lia', 'l', 'quan', 'dar', 'ies', '[SEP]']\n",
      "attribution.size torch.Size([1, 14, 768])\n",
      "word attr tensor([ 0.3305,  0.7048,  0.0519, -0.2189, -0.2533, -0.1536,  0.0497, -0.2567,\n",
      "        -0.3225,  0.1395,  0.0486,  0.2158,  0.0122, -0.1213], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.3305, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.3783, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.2189, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2035, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.0367, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0722, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.1213, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'rep', 'lete', 'with', 'stereo', 'typical', 'fa', 'mi', 'lia', 'l', 'quan', 'dar', 'ies', '[SEP]']\n",
      "len conti_raw 7\n",
      "conti_raw ['[CLS]', 'replete', 'with', 'stereotypical', 'familial', 'quandaries', '[SEP]']\n",
      "conti_attri_size: torch.Size([7]), raw_input_len:  7\n",
      "word attributions tensor([ 0.3305,  0.7048,  0.0519, -0.2189, -0.2533, -0.1536,  0.0497, -0.2567,\n",
      "        -0.3225,  0.1395,  0.0486,  0.2158,  0.0122, -0.1213], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.07909509539604187\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[-4.2301e-04, -6.2074e-04,  1.2504e-03,  ...,  1.5687e-04,\n",
      "          -8.1551e-05,  1.0383e-04],\n",
      "         [ 5.6706e-05,  4.6869e-04,  4.1097e-04,  ...,  7.3573e-04,\n",
      "           9.6138e-04, -2.7538e-03],\n",
      "         [-1.6844e-03,  4.5589e-04,  1.3191e-05,  ...,  1.0686e-03,\n",
      "           5.9927e-05,  1.2070e-04],\n",
      "         ...,\n",
      "         [ 6.9507e-04, -7.4989e-04, -6.1717e-05,  ..., -2.4249e-04,\n",
      "          -1.9341e-03, -3.0731e-05],\n",
      "         [-7.4405e-04,  2.0999e-04,  8.1171e-04,  ..., -1.1786e-03,\n",
      "          -8.2392e-04,  9.0028e-04],\n",
      "         [-1.9715e-03, -4.1713e-05, -4.8657e-05,  ..., -7.8254e-04,\n",
      "          -5.4674e-04,  4.7547e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.22672903537750244\n",
      "raw_input ['[CLS]', 'replete', 'with', 'stereotypical', 'familial', 'quandaries', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.08)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.23</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> replete                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stereotypical                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> familial                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> quandaries                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: quirky comedy \n",
      "GT target: 1\n",
      "detokenized: ['[CLS]', 'qui', 'rky', 'comedy', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.2973,  0.2409,  0.5612, -0.7281,  0.0920], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2973, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.4011, device='cuda:0', grad_fn=<DivBackward0>), tensor(-0.7281, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(0.0920, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'qui', 'rky', 'comedy', '[SEP]']\n",
      "len conti_raw 4\n",
      "conti_raw ['[CLS]', 'quirky', 'comedy', '[SEP]']\n",
      "conti_attri_size: torch.Size([4]), raw_input_len:  4\n",
      "word attributions tensor([ 0.2973,  0.2409,  0.5612, -0.7281,  0.0920], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.9688959121704102\n",
      "pred_class Pos\n",
      "true_class Pos\n",
      "attribution tensor([[[ 1.5813e-06,  5.5474e-05,  5.4438e-05,  ..., -3.5604e-06,\n",
      "          -2.7470e-06,  3.2761e-06],\n",
      "         [ 2.7083e-06,  8.4338e-05,  7.3154e-05,  ...,  9.5819e-06,\n",
      "          -8.7315e-05,  9.8163e-06],\n",
      "         [-1.2681e-05, -5.1503e-05,  1.5754e-05,  ...,  3.9296e-05,\n",
      "          -3.3165e-05,  4.5282e-05],\n",
      "         [-9.0289e-05,  1.7105e-05,  9.2016e-05,  ...,  9.2032e-06,\n",
      "          -1.9556e-05,  6.1270e-05],\n",
      "         [ 2.3776e-05, -8.0215e-06, -2.6170e-06,  ...,  2.0557e-05,\n",
      "          -1.1639e-05,  5.1088e-07]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score 0.4633976221084595\n",
      "raw_input ['[CLS]', 'quirky', 'comedy', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Pos</b></text></td><td><text style=\"padding-right:2em\"><b>Pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>0.46</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> quirky                    </font></mark><mark style=\"background-color: hsl(0, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> comedy                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: unrewarding \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'un', 'rew', 'arding', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.7271, -0.4784, -0.4719, -0.0064,  0.1405], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.7271, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.2407, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.1405, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'un', 'rew', 'arding', '[SEP]']\n",
      "len conti_raw 3\n",
      "conti_raw ['[CLS]', 'unrewarding', '[SEP]']\n",
      "conti_attri_size: torch.Size([3]), raw_input_len:  3\n",
      "word attributions tensor([ 0.7271, -0.4784, -0.4719, -0.0064,  0.1405], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.136697456240654\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 1.7160e-04, -6.7170e-04,  9.2334e-04,  ..., -2.2524e-04,\n",
      "          -3.9245e-04, -1.5112e-03],\n",
      "         [-1.5045e-03, -1.4908e-04,  9.8187e-05,  ...,  1.1406e-03,\n",
      "          -5.8529e-04, -1.9372e-03],\n",
      "         [ 5.1399e-03,  8.5475e-03, -9.7246e-03,  ...,  6.4563e-03,\n",
      "           5.1946e-04,  7.2597e-03],\n",
      "         [-1.0251e-02,  1.1558e-02, -6.6328e-03,  ..., -6.8432e-03,\n",
      "           1.6537e-03,  1.5233e-03],\n",
      "         [-3.3848e-03,  1.1663e-03,  1.5905e-04,  ..., -1.3955e-05,\n",
      "          -5.6561e-04,  3.8087e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.0890161395072937\n",
      "raw_input ['[CLS]', 'unrewarding', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.14)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.09</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> unrewarding                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw review: old-hat \n",
      "GT target: 0\n",
      "detokenized: ['[CLS]', 'old', '-', 'hat', '[SEP]']\n",
      "attribution.size torch.Size([1, 5, 768])\n",
      "word attr tensor([ 0.4042, -0.6943, -0.4808, -0.2567,  0.2398], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4042, device='cuda:0', grad_fn=<UnbindBackward0>), tensor(-0.4221, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2398, device='cuda:0', grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'old', '-', 'hat', '[SEP]']\n",
      "len conti_raw 3\n",
      "conti_raw ['[CLS]', 'old-hat', '[SEP]']\n",
      "conti_attri_size: torch.Size([3]), raw_input_len:  3\n",
      "word attributions tensor([ 0.4042, -0.6943, -0.4808, -0.2567,  0.2398], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "pred_prob 0.05097651481628418\n",
      "pred_class Neg\n",
      "true_class Neg\n",
      "attribution tensor([[[ 9.9881e-05, -2.9060e-05,  2.7545e-05,  ...,  2.1199e-05,\n",
      "           1.3056e-05,  6.3890e-05],\n",
      "         [ 4.6974e-05,  9.7913e-05,  1.1328e-04,  ..., -6.5330e-04,\n",
      "          -2.0544e-05,  3.2899e-04],\n",
      "         [ 2.9737e-06, -3.4201e-05, -1.2171e-04,  ..., -2.2911e-04,\n",
      "           9.3750e-05,  1.0719e-04],\n",
      "         [ 4.1445e-04,  9.8811e-04,  2.9642e-05,  ..., -1.6177e-06,\n",
      "          -3.4267e-04, -4.8535e-04],\n",
      "         [-3.1616e-04,  1.5100e-04,  3.5193e-05,  ...,  1.2676e-05,\n",
      "           1.8402e-05, -2.1962e-05]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "attr_class Neg\n",
      "attr_score -0.7877177596092224\n",
      "raw_input ['[CLS]', 'old-hat', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>Neg (0.05)</b></text></td><td><text style=\"padding-right:2em\"><b>Neg</b></text></td><td><text style=\"padding-right:2em\"><b>-0.79</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> old-hat                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n"
     ]
    }
   ],
   "source": [
    "for i, (datum_raw, target) in enumerate(zip(sst2_data_raw, targets), start=1):\n",
    "    print(f'Raw review: {datum_raw}')\n",
    "    print(f'GT target: {target}')\n",
    "    visual_record=generate_record(datum_raw, target)\n",
    "    print(visualization.visualize_text([visual_record]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ca5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_base = 'DeepLIFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce01966e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices': [27798,\n",
       "  28244,\n",
       "  48083,\n",
       "  8795,\n",
       "  91,\n",
       "  20682,\n",
       "  59890,\n",
       "  41763,\n",
       "  19795,\n",
       "  9957,\n",
       "  28334,\n",
       "  55664,\n",
       "  16438,\n",
       "  18031,\n",
       "  9297,\n",
       "  66913,\n",
       "  9289,\n",
       "  6789,\n",
       "  25294,\n",
       "  33294,\n",
       "  29699,\n",
       "  41960,\n",
       "  6554,\n",
       "  8772,\n",
       "  17921,\n",
       "  8027,\n",
       "  49091,\n",
       "  54186,\n",
       "  15080,\n",
       "  22382,\n",
       "  23856,\n",
       "  9464,\n",
       "  23831,\n",
       "  59434,\n",
       "  17862,\n",
       "  24584,\n",
       "  26216,\n",
       "  63094,\n",
       "  55787,\n",
       "  3993,\n",
       "  25463,\n",
       "  17540,\n",
       "  51128,\n",
       "  46224,\n",
       "  32656,\n",
       "  30105,\n",
       "  28646,\n",
       "  17011,\n",
       "  7812,\n",
       "  48236],\n",
       " 'raw_data': ['its oscar nomination ',\n",
       "  'shenanigans and slapstick ',\n",
       "  'an unsettling sight , ',\n",
       "  'the climactic hourlong cricket match ',\n",
       "  'alternating between facetious comic parody and pulp melodrama , this smart-aleck movie ... tosses around some intriguing questions about the difference between human and android life ',\n",
       "  'to be a part of that elusive adult world ',\n",
       "  'emotional power ',\n",
       "  'reminds you of why animation is such a perfect medium for children , because of the way it allows the mind to enter and accept another world ',\n",
       "  'unparalleled proportions , writer-director parker ',\n",
       "  'this surprisingly decent flick ',\n",
       "  \"about the best thing you could say about narc is that it 's a rock-solid little genre picture . \",\n",
       "  'the very best ',\n",
       "  'been modeled on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ',\n",
       "  'tell you ',\n",
       "  'utterly absorbing ',\n",
       "  'restate ',\n",
       "  'bears about as much resemblance to the experiences of most battered women as spider-man ',\n",
       "  'expressively performed ',\n",
       "  'the acting is amateurish , the cinematography is atrocious ',\n",
       "  'solidly constructed ',\n",
       "  \"are undermined by the movie 's presentation , which is way too stagy \",\n",
       "  'a great film ',\n",
       "  'charm ',\n",
       "  'this new jangle of noise , mayhem and stupidity ',\n",
       "  'sustains it ',\n",
       "  'is so deadly dull that watching the proverbial paint dry would be a welcome improvement . ',\n",
       "  'to accomplish ',\n",
       "  \"does n't work . \",\n",
       "  'given it a one-star rating ',\n",
       "  \"ice cube is n't quite out of ripe screwball ideas , but friday after next spreads them pretty thin \",\n",
       "  'been trying to forget ',\n",
       "  'big stars and ',\n",
       "  'of cheesy dialogue ',\n",
       "  \"the château is never quite able to overcome the cultural moat surrounding its ludicrous and contrived plot . ' \",\n",
       "  'principled ',\n",
       "  'end it all by stuffing himself into an electric pencil sharpener ',\n",
       "  'funniest idea ',\n",
       "  'silly and tedious ',\n",
       "  'two surefire , beloved genres ',\n",
       "  'most charmless ',\n",
       "  'offers us the sense that on some elemental level , lilia deeply wants to break free of her old life . ',\n",
       "  'might be best forgotten ',\n",
       "  \"a substantial arc of change that does n't produce any real transformation \",\n",
       "  'unadorned ',\n",
       "  \"this is the kind of movie that you only need to watch for about thirty seconds before you say to yourself , ` ah , yes , here we have a bad , bad , bad movie . ' \",\n",
       "  'feel sanitised and stagey ',\n",
       "  'replete with stereotypical familial quandaries ',\n",
       "  'quirky comedy ',\n",
       "  'unrewarding ',\n",
       "  'old-hat '],\n",
       " 'targets': [1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0],\n",
       " 'model_out_list': [0.9578894972801208,\n",
       "  0.02607199177145958,\n",
       "  0.9280797243118286,\n",
       "  0.6026754379272461,\n",
       "  0.9272036552429199,\n",
       "  0.9597240090370178,\n",
       "  0.9770727157592773,\n",
       "  0.9707044959068298,\n",
       "  0.8834286332130432,\n",
       "  0.9658218622207642,\n",
       "  0.9529748558998108,\n",
       "  0.9749705791473389,\n",
       "  0.030051304027438164,\n",
       "  0.8954260945320129,\n",
       "  0.974568247795105,\n",
       "  0.5530857443809509,\n",
       "  0.7086556553840637,\n",
       "  0.9764208197593689,\n",
       "  0.031884290277957916,\n",
       "  0.9670499563217163,\n",
       "  0.04456028714776039,\n",
       "  0.9733861684799194,\n",
       "  0.9737695455551147,\n",
       "  0.09326992928981781,\n",
       "  0.9708950519561768,\n",
       "  0.41628754138946533,\n",
       "  0.9736232161521912,\n",
       "  0.04155701398849487,\n",
       "  0.7330113053321838,\n",
       "  0.04169812798500061,\n",
       "  0.06650674343109131,\n",
       "  0.9741768836975098,\n",
       "  0.02389264479279518,\n",
       "  0.060439176857471466,\n",
       "  0.9714932441711426,\n",
       "  0.06462432444095612,\n",
       "  0.9628397226333618,\n",
       "  0.035474635660648346,\n",
       "  0.9750030040740967,\n",
       "  0.9686804413795471,\n",
       "  0.9634529948234558,\n",
       "  0.06778600811958313,\n",
       "  0.0436873659491539,\n",
       "  0.058859918266534805,\n",
       "  0.06825332343578339,\n",
       "  0.026674073189496994,\n",
       "  0.07909509539604187,\n",
       "  0.9688959121704102,\n",
       "  0.136697456240654,\n",
       "  0.05097651481628418],\n",
       " 'raw_attr_list': [tensor([[[ 5.6118e-05,  2.9305e-04,  9.7753e-05,  ...,  2.9469e-06,\n",
       "             1.9243e-06,  1.2387e-04],\n",
       "           [-4.7869e-05, -1.5525e-05, -5.5945e-05,  ...,  1.0678e-04,\n",
       "            -2.7966e-04,  7.9963e-05],\n",
       "           [-7.4345e-05, -1.2982e-05,  5.1472e-05,  ...,  1.2174e-04,\n",
       "             2.2254e-05, -6.7197e-04],\n",
       "           [-1.2129e-04, -9.9070e-05, -2.7507e-04,  ...,  4.9241e-06,\n",
       "             9.1832e-05,  6.7943e-05],\n",
       "           [ 3.6586e-05,  3.4733e-05,  1.5339e-05,  ...,  2.1184e-06,\n",
       "            -2.2236e-05, -1.2717e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.2240e-05,  2.0466e-05, -1.5911e-05,  ...,  1.2393e-06,\n",
       "            -7.1767e-06,  2.2254e-05],\n",
       "           [ 3.2862e-04,  1.0194e-05, -1.1568e-04,  ..., -4.8323e-05,\n",
       "            -3.8453e-05,  3.3143e-05],\n",
       "           [ 1.5021e-05, -4.2352e-05,  4.4267e-07,  ..., -1.2696e-05,\n",
       "            -9.0889e-05,  1.5951e-05],\n",
       "           ...,\n",
       "           [-8.4689e-05,  8.7179e-05, -1.0657e-06,  ..., -4.5986e-05,\n",
       "             3.8754e-05, -7.3308e-05],\n",
       "           [ 2.5701e-05,  5.6159e-04,  3.1380e-05,  ..., -4.4703e-05,\n",
       "             5.0181e-05, -1.2103e-04],\n",
       "           [ 9.5672e-06,  3.0635e-06, -1.3325e-06,  ..., -2.0004e-05,\n",
       "            -2.1287e-05, -7.0118e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-8.4471e-04, -1.3056e-03,  2.1097e-03,  ...,  8.1015e-06,\n",
       "             1.1388e-04, -1.2068e-03],\n",
       "           [-1.4752e-04,  4.7157e-04,  1.4792e-03,  ...,  1.4924e-04,\n",
       "            -2.6515e-05, -9.7217e-05],\n",
       "           [-5.9664e-03,  1.8475e-04, -1.1587e-03,  ...,  4.5340e-04,\n",
       "             6.3575e-04, -3.3946e-03],\n",
       "           ...,\n",
       "           [ 3.9218e-04,  5.4181e-03,  7.5541e-04,  ...,  1.6092e-03,\n",
       "             2.4072e-03,  1.9684e-03],\n",
       "           [-9.4711e-04, -2.0052e-04,  7.9903e-04,  ..., -1.5353e-04,\n",
       "            -2.1151e-04, -3.7344e-04],\n",
       "           [-7.0907e-04,  3.5432e-04,  3.9687e-05,  ...,  1.6068e-04,\n",
       "             4.8841e-04,  1.8676e-04]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.7535e-03, -3.6461e-03,  3.6146e-03,  ...,  6.4074e-04,\n",
       "            -5.1886e-04, -3.7664e-04],\n",
       "           [ 9.2471e-04,  2.5147e-03,  1.2014e-04,  ...,  1.3683e-04,\n",
       "             2.0381e-03,  2.9042e-03],\n",
       "           [-2.2307e-03, -1.1027e-03,  3.9112e-04,  ...,  2.0170e-03,\n",
       "             3.3976e-03, -5.1761e-04],\n",
       "           ...,\n",
       "           [-3.9418e-03, -4.1627e-04,  6.5526e-03,  ...,  2.2180e-04,\n",
       "            -9.0398e-04,  3.1244e-05],\n",
       "           [-4.0247e-03, -9.3806e-04, -1.0850e-03,  ..., -9.0199e-04,\n",
       "            -5.1297e-06, -3.0710e-03],\n",
       "           [ 5.7123e-05,  6.6015e-05,  5.2154e-04,  ..., -1.8634e-04,\n",
       "             3.5000e-04, -2.6765e-04]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.7046e-05, -4.1725e-04,  4.6891e-04,  ...,  5.0591e-06,\n",
       "             3.4942e-05, -1.0506e-04],\n",
       "           [ 9.9722e-05, -1.1713e-05, -9.5881e-05,  ...,  1.8912e-04,\n",
       "             3.7043e-06,  2.8597e-04],\n",
       "           [-4.4453e-05,  1.1189e-04,  1.2184e-04,  ..., -3.9734e-05,\n",
       "             4.4952e-05,  7.2982e-05],\n",
       "           ...,\n",
       "           [-1.9071e-04,  1.8042e-04,  2.0304e-05,  ...,  5.9772e-05,\n",
       "             2.1915e-05,  1.9642e-05],\n",
       "           [-8.3036e-05,  7.6641e-05, -2.0500e-05,  ..., -2.1299e-05,\n",
       "            -4.8015e-06, -2.1186e-05],\n",
       "           [-4.1505e-05, -6.7389e-06, -3.0532e-06,  ..., -1.2476e-04,\n",
       "            -3.3247e-05, -5.3039e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.7106e-06, -4.4951e-05, -7.3287e-05,  ...,  6.0580e-06,\n",
       "             1.1504e-05, -5.0293e-05],\n",
       "           [-7.9074e-06,  2.7797e-05, -1.0174e-05,  ..., -5.5681e-05,\n",
       "             3.0717e-05,  6.9963e-05],\n",
       "           [-1.8169e-05,  1.9898e-05,  1.1885e-05,  ..., -1.3127e-04,\n",
       "            -1.0561e-05,  3.6324e-05],\n",
       "           ...,\n",
       "           [-1.1675e-04,  2.5093e-05, -1.2660e-04,  ..., -5.1119e-04,\n",
       "            -6.3490e-04, -2.7509e-04],\n",
       "           [-8.1122e-06, -1.4693e-04,  3.5224e-06,  ...,  5.1901e-05,\n",
       "            -8.1771e-06, -4.2823e-05],\n",
       "           [-1.7240e-06, -3.2325e-08,  2.1431e-07,  ...,  2.5354e-05,\n",
       "            -5.6306e-07,  9.8437e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.3127e-05,  2.2723e-05,  6.3689e-05,  ...,  1.0705e-07,\n",
       "             7.7197e-06,  4.7968e-06],\n",
       "           [-3.9285e-05, -3.1470e-06,  3.7734e-05,  ..., -1.2771e-05,\n",
       "            -1.1550e-04,  2.2015e-05],\n",
       "           [-2.2206e-05, -4.9624e-06, -9.6476e-06,  ..., -5.5834e-05,\n",
       "            -4.7645e-05, -3.9261e-06],\n",
       "           [-7.3419e-06,  2.1784e-05, -1.6599e-06,  ...,  1.6574e-05,\n",
       "             6.0319e-07,  1.9300e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.9493e-06, -2.7472e-05,  4.5581e-05,  ..., -3.2066e-07,\n",
       "             3.6805e-07,  2.7350e-06],\n",
       "           [-5.9369e-05,  1.8699e-04,  1.8960e-05,  ...,  4.1680e-05,\n",
       "            -6.3430e-05,  3.3588e-05],\n",
       "           [ 4.5409e-05,  1.2603e-05,  8.8290e-06,  ...,  2.4557e-05,\n",
       "             1.7124e-05, -1.4570e-05],\n",
       "           ...,\n",
       "           [ 3.0550e-08,  1.3725e-05, -1.7679e-07,  ...,  7.0417e-07,\n",
       "            -1.4703e-06, -9.0486e-07],\n",
       "           [-1.4697e-08,  8.3785e-06, -7.1520e-06,  ...,  1.9073e-05,\n",
       "            -7.8026e-06, -4.6081e-06],\n",
       "           [ 1.9716e-06,  2.6898e-06, -6.0718e-06,  ...,  1.8756e-05,\n",
       "             7.7612e-07, -2.2300e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.3690e-03,  5.3666e-04,  4.6986e-03,  ...,  3.6927e-04,\n",
       "            -1.1285e-04,  9.6212e-04],\n",
       "           [-4.8043e-03, -6.6548e-04,  3.5503e-04,  ..., -1.0213e-03,\n",
       "             5.8423e-04, -1.0551e-03],\n",
       "           [ 2.8534e-04, -1.2939e-02,  1.8908e-03,  ..., -2.5307e-03,\n",
       "             2.1004e-03, -8.0169e-04],\n",
       "           ...,\n",
       "           [ 2.5952e-03,  1.6008e-03,  1.3781e-03,  ...,  2.0294e-03,\n",
       "            -1.6675e-03, -3.8248e-04],\n",
       "           [ 2.6019e-03, -2.0525e-04, -7.4006e-03,  ...,  1.0706e-03,\n",
       "             1.6927e-03,  8.1553e-04],\n",
       "           [ 5.2381e-05, -2.3889e-04,  9.8754e-05,  ...,  6.4149e-04,\n",
       "             1.6421e-03,  1.8162e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.7908e-05, -4.9183e-05,  8.6487e-06,  ..., -4.1470e-06,\n",
       "             1.8032e-06, -4.7871e-05],\n",
       "           [ 6.3360e-05,  1.7524e-05, -8.9799e-07,  ..., -1.2517e-05,\n",
       "             4.8621e-06, -2.0737e-05],\n",
       "           [-2.1350e-05, -9.5027e-05,  1.6444e-04,  ..., -1.3762e-05,\n",
       "            -1.1767e-06, -6.2087e-05],\n",
       "           [-9.8799e-05, -7.9502e-05,  7.8448e-05,  ...,  1.1757e-05,\n",
       "             1.7321e-05,  5.1947e-05],\n",
       "           [ 5.6605e-05,  3.0655e-05, -2.2095e-06,  ...,  8.8350e-05,\n",
       "             7.4597e-05,  1.1686e-04],\n",
       "           [-5.1702e-05,  8.0042e-06, -1.3448e-05,  ..., -5.1371e-05,\n",
       "            -1.6773e-06,  8.5062e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-8.3879e-05, -2.4201e-05,  9.9320e-05,  ...,  1.1370e-05,\n",
       "            -2.4173e-05, -5.1113e-05],\n",
       "           [ 3.5297e-08,  5.2696e-06,  4.7844e-07,  ...,  7.8000e-05,\n",
       "             1.1606e-05, -2.0265e-05],\n",
       "           [ 1.8848e-04,  6.0568e-05,  1.0493e-05,  ..., -4.3904e-06,\n",
       "            -1.2509e-05, -3.3004e-05],\n",
       "           ...,\n",
       "           [-2.2781e-04,  1.4775e-04,  2.5136e-04,  ...,  7.3135e-06,\n",
       "            -6.2341e-04, -1.0640e-04],\n",
       "           [ 3.1001e-05,  1.3900e-04,  3.8195e-05,  ..., -6.9541e-05,\n",
       "             8.3273e-05,  2.9944e-05],\n",
       "           [ 1.3619e-05,  1.0677e-05,  3.2851e-05,  ..., -2.3965e-06,\n",
       "            -2.2746e-05, -2.3708e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.5726e-06, -3.7741e-06, -2.2028e-06,  ...,  1.3993e-07,\n",
       "            -5.0455e-07, -3.4728e-06],\n",
       "           [ 3.8678e-06, -9.3996e-06,  1.4354e-07,  ..., -4.6208e-07,\n",
       "             2.3996e-05,  3.6043e-08],\n",
       "           [-3.9884e-05, -1.8896e-05,  2.6419e-05,  ..., -8.6467e-06,\n",
       "             6.5382e-05, -4.8945e-05],\n",
       "           [ 5.1369e-06, -1.7922e-05, -9.6659e-05,  ...,  2.8248e-05,\n",
       "             2.3627e-05, -1.0502e-05],\n",
       "           [ 2.1263e-05, -1.7121e-06,  1.1478e-06,  ...,  5.0158e-06,\n",
       "             2.4289e-06, -3.1984e-07]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.4502e-05,  1.4741e-05, -2.5708e-05,  ..., -1.5564e-06,\n",
       "            -2.4454e-07, -9.3092e-06],\n",
       "           [-1.6708e-05, -6.6752e-06, -4.2105e-06,  ..., -1.6519e-05,\n",
       "             1.8434e-05,  5.7428e-05],\n",
       "           [ 1.1637e-05, -3.4137e-05, -4.6630e-05,  ..., -7.7274e-06,\n",
       "             2.0469e-05,  2.5676e-06],\n",
       "           ...,\n",
       "           [ 3.6232e-06,  1.7431e-05, -2.7441e-05,  ..., -2.1750e-05,\n",
       "             1.9033e-06, -7.8760e-06],\n",
       "           [ 5.3451e-06, -1.0707e-05, -6.2580e-06,  ..., -2.4123e-06,\n",
       "            -3.3773e-05, -7.6056e-07],\n",
       "           [-3.7619e-06, -1.7280e-05, -7.1128e-08,  ..., -9.6150e-06,\n",
       "            -4.1897e-06, -5.5655e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.6948e-04,  7.5237e-05,  7.2884e-04,  ...,  2.4884e-05,\n",
       "             5.3599e-05, -2.7698e-04],\n",
       "           [-5.3324e-05, -4.0641e-04,  9.7193e-04,  ...,  1.7618e-03,\n",
       "            -5.5861e-03, -3.4724e-03],\n",
       "           [ 3.2000e-04,  3.1976e-04, -4.9410e-04,  ...,  4.9872e-04,\n",
       "             7.4494e-04,  3.8955e-05],\n",
       "           [ 1.3637e-04,  1.3046e-04, -2.0151e-04,  ...,  7.2926e-05,\n",
       "             1.5687e-06,  1.2726e-04]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.8755e-06, -7.5190e-06,  3.2281e-06,  ..., -7.2082e-07,\n",
       "             2.9925e-06,  2.7231e-06],\n",
       "           [-3.4200e-05, -6.0501e-06, -2.4908e-07,  ..., -3.4590e-05,\n",
       "            -1.6954e-06, -7.4547e-06],\n",
       "           [-3.9505e-05, -1.3914e-05,  1.7442e-05,  ..., -3.8390e-05,\n",
       "            -5.0091e-05, -6.1136e-06],\n",
       "           [ 1.8123e-05, -3.6713e-06,  1.2399e-06,  ...,  1.1440e-05,\n",
       "            -1.8561e-07,  5.5115e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 0.0011, -0.0052, -0.0098,  ...,  0.0008, -0.0006, -0.0057],\n",
       "           [-0.0025, -0.0028,  0.0131,  ..., -0.0083, -0.0075, -0.0011],\n",
       "           [ 0.0056,  0.0008, -0.0446,  ..., -0.0038, -0.0153,  0.0231],\n",
       "           [-0.0013, -0.0002, -0.0063,  ..., -0.0035, -0.0009,  0.0033]]],\n",
       "         device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.0609e-04, -9.4074e-04,  1.0023e-03,  ...,  7.6250e-05,\n",
       "             1.4261e-04, -1.1315e-03],\n",
       "           [ 2.9596e-03,  1.2237e-03,  9.3196e-04,  ..., -1.9112e-03,\n",
       "            -2.4170e-03, -2.9512e-03],\n",
       "           [-1.3850e-03, -8.1027e-05,  4.2842e-04,  ..., -1.8960e-03,\n",
       "            -1.6938e-04, -7.4819e-04],\n",
       "           ...,\n",
       "           [ 3.3389e-05,  2.4678e-04, -5.1011e-04,  ...,  9.3583e-05,\n",
       "             2.5418e-04, -9.0653e-05],\n",
       "           [-1.8092e-05, -5.9786e-04,  1.5406e-05,  ...,  7.5957e-04,\n",
       "            -1.8389e-03,  1.5642e-03],\n",
       "           [ 1.2571e-04, -3.6306e-04, -1.1855e-04,  ...,  6.3281e-05,\n",
       "             2.3516e-04,  3.7622e-04]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.2622e-05, -2.6069e-05,  6.6887e-06,  ...,  3.9094e-07,\n",
       "            -2.8721e-06, -1.8889e-05],\n",
       "           [-1.0269e-04, -1.3553e-04,  5.5502e-06,  ...,  1.0555e-05,\n",
       "             3.9240e-07,  7.4170e-05],\n",
       "           [-6.6972e-06, -4.4028e-06,  3.1622e-05,  ...,  7.8215e-08,\n",
       "            -7.0880e-05, -1.3707e-05],\n",
       "           [-4.6956e-06, -3.7383e-06, -1.5216e-04,  ...,  3.3071e-05,\n",
       "             9.7264e-06, -1.3651e-05],\n",
       "           [-1.9373e-05, -3.0151e-06,  2.7378e-06,  ...,  5.2237e-06,\n",
       "            -1.5071e-06,  9.2984e-07]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 3.1152e-05,  9.8489e-07,  1.8055e-04,  ...,  2.1063e-05,\n",
       "            -8.8610e-06,  7.6682e-05],\n",
       "           [ 9.9634e-06,  1.1925e-05, -7.2186e-07,  ...,  4.5815e-06,\n",
       "            -1.0859e-05, -2.5873e-05],\n",
       "           [ 9.5554e-07, -3.9607e-06,  2.3549e-06,  ...,  1.7172e-04,\n",
       "             8.1369e-05, -2.7915e-06],\n",
       "           ...,\n",
       "           [ 2.2013e-04,  1.7475e-05,  7.7590e-05,  ...,  1.9327e-05,\n",
       "            -1.7467e-04,  7.3584e-05],\n",
       "           [ 1.1327e-04, -1.0497e-05,  3.6865e-05,  ..., -9.3987e-04,\n",
       "             1.0718e-04, -3.6612e-05],\n",
       "           [-4.6008e-05,  1.2201e-05, -4.0564e-05,  ...,  7.2045e-05,\n",
       "            -1.4340e-05,  7.4914e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 9.0524e-06, -4.3318e-05,  2.5908e-05,  ...,  1.1114e-05,\n",
       "             1.8061e-05,  4.2793e-05],\n",
       "           [ 3.0117e-04, -7.9200e-06, -1.7391e-04,  ..., -1.3817e-06,\n",
       "             4.6694e-04, -4.1211e-06],\n",
       "           [-2.8881e-05, -2.3238e-05, -1.9858e-05,  ..., -3.1670e-06,\n",
       "             1.1579e-04,  3.3367e-05],\n",
       "           [-6.5006e-05, -6.1647e-04, -4.3587e-04,  ..., -3.9856e-05,\n",
       "             6.3274e-05,  7.9873e-05],\n",
       "           [ 5.3489e-06,  9.9481e-06,  1.3974e-05,  ...,  3.6162e-05,\n",
       "             3.0708e-05, -8.5965e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.1709e-05, -1.0990e-05, -9.1821e-06,  ..., -2.3799e-06,\n",
       "            -2.3174e-06,  4.0010e-07],\n",
       "           [-6.3746e-06, -6.2289e-07, -3.1407e-05,  ..., -7.3458e-06,\n",
       "             2.1751e-06,  6.7365e-06],\n",
       "           [-9.8520e-05, -4.7012e-05, -7.6850e-06,  ...,  9.8600e-07,\n",
       "            -6.0562e-05,  3.2081e-05],\n",
       "           ...,\n",
       "           [-4.6194e-05,  5.7704e-05, -2.2622e-05,  ..., -1.4948e-06,\n",
       "            -4.6414e-05,  3.4469e-06],\n",
       "           [ 1.5426e-05,  7.0727e-07,  9.2346e-06,  ..., -1.2389e-05,\n",
       "            -1.9147e-07,  7.3344e-06],\n",
       "           [ 2.2232e-06,  5.4812e-07, -6.1788e-08,  ...,  2.4420e-07,\n",
       "            -1.1138e-06, -3.8960e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 3.8071e-06,  6.5181e-06,  3.8087e-05,  ...,  2.5980e-07,\n",
       "             9.8095e-07,  1.6948e-06],\n",
       "           [ 8.0576e-06, -3.0133e-05, -5.5306e-07,  ...,  1.5439e-05,\n",
       "             1.6651e-05, -2.1615e-05],\n",
       "           [ 3.0059e-06, -7.2396e-06, -4.0301e-05,  ..., -4.0350e-05,\n",
       "            -4.8268e-05,  7.1154e-06],\n",
       "           [-4.3089e-05,  3.7098e-07,  3.2425e-05,  ...,  1.5705e-05,\n",
       "             8.5213e-06,  8.4914e-06],\n",
       "           [-3.3712e-06,  2.8807e-06, -2.7770e-06,  ...,  3.2585e-06,\n",
       "             2.8002e-06, -6.0028e-07]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.7756e-05, -5.4807e-05,  2.9673e-05,  ...,  2.5451e-06,\n",
       "            -7.0754e-06, -8.8289e-05],\n",
       "           [ 1.6365e-05,  1.9021e-04, -2.0888e-05,  ..., -6.9746e-05,\n",
       "             1.6556e-04, -1.2634e-04],\n",
       "           [ 1.1341e-05, -1.4848e-06, -3.5010e-08,  ..., -2.7444e-05,\n",
       "             5.8132e-06,  1.0974e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-7.9858e-04, -5.7894e-05,  1.3077e-03,  ...,  1.9946e-06,\n",
       "             1.4976e-04,  4.8682e-05],\n",
       "           [ 1.4561e-04, -2.7210e-04,  9.1617e-05,  ..., -4.0648e-04,\n",
       "            -1.6151e-04, -1.6426e-04],\n",
       "           [ 2.4620e-04, -2.5611e-05, -8.1231e-04,  ...,  1.7174e-03,\n",
       "            -6.0632e-04,  3.4388e-04],\n",
       "           ...,\n",
       "           [ 2.0533e-04,  1.3401e-04,  2.9562e-04,  ...,  9.9795e-04,\n",
       "            -7.1221e-04,  6.7601e-04],\n",
       "           [ 1.1342e-03, -4.0498e-04, -8.5521e-05,  ..., -7.6541e-04,\n",
       "             1.8276e-03, -3.0406e-03],\n",
       "           [-4.3591e-04,  7.6237e-05,  9.3653e-05,  ..., -5.7256e-04,\n",
       "            -2.4154e-04,  5.4355e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.2169e-05, -6.1126e-05,  1.1738e-04,  ...,  1.0516e-06,\n",
       "            -2.8821e-06, -2.1278e-05],\n",
       "           [ 2.9901e-05, -6.7084e-05, -2.1465e-05,  ..., -1.0447e-05,\n",
       "            -1.3921e-04,  5.2941e-05],\n",
       "           [-1.5009e-05,  6.7858e-07, -6.8856e-05,  ...,  5.2170e-06,\n",
       "             1.1680e-06,  3.3056e-06],\n",
       "           [-2.1204e-05,  1.1838e-05, -8.5127e-06,  ..., -5.4897e-05,\n",
       "             2.1695e-05, -8.9943e-06],\n",
       "           [-1.1647e-05,  7.5310e-07, -1.4874e-06,  ...,  2.0025e-05,\n",
       "             6.9576e-07, -1.7007e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.3901e-03, -6.5693e-04,  2.8960e-03,  ...,  2.8596e-04,\n",
       "             1.5143e-04, -2.0218e-03],\n",
       "           [-1.6023e-04,  2.2168e-05,  9.1537e-04,  ..., -2.9368e-03,\n",
       "            -5.8927e-04, -5.2006e-04],\n",
       "           [ 2.2618e-03,  1.5420e-03,  2.7514e-04,  ...,  6.0882e-04,\n",
       "             1.5734e-03,  1.7694e-03],\n",
       "           ...,\n",
       "           [ 8.2648e-04,  1.1560e-02, -1.5523e-03,  ..., -2.0432e-03,\n",
       "            -9.6284e-03, -4.1791e-03],\n",
       "           [ 1.9582e-03,  2.7271e-03,  4.2241e-05,  ...,  6.6640e-04,\n",
       "             2.3050e-03,  2.3111e-03],\n",
       "           [ 1.0196e-03,  4.0439e-05,  6.6492e-04,  ..., -8.5574e-04,\n",
       "             1.8582e-05, -1.2017e-04]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.3343e-05, -2.3361e-05,  1.1454e-04,  ...,  8.9868e-07,\n",
       "             1.2645e-05, -1.7938e-05],\n",
       "           [ 6.1118e-05,  6.1600e-05,  3.6646e-05,  ..., -1.2065e-04,\n",
       "             8.2937e-06,  1.4947e-05],\n",
       "           [ 1.4476e-04,  2.0582e-04, -5.2648e-05,  ..., -1.3833e-05,\n",
       "             5.1991e-05, -7.5593e-06],\n",
       "           [-7.8072e-05,  1.3316e-05, -8.9833e-06,  ...,  2.0717e-05,\n",
       "            -7.5088e-06, -2.2836e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.1942e-05,  1.2470e-05, -1.5882e-06,  ..., -1.5837e-06,\n",
       "             2.9576e-06, -1.3241e-06],\n",
       "           [ 7.2940e-05, -1.6574e-05,  2.2394e-05,  ...,  2.1936e-06,\n",
       "            -1.1238e-05,  1.0915e-05],\n",
       "           [ 1.0305e-06,  2.0906e-05, -7.3598e-05,  ..., -5.0954e-06,\n",
       "            -4.7961e-06, -4.0087e-05],\n",
       "           ...,\n",
       "           [-5.4147e-05,  1.8609e-05,  9.3747e-07,  ...,  1.6975e-05,\n",
       "             4.0806e-07,  2.1564e-06],\n",
       "           [-7.7567e-06, -2.0813e-06,  5.2366e-06,  ...,  6.0409e-06,\n",
       "            -6.0292e-06,  8.4612e-06],\n",
       "           [-4.4959e-06,  7.2653e-06, -2.4047e-06,  ..., -3.2687e-05,\n",
       "            -9.0924e-07, -3.0917e-07]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 9.9116e-04, -2.7847e-03,  4.7868e-03,  ..., -3.3443e-04,\n",
       "            -7.1248e-05, -2.9241e-04],\n",
       "           [-7.5598e-03, -5.8167e-05, -1.5867e-03,  ..., -1.0491e-02,\n",
       "            -6.8996e-04,  1.2664e-02],\n",
       "           [ 5.5208e-03, -1.4954e-04,  1.7701e-04,  ..., -1.2888e-03,\n",
       "             4.0579e-03,  4.5639e-04],\n",
       "           ...,\n",
       "           [ 5.9983e-03,  5.8060e-03,  8.1891e-03,  ..., -9.0070e-05,\n",
       "             1.1040e-02, -6.5972e-03],\n",
       "           [ 1.8468e-02, -2.0957e-02,  3.5347e-03,  ..., -2.8132e-05,\n",
       "            -8.3085e-03,  2.1958e-02],\n",
       "           [-1.4205e-03,  1.7160e-03, -1.7241e-04,  ..., -5.1928e-03,\n",
       "             1.4986e-03, -1.5349e-04]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.6635e-05,  1.1284e-04, -1.1036e-04,  ...,  6.3797e-07,\n",
       "            -9.5204e-06, -1.1014e-05],\n",
       "           [ 2.8110e-06, -8.1474e-07, -2.4583e-05,  ..., -7.2258e-06,\n",
       "            -1.0113e-04,  1.5770e-04],\n",
       "           [-3.4061e-05, -7.3432e-05,  2.1656e-04,  ...,  1.9766e-05,\n",
       "             6.5011e-06, -7.1494e-06],\n",
       "           ...,\n",
       "           [ 4.4248e-06, -3.1386e-05,  3.3340e-05,  ..., -1.1634e-05,\n",
       "            -1.4394e-04, -2.0230e-05],\n",
       "           [-1.2557e-04, -1.4315e-04, -2.6913e-06,  ...,  6.3474e-05,\n",
       "            -1.6742e-04,  2.4762e-05],\n",
       "           [-3.3104e-06,  1.0262e-05,  2.3399e-07,  ...,  1.7745e-07,\n",
       "             9.1472e-06, -6.5062e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-8.3760e-05, -3.2577e-07, -7.2443e-05,  ..., -6.9045e-06,\n",
       "            -1.1266e-04,  2.3770e-04],\n",
       "           [ 3.5900e-04,  1.0539e-03,  7.4832e-05,  ..., -2.6654e-04,\n",
       "             8.5969e-04, -4.2134e-04],\n",
       "           [ 4.8394e-04,  4.7084e-04, -3.5098e-04,  ...,  1.6913e-04,\n",
       "             2.4788e-04,  8.9316e-04],\n",
       "           [-1.6766e-04, -1.0352e-04,  1.3098e-06,  ...,  4.4922e-05,\n",
       "            -7.7971e-05, -1.3330e-04],\n",
       "           [-7.0681e-04,  2.6462e-03,  7.4123e-05,  ..., -2.4295e-05,\n",
       "             7.6408e-04, -4.0252e-05],\n",
       "           [-1.6396e-04,  2.5793e-04, -1.2306e-04,  ..., -6.0981e-05,\n",
       "            -3.6281e-04,  1.6424e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.2368e-05, -4.2201e-05,  1.1938e-06,  ..., -2.3166e-06,\n",
       "            -1.9747e-06,  8.5891e-07],\n",
       "           [-4.9124e-06, -1.2696e-04, -5.6553e-05,  ...,  3.9120e-06,\n",
       "            -1.8003e-04, -1.0751e-05],\n",
       "           [ 6.1404e-05, -5.1238e-05, -1.8792e-04,  ..., -1.8766e-06,\n",
       "            -2.9393e-05, -1.9601e-05],\n",
       "           [ 8.4486e-06, -2.7757e-05, -7.4137e-06,  ..., -4.1251e-05,\n",
       "            -1.0682e-05,  2.1416e-05],\n",
       "           [ 5.0507e-05,  1.2932e-06,  9.9915e-06,  ...,  8.1489e-06,\n",
       "             2.1811e-05, -4.6258e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.9801e-05, -9.0758e-06,  1.0570e-05,  ...,  5.7816e-06,\n",
       "            -4.7012e-06,  1.9117e-05],\n",
       "           [-6.2349e-06, -2.2102e-06, -1.2051e-06,  ...,  3.0041e-05,\n",
       "             4.1574e-05, -3.1511e-06],\n",
       "           [ 3.9155e-05, -1.3769e-05,  3.3695e-06,  ..., -5.4160e-05,\n",
       "            -2.3346e-06, -1.2024e-05],\n",
       "           ...,\n",
       "           [-4.1446e-06,  5.7195e-06,  2.3660e-05,  ..., -1.9267e-06,\n",
       "             1.4023e-06, -2.8339e-05],\n",
       "           [ 4.9730e-05, -1.6036e-05, -1.1638e-05,  ..., -1.9036e-05,\n",
       "             1.3651e-05, -2.9961e-05],\n",
       "           [-6.0888e-07, -1.2886e-06, -6.4118e-06,  ..., -1.1765e-05,\n",
       "             3.0122e-06, -3.2444e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.9042e-05,  1.8278e-04,  8.5474e-05,  ...,  1.0801e-05,\n",
       "             3.8661e-06, -4.0562e-05],\n",
       "           [ 9.7180e-05,  5.7750e-05,  3.2696e-06,  ..., -2.9894e-06,\n",
       "            -1.2729e-05,  6.8346e-06],\n",
       "           [ 1.7774e-05, -2.1695e-05,  2.5749e-05,  ...,  3.3581e-05,\n",
       "            -7.5368e-05,  2.8521e-04],\n",
       "           ...,\n",
       "           [ 1.6308e-05, -5.2560e-05,  8.2617e-05,  ...,  3.0120e-05,\n",
       "             8.8284e-05,  9.9966e-05],\n",
       "           [ 5.7471e-05, -1.9445e-04, -6.3393e-05,  ..., -2.6926e-04,\n",
       "             6.7456e-05,  5.6434e-04],\n",
       "           [ 2.3326e-05, -4.8799e-05, -2.7280e-05,  ...,  3.6866e-05,\n",
       "             3.0678e-05,  1.4843e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-8.6671e-07, -2.2054e-05,  1.6083e-05,  ...,  3.0221e-06,\n",
       "            -1.0206e-06,  3.5013e-05],\n",
       "           [-7.6407e-05,  2.2034e-05, -1.0459e-04,  ...,  1.1570e-05,\n",
       "            -2.7344e-06,  4.1473e-05],\n",
       "           [ 1.3018e-05, -2.7149e-05, -4.5132e-05,  ...,  9.2963e-05,\n",
       "            -2.3813e-05,  1.9574e-05],\n",
       "           [ 1.4112e-05, -1.8912e-05, -5.7022e-06,  ...,  2.3022e-05,\n",
       "            -8.0425e-08, -2.2106e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.9817e-04,  2.3735e-05, -1.7401e-04,  ..., -3.8237e-07,\n",
       "            -1.6468e-05, -1.2356e-04],\n",
       "           [-1.3887e-05, -2.2484e-04, -5.7021e-05,  ...,  4.7934e-05,\n",
       "             7.7219e-06,  1.0408e-04],\n",
       "           [ 6.2319e-05, -1.0129e-05,  1.2506e-06,  ..., -1.7780e-04,\n",
       "             3.1702e-05, -3.1370e-05],\n",
       "           ...,\n",
       "           [ 8.8288e-06, -4.9111e-05, -1.3808e-04,  ..., -2.1250e-04,\n",
       "            -4.8968e-06, -1.3757e-05],\n",
       "           [-6.3140e-05, -1.0999e-06,  8.1773e-05,  ..., -3.4984e-05,\n",
       "             1.0493e-05, -2.7084e-05],\n",
       "           [ 1.9229e-04,  7.5351e-07,  1.3233e-05,  ..., -1.8215e-05,\n",
       "             7.2056e-06,  1.0230e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-6.6441e-05, -8.3544e-05,  1.7784e-04,  ...,  1.0857e-05,\n",
       "            -8.9894e-06,  1.0729e-06],\n",
       "           [-3.5605e-04,  3.8828e-06, -1.7129e-05,  ..., -2.2378e-05,\n",
       "            -5.0690e-05,  4.4922e-07],\n",
       "           [ 8.6835e-06,  1.0296e-04, -2.8976e-04,  ...,  7.8794e-05,\n",
       "            -9.1170e-05,  8.8553e-05],\n",
       "           [ 2.4934e-04, -5.6335e-06, -1.2534e-04,  ..., -9.2085e-05,\n",
       "             1.1307e-05, -1.3900e-05],\n",
       "           [-5.5930e-05, -8.4604e-05,  8.2236e-05,  ...,  1.5878e-06,\n",
       "             2.3311e-05,  1.6142e-04],\n",
       "           [ 1.4131e-04, -7.2575e-06,  2.5924e-05,  ...,  6.2854e-05,\n",
       "             9.5706e-05, -2.9407e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.0472e-04, -1.2286e-04,  1.5446e-05,  ...,  1.4780e-05,\n",
       "            -4.2645e-06,  7.6638e-05],\n",
       "           [ 8.4665e-05, -1.1964e-04,  2.3985e-04,  ..., -2.8419e-04,\n",
       "             1.0989e-04,  7.3927e-05],\n",
       "           [ 2.1969e-04,  1.5050e-05,  1.1233e-06,  ...,  2.8086e-06,\n",
       "            -8.9091e-05,  8.1865e-05],\n",
       "           [-3.9292e-04,  5.7023e-04, -5.3597e-05,  ..., -8.6774e-05,\n",
       "            -2.1747e-04,  4.5271e-04],\n",
       "           [-6.8893e-04, -3.5115e-04,  3.6147e-05,  ..., -5.2622e-05,\n",
       "            -3.3489e-04, -1.5096e-04],\n",
       "           [ 8.4317e-05, -4.3053e-05, -2.0074e-05,  ...,  5.0146e-05,\n",
       "            -1.7643e-05, -2.8070e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.5841e-05, -4.0055e-05,  1.3962e-05,  ...,  1.5808e-06,\n",
       "            -6.9240e-07,  1.3099e-05],\n",
       "           [-2.4925e-06,  1.9772e-06, -2.3693e-08,  ..., -2.2529e-06,\n",
       "            -3.9616e-06,  1.5391e-05],\n",
       "           [ 3.3581e-05, -1.8613e-06,  6.1800e-06,  ...,  1.2109e-05,\n",
       "            -3.9117e-06, -4.8674e-06],\n",
       "           ...,\n",
       "           [ 1.0057e-04,  2.3435e-06, -1.6810e-05,  ...,  6.9540e-05,\n",
       "             4.6464e-05,  1.4025e-05],\n",
       "           [-5.0036e-05,  3.3869e-05, -9.0419e-08,  ...,  3.5055e-06,\n",
       "            -1.1748e-04, -2.5713e-05],\n",
       "           [ 1.5728e-05, -2.8351e-06, -4.4458e-06,  ...,  4.8946e-06,\n",
       "            -1.6714e-06, -4.1742e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.7838e-05, -1.0294e-04,  8.6529e-05,  ...,  1.0215e-05,\n",
       "            -2.8571e-05, -5.1206e-05],\n",
       "           [-9.8080e-05, -4.1954e-04, -3.2463e-05,  ..., -3.3777e-04,\n",
       "            -8.4599e-05,  1.5167e-05],\n",
       "           [ 1.4036e-05,  5.5807e-04, -1.6762e-04,  ..., -1.0826e-03,\n",
       "             2.3765e-03, -8.7056e-04],\n",
       "           [ 1.0250e-04, -5.4617e-05, -1.9594e-04,  ...,  2.7668e-05,\n",
       "            -8.4031e-04,  3.0026e-04],\n",
       "           [ 9.2369e-07, -1.9258e-05,  2.9939e-05,  ..., -1.1834e-05,\n",
       "             3.0345e-05, -9.9724e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.8323e-05, -6.9529e-05,  5.0805e-05,  ...,  3.4530e-06,\n",
       "            -2.6187e-06,  1.0072e-05],\n",
       "           [ 2.9681e-05,  1.8740e-06,  4.4954e-05,  ..., -8.2763e-06,\n",
       "            -8.1420e-05, -1.4135e-05],\n",
       "           [ 1.9085e-05, -4.4207e-06,  4.9691e-05,  ..., -1.9821e-04,\n",
       "            -6.2178e-05, -9.3180e-05],\n",
       "           ...,\n",
       "           [-3.8878e-05,  1.0112e-04, -8.4732e-06,  ...,  9.2188e-06,\n",
       "            -1.0378e-05,  2.2695e-05],\n",
       "           [ 9.6987e-06,  3.3312e-06, -5.6509e-06,  ...,  3.3357e-05,\n",
       "             2.5269e-05,  3.7695e-05],\n",
       "           [ 1.9542e-05,  2.9475e-06, -4.5791e-07,  ..., -1.0965e-05,\n",
       "            -2.7434e-06, -3.0567e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.6103e-04, -3.0831e-04, -2.5143e-04,  ..., -4.3445e-05,\n",
       "            -1.6570e-04,  8.0589e-06],\n",
       "           [-1.7042e-04, -3.8339e-04, -4.1214e-04,  ...,  1.4887e-05,\n",
       "            -4.3873e-05,  2.0356e-03],\n",
       "           [ 1.6719e-05, -8.6997e-05,  1.6819e-04,  ..., -6.8126e-04,\n",
       "             1.5002e-04,  8.5864e-05],\n",
       "           [-6.9395e-05,  3.0101e-04,  6.4874e-04,  ..., -7.4524e-05,\n",
       "            -4.9351e-04,  1.2174e-04],\n",
       "           [-1.0115e-05,  6.6065e-04, -1.1478e-04,  ...,  3.1142e-05,\n",
       "            -1.4346e-04,  9.7698e-05],\n",
       "           [ 3.8689e-05,  3.0805e-05,  9.5421e-05,  ..., -2.5014e-04,\n",
       "            -5.3173e-04, -7.6428e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.7197e-05,  1.1296e-04, -8.5880e-05,  ...,  4.6198e-06,\n",
       "             9.6052e-06,  2.1354e-05],\n",
       "           [ 2.6142e-05,  2.4231e-05,  2.2442e-06,  ...,  1.9072e-05,\n",
       "             2.3877e-05, -2.1547e-05],\n",
       "           [ 2.5393e-06,  7.1075e-05, -3.2784e-05,  ..., -9.2138e-06,\n",
       "            -1.0792e-05, -2.0384e-05],\n",
       "           ...,\n",
       "           [ 8.6237e-08, -2.4571e-05, -1.1066e-05,  ...,  8.4661e-05,\n",
       "            -3.7210e-06, -9.6249e-07],\n",
       "           [-4.2932e-05, -1.1937e-04, -1.0635e-04,  ..., -1.1659e-04,\n",
       "            -5.3850e-06, -4.8406e-05],\n",
       "           [ 3.0086e-05,  1.3992e-05,  1.7960e-05,  ...,  2.4470e-06,\n",
       "            -9.3841e-06, -4.8605e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-6.5135e-04, -7.9913e-04,  5.6523e-04,  ...,  3.2998e-05,\n",
       "            -5.6369e-05,  2.1507e-04],\n",
       "           [-7.1676e-04, -7.2108e-04,  6.6194e-04,  ..., -3.9838e-05,\n",
       "            -3.8586e-04,  5.9887e-06],\n",
       "           [-8.3998e-04,  1.5451e-04, -1.9032e-04,  ..., -2.0753e-04,\n",
       "            -3.3329e-05, -2.6720e-03],\n",
       "           [ 5.6651e-04,  3.4404e-03, -2.5126e-04,  ..., -5.3545e-04,\n",
       "             7.0895e-04,  2.5596e-04],\n",
       "           [-6.4728e-04,  1.5006e-04, -6.9665e-05,  ...,  1.3534e-04,\n",
       "             2.9914e-05,  2.2283e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.6276e-05,  3.7201e-05,  4.0487e-06,  ..., -2.7480e-06,\n",
       "            -8.3599e-06, -2.9737e-05],\n",
       "           [-2.8363e-05,  7.4823e-06, -2.7392e-06,  ...,  8.3707e-06,\n",
       "             1.7607e-05, -6.3278e-06],\n",
       "           [-1.1679e-06,  2.8555e-06, -5.5088e-06,  ...,  1.9565e-06,\n",
       "             1.0683e-05,  1.9728e-05],\n",
       "           ...,\n",
       "           [-6.9168e-06, -4.7917e-06, -5.3040e-06,  ...,  1.1239e-05,\n",
       "            -4.6062e-06, -3.0246e-05],\n",
       "           [ 1.0028e-05, -7.2306e-06, -6.1426e-06,  ...,  1.5533e-05,\n",
       "             5.5870e-06, -8.0438e-06],\n",
       "           [-1.2198e-06,  5.7928e-07, -1.1956e-06,  ..., -6.3132e-05,\n",
       "             1.3834e-05,  9.4613e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 5.1744e-06,  8.4274e-06, -1.9106e-05,  ..., -1.3140e-06,\n",
       "             5.4614e-07,  5.0827e-06],\n",
       "           [ 2.4882e-05, -7.4711e-06,  8.9851e-06,  ..., -8.2522e-06,\n",
       "            -7.4577e-05,  2.1217e-06],\n",
       "           [ 1.5851e-06,  2.2482e-06,  6.5901e-06,  ...,  3.0072e-06,\n",
       "             2.2203e-05, -5.4184e-05],\n",
       "           ...,\n",
       "           [ 5.6668e-05, -2.2350e-05, -1.8512e-04,  ..., -4.5703e-05,\n",
       "            -2.2408e-05, -5.3592e-05],\n",
       "           [-2.8287e-07, -6.9934e-06,  5.6447e-06,  ...,  1.4806e-05,\n",
       "            -4.6104e-06, -8.6863e-06],\n",
       "           [-6.6935e-06,  2.1267e-08, -2.7334e-06,  ..., -9.8862e-06,\n",
       "            -1.5683e-05, -1.5170e-06]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.2301e-04, -6.2074e-04,  1.2504e-03,  ...,  1.5687e-04,\n",
       "            -8.1551e-05,  1.0383e-04],\n",
       "           [ 5.6706e-05,  4.6869e-04,  4.1097e-04,  ...,  7.3573e-04,\n",
       "             9.6138e-04, -2.7538e-03],\n",
       "           [-1.6844e-03,  4.5589e-04,  1.3191e-05,  ...,  1.0686e-03,\n",
       "             5.9927e-05,  1.2070e-04],\n",
       "           ...,\n",
       "           [ 6.9507e-04, -7.4989e-04, -6.1717e-05,  ..., -2.4249e-04,\n",
       "            -1.9341e-03, -3.0731e-05],\n",
       "           [-7.4405e-04,  2.0999e-04,  8.1171e-04,  ..., -1.1786e-03,\n",
       "            -8.2392e-04,  9.0028e-04],\n",
       "           [-1.9715e-03, -4.1713e-05, -4.8657e-05,  ..., -7.8254e-04,\n",
       "            -5.4674e-04,  4.7547e-05]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.5813e-06,  5.5474e-05,  5.4438e-05,  ..., -3.5604e-06,\n",
       "            -2.7470e-06,  3.2761e-06],\n",
       "           [ 2.7083e-06,  8.4338e-05,  7.3154e-05,  ...,  9.5819e-06,\n",
       "            -8.7315e-05,  9.8163e-06],\n",
       "           [-1.2681e-05, -5.1503e-05,  1.5754e-05,  ...,  3.9296e-05,\n",
       "            -3.3165e-05,  4.5282e-05],\n",
       "           [-9.0289e-05,  1.7105e-05,  9.2016e-05,  ...,  9.2032e-06,\n",
       "            -1.9556e-05,  6.1270e-05],\n",
       "           [ 2.3776e-05, -8.0215e-06, -2.6170e-06,  ...,  2.0557e-05,\n",
       "            -1.1639e-05,  5.1088e-07]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.7160e-04, -6.7170e-04,  9.2334e-04,  ..., -2.2524e-04,\n",
       "            -3.9245e-04, -1.5112e-03],\n",
       "           [-1.5045e-03, -1.4908e-04,  9.8187e-05,  ...,  1.1406e-03,\n",
       "            -5.8529e-04, -1.9372e-03],\n",
       "           [ 5.1399e-03,  8.5475e-03, -9.7246e-03,  ...,  6.4563e-03,\n",
       "             5.1946e-04,  7.2597e-03],\n",
       "           [-1.0251e-02,  1.1558e-02, -6.6328e-03,  ..., -6.8432e-03,\n",
       "             1.6537e-03,  1.5233e-03],\n",
       "           [-3.3848e-03,  1.1663e-03,  1.5905e-04,  ..., -1.3955e-05,\n",
       "            -5.6561e-04,  3.8087e-04]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 9.9881e-05, -2.9060e-05,  2.7545e-05,  ...,  2.1199e-05,\n",
       "             1.3056e-05,  6.3890e-05],\n",
       "           [ 4.6974e-05,  9.7913e-05,  1.1328e-04,  ..., -6.5330e-04,\n",
       "            -2.0544e-05,  3.2899e-04],\n",
       "           [ 2.9737e-06, -3.4201e-05, -1.2171e-04,  ..., -2.2911e-04,\n",
       "             9.3750e-05,  1.0719e-04],\n",
       "           [ 4.1445e-04,  9.8811e-04,  2.9642e-05,  ..., -1.6177e-06,\n",
       "            -3.4267e-04, -4.8535e-04],\n",
       "           [-3.1616e-04,  1.5100e-04,  3.5193e-05,  ...,  1.2676e-05,\n",
       "             1.8402e-05, -2.1962e-05]]], device='cuda:0', grad_fn=<MulBackward0>)],\n",
       " 'conti_attr_list': [tensor([-0.0169,  0.9381,  0.0499, -0.0706, -0.3349]),\n",
       "  tensor([ 0.2709,  0.0296, -0.3118, -0.4231,  0.3932]),\n",
       "  tensor([ 0.1475, -0.2139,  0.0468,  0.4351, -0.0191, -0.1624]),\n",
       "  tensor([ 0.0127,  0.1172, -0.5242,  0.1779,  0.2001,  0.1508,  0.1066]),\n",
       "  tensor([ 0.1010,  0.0262, -0.1032, -0.1563,  0.3789, -0.1731,  0.1659,  0.0044,\n",
       "           0.0250,  0.0562,  0.0447,  0.0041,  0.1400, -0.0363, -0.2584,  0.3163,\n",
       "          -0.0946,  0.2628, -0.0248,  0.0237,  0.0427, -0.0866,  0.0038, -0.1079,\n",
       "           0.0833,  0.0182,  0.1446,  0.1154]),\n",
       "  tensor([-0.2217, -0.1169, -0.2887, -0.0235,  0.0704,  0.5797,  0.3952,  0.4382,\n",
       "          -0.3514, -0.1747,  0.1001]),\n",
       "  tensor([ 0.2289,  0.8345, -0.2999, -0.4016]),\n",
       "  tensor([-1.7282e-01, -4.3502e-01,  3.5896e-01, -1.2356e-02,  4.0488e-02,\n",
       "          -3.9486e-01,  1.5669e-01,  4.2826e-03,  8.9566e-02, -4.2371e-02,\n",
       "           2.0590e-02,  1.5962e-01,  1.8793e-01,  5.2300e-01, -5.7915e-03,\n",
       "           1.5629e-01, -5.9820e-03, -1.4820e-02,  1.2546e-01, -5.2364e-02,\n",
       "          -1.1015e-01, -6.7547e-03,  1.1125e-01, -1.4747e-01,  1.3142e-01,\n",
       "          -1.3559e-04, -2.3657e-02,  1.2558e-01, -4.5692e-02]),\n",
       "  tensor([-0.0780,  0.3852, -0.3191, -0.3551, -0.0785, -0.2347, -0.0453]),\n",
       "  tensor([-0.0320,  0.1857, -0.0830, -0.8073,  0.5528, -0.0162]),\n",
       "  tensor([-1.0571e-01,  1.0276e-02,  1.3271e-02, -1.8174e-01, -2.4987e-01,\n",
       "          -1.6005e-01, -3.9558e-02, -2.3003e-01,  3.4982e-04,  2.3036e-01,\n",
       "          -1.6324e-01,  7.6906e-02, -7.4246e-02,  1.2973e-02,  4.5712e-01,\n",
       "           1.0056e-01,  2.7484e-02,  3.1365e-01, -3.6857e-01,  6.6165e-02,\n",
       "          -1.6048e-02]),\n",
       "  tensor([ 0.4469,  0.0911, -0.6789, -0.3612,  0.4479]),\n",
       "  tensor([ 0.1239, -0.1179, -0.2977,  0.0219, -0.0534,  0.3971, -0.0925,  0.1143,\n",
       "          -0.3645, -0.2225,  0.2042,  0.0837, -0.0466, -0.1986]),\n",
       "  tensor([-0.1917,  0.8105,  0.2391, -0.4991]),\n",
       "  tensor([ 0.7587,  0.2889, -0.5804,  0.0639]),\n",
       "  tensor([-0.6817,  0.0294,  0.7291]),\n",
       "  tensor([-0.1064, -0.3416, -0.0236,  0.1678, -0.3179, -0.0244,  0.1094, -0.2324,\n",
       "           0.7117,  0.0610, -0.1641,  0.0336,  0.3203, -0.0346, -0.0921, -0.0487]),\n",
       "  tensor([ 0.7281,  0.2400, -0.4946, -0.0677]),\n",
       "  tensor([ 0.0726, -0.0022,  0.2605, -0.0500,  0.0690,  0.0793,  0.0374,  0.1258,\n",
       "          -0.2427, -0.0997, -0.3405]),\n",
       "  tensor([ 0.1286,  0.3014, -0.8465, -0.0695]),\n",
       "  tensor([-0.0901,  0.1143, -0.1104,  0.0376,  0.0031,  0.2809,  0.0261, -0.0248,\n",
       "          -0.4176,  0.1376, -0.1121, -0.0952, -0.0285,  0.0693,  0.1051]),\n",
       "  tensor([ 0.4263,  0.0523, -0.6309,  0.4843, -0.4278]),\n",
       "  tensor([ 0.4231,  0.3307, -0.8436]),\n",
       "  tensor([ 0.2218, -0.0386,  0.0836,  0.0800,  0.2109, -0.4157, -0.0611,  0.4498,\n",
       "           0.0442, -0.5193, -0.3478]),\n",
       "  tensor([ 0.1955,  0.4654, -0.5361, -0.3050]),\n",
       "  tensor([-0.3513, -0.0648,  0.1475,  0.6808, -0.1779,  0.1738,  0.0773, -0.0202,\n",
       "           0.0155, -0.1180, -0.1224, -0.0651, -0.1918,  0.1865,  0.0703, -0.0817,\n",
       "           0.1946, -0.0343]),\n",
       "  tensor([-0.2798,  0.3581,  0.5793, -0.6766]),\n",
       "  tensor([-0.1869, -0.0428, -0.0949, -0.0029,  0.1533, -0.0024]),\n",
       "  tensor([ 0.4724, -0.3102, -0.1172,  0.0596, -0.2339, -0.0911, -0.1865]),\n",
       "  tensor([ 0.1420,  0.1307, -0.1961, -0.0852, -0.0109, -0.0021, -0.1367,  0.0706,\n",
       "           0.2590, -0.0469, -0.5723, -0.5195,  0.1086,  0.2158, -0.0009,  0.0508,\n",
       "          -0.2730,  0.1639, -0.1255,  0.1350, -0.0512]),\n",
       "  tensor([ 0.2060, -0.4474, -0.0405,  0.3503,  0.4659, -0.6450]),\n",
       "  tensor([ 0.4711, -0.6442,  0.1502, -0.5694,  0.1276]),\n",
       "  tensor([-0.3926,  0.0644,  0.0967, -0.7918, -0.0452]),\n",
       "  tensor([ 0.0122,  0.0127, -0.1748,  0.1025,  0.2031, -0.1094,  0.0943, -0.0688,\n",
       "           0.0498,  0.0519, -0.0563,  0.0028, -0.0648, -0.0082, -0.1819,  0.1419,\n",
       "           0.1011, -0.1031, -0.3353,  0.2412, -0.3273]),\n",
       "  tensor([ 0.8245, -0.0298, -0.2536]),\n",
       "  tensor([ 0.5746, -0.0817, -0.3704, -0.0077, -0.5246,  0.0094,  0.0275, -0.1248,\n",
       "          -0.3190, -0.0629, -0.0019, -0.0098, -0.0876]),\n",
       "  tensor([-0.1961, -0.3588, -0.3823,  0.6362]),\n",
       "  tensor([-0.2907,  0.4455,  0.0885, -0.0892, -0.3862]),\n",
       "  tensor([-0.3083, -0.2355,  0.1176, -0.3376, -0.3643,  0.2075,  0.3572]),\n",
       "  tensor([-0.1707, -0.6284,  0.1950,  0.6161]),\n",
       "  tensor([-0.1206,  0.1659, -0.1774,  0.4294,  0.2112,  0.1563, -0.1412, -0.2674,\n",
       "          -0.3360,  0.1312,  0.0867,  0.0009, -0.1360, -0.1888,  0.2256,  0.0395,\n",
       "           0.0324,  0.0661, -0.1398,  0.0301,  0.4908, -0.0743, -0.1034]),\n",
       "  tensor([ 0.1531, -0.1980, -0.6500, -0.6200,  0.0287,  0.3600]),\n",
       "  tensor([-0.0196,  0.0819, -0.1579, -0.0174, -0.0503, -0.0753,  0.0985,  0.0941,\n",
       "          -0.0846, -0.1111,  0.1855,  0.0874,  0.0531, -0.0363]),\n",
       "  tensor([-0.2473,  0.1908, -0.0734]),\n",
       "  tensor([ 0.2111,  0.0282, -0.0291, -0.0099, -0.0414,  0.0400,  0.0463, -0.0331,\n",
       "          -0.0307,  0.0427,  0.0505, -0.0328, -0.1487,  0.0187,  0.0150,  0.0087,\n",
       "           0.0728,  0.0123, -0.1122, -0.0820, -0.1369, -0.0229, -0.0151,  0.0106,\n",
       "           0.0525, -0.1429, -0.0809,  0.0642, -0.0849, -0.1381, -0.3035,  0.0018,\n",
       "           0.2621,  0.0418,  0.5613, -0.2263,  0.0905, -0.2437, -0.4391,  0.0461,\n",
       "          -0.1487]),\n",
       "  tensor([-0.0517, -0.2406,  0.0957,  0.1674,  0.0545, -0.1177]),\n",
       "  tensor([ 0.3305,  0.3783, -0.2189, -0.2035, -0.0367,  0.0722, -0.1213]),\n",
       "  tensor([ 0.2973,  0.4011, -0.7281,  0.0920]),\n",
       "  tensor([ 0.7271, -0.2407,  0.1405]),\n",
       "  tensor([ 0.4042, -0.4221,  0.2398])],\n",
       " 'raw_input_list': [['[CLS]', 'its', 'oscar', 'nomination', '[SEP]'],\n",
       "  ['[CLS]', 'shenanigans', 'and', 'slapstick', '[SEP]'],\n",
       "  ['[CLS]', 'an', 'unsettling', 'sight', ',', '[SEP]'],\n",
       "  ['[CLS]', 'the', 'climactic', 'hourlong', 'cricket', 'match', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'alternating',\n",
       "   'between',\n",
       "   'facetious',\n",
       "   'comic',\n",
       "   'parody',\n",
       "   'and',\n",
       "   'pulp',\n",
       "   'melodrama',\n",
       "   ',',\n",
       "   'this',\n",
       "   'smart-aleck',\n",
       "   'movie',\n",
       "   '...',\n",
       "   'tosses',\n",
       "   'around',\n",
       "   'some',\n",
       "   'intriguing',\n",
       "   'questions',\n",
       "   'about',\n",
       "   'the',\n",
       "   'difference',\n",
       "   'between',\n",
       "   'human',\n",
       "   'and',\n",
       "   'android',\n",
       "   'life',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'to',\n",
       "   'be',\n",
       "   'a',\n",
       "   'part',\n",
       "   'of',\n",
       "   'that',\n",
       "   'elusive',\n",
       "   'adult',\n",
       "   'world',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'emotional', 'power', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'reminds',\n",
       "   'you',\n",
       "   'of',\n",
       "   'why',\n",
       "   'animation',\n",
       "   'is',\n",
       "   'such',\n",
       "   'a',\n",
       "   'perfect',\n",
       "   'medium',\n",
       "   'for',\n",
       "   'children',\n",
       "   ',',\n",
       "   'because',\n",
       "   'of',\n",
       "   'the',\n",
       "   'way',\n",
       "   'it',\n",
       "   'allows',\n",
       "   'the',\n",
       "   'mind',\n",
       "   'to',\n",
       "   'enter',\n",
       "   'and',\n",
       "   'accept',\n",
       "   'another',\n",
       "   'world',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'unparalleled',\n",
       "   'proportions',\n",
       "   ',',\n",
       "   'writer-director',\n",
       "   'parker',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'this', 'surprisingly', 'decent', 'flick', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'about',\n",
       "   'the',\n",
       "   'best',\n",
       "   'thing',\n",
       "   'you',\n",
       "   'could',\n",
       "   'say',\n",
       "   'about',\n",
       "   'narc',\n",
       "   'is',\n",
       "   'that',\n",
       "   'it',\n",
       "   \"'s\",\n",
       "   'a',\n",
       "   'rock-solid',\n",
       "   'little',\n",
       "   'genre',\n",
       "   'picture',\n",
       "   '.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'the', 'very', 'best', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'been',\n",
       "   'modeled',\n",
       "   'on',\n",
       "   'the',\n",
       "   'worst',\n",
       "   'revenge-of-the-nerds',\n",
       "   'cliches',\n",
       "   'the',\n",
       "   'filmmakers',\n",
       "   'could',\n",
       "   'dredge',\n",
       "   'up',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'tell', 'you', '[SEP]'],\n",
       "  ['[CLS]', 'utterly', 'absorbing', '[SEP]'],\n",
       "  ['[CLS]', 'restate', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'bears',\n",
       "   'about',\n",
       "   'as',\n",
       "   'much',\n",
       "   'resemblance',\n",
       "   'to',\n",
       "   'the',\n",
       "   'experiences',\n",
       "   'of',\n",
       "   'most',\n",
       "   'battered',\n",
       "   'women',\n",
       "   'as',\n",
       "   'spider-man',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'expressively', 'performed', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'the',\n",
       "   'acting',\n",
       "   'is',\n",
       "   'amateurish',\n",
       "   ',',\n",
       "   'the',\n",
       "   'cinematography',\n",
       "   'is',\n",
       "   'atrocious',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'solidly', 'constructed', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'are',\n",
       "   'undermined',\n",
       "   'by',\n",
       "   'the',\n",
       "   'movie',\n",
       "   \"'s\",\n",
       "   'presentation',\n",
       "   ',',\n",
       "   'which',\n",
       "   'is',\n",
       "   'way',\n",
       "   'too',\n",
       "   'stagy',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'a', 'great', 'film', '[SEP]'],\n",
       "  ['[CLS]', 'charm', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'this',\n",
       "   'new',\n",
       "   'jangle',\n",
       "   'of',\n",
       "   'noise',\n",
       "   ',',\n",
       "   'mayhem',\n",
       "   'and',\n",
       "   'stupidity',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'sustains', 'it', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'is',\n",
       "   'so',\n",
       "   'deadly',\n",
       "   'dull',\n",
       "   'that',\n",
       "   'watching',\n",
       "   'the',\n",
       "   'proverbial',\n",
       "   'paint',\n",
       "   'dry',\n",
       "   'would',\n",
       "   'be',\n",
       "   'a',\n",
       "   'welcome',\n",
       "   'improvement',\n",
       "   '.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'to', 'accomplish', '[SEP]'],\n",
       "  ['[CLS]', 'does', \"n't\", 'work', '.', '[SEP]'],\n",
       "  ['[CLS]', 'given', 'it', 'a', 'one-star', 'rating', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'ice',\n",
       "   'cube',\n",
       "   'is',\n",
       "   \"n't\",\n",
       "   'quite',\n",
       "   'out',\n",
       "   'of',\n",
       "   'ripe',\n",
       "   'screwball',\n",
       "   'ideas',\n",
       "   ',',\n",
       "   'but',\n",
       "   'friday',\n",
       "   'after',\n",
       "   'next',\n",
       "   'spreads',\n",
       "   'them',\n",
       "   'pretty',\n",
       "   'thin',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'been', 'trying', 'to', 'forget', '[SEP]'],\n",
       "  ['[CLS]', 'big', 'stars', 'and', '[SEP]'],\n",
       "  ['[CLS]', 'of', 'cheesy', 'dialogue', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'the',\n",
       "   'chateau',\n",
       "   'is',\n",
       "   'never',\n",
       "   'quite',\n",
       "   'able',\n",
       "   'to',\n",
       "   'overcome',\n",
       "   'the',\n",
       "   'cultural',\n",
       "   'moat',\n",
       "   'surrounding',\n",
       "   'its',\n",
       "   'ludicrous',\n",
       "   'and',\n",
       "   'contrived',\n",
       "   'plot',\n",
       "   '.',\n",
       "   \"'\",\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'principled', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'end',\n",
       "   'it',\n",
       "   'all',\n",
       "   'by',\n",
       "   'stuffing',\n",
       "   'himself',\n",
       "   'into',\n",
       "   'an',\n",
       "   'electric',\n",
       "   'pencil',\n",
       "   'sharpener',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'funniest', 'idea', '[SEP]'],\n",
       "  ['[CLS]', 'silly', 'and', 'tedious', '[SEP]'],\n",
       "  ['[CLS]', 'two', 'surefire', ',', 'beloved', 'genres', '[SEP]'],\n",
       "  ['[CLS]', 'most', 'charmless', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'offers',\n",
       "   'us',\n",
       "   'the',\n",
       "   'sense',\n",
       "   'that',\n",
       "   'on',\n",
       "   'some',\n",
       "   'elemental',\n",
       "   'level',\n",
       "   ',',\n",
       "   'lilia',\n",
       "   'deeply',\n",
       "   'wants',\n",
       "   'to',\n",
       "   'break',\n",
       "   'free',\n",
       "   'of',\n",
       "   'her',\n",
       "   'old',\n",
       "   'life',\n",
       "   '.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'might', 'be', 'best', 'forgotten', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'a',\n",
       "   'substantial',\n",
       "   'arc',\n",
       "   'of',\n",
       "   'change',\n",
       "   'that',\n",
       "   'does',\n",
       "   \"n't\",\n",
       "   'produce',\n",
       "   'any',\n",
       "   'real',\n",
       "   'transformation',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'unadorned', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'this',\n",
       "   'is',\n",
       "   'the',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'movie',\n",
       "   'that',\n",
       "   'you',\n",
       "   'only',\n",
       "   'need',\n",
       "   'to',\n",
       "   'watch',\n",
       "   'for',\n",
       "   'about',\n",
       "   'thirty',\n",
       "   'seconds',\n",
       "   'before',\n",
       "   'you',\n",
       "   'say',\n",
       "   'to',\n",
       "   'yourself',\n",
       "   ',',\n",
       "   '`',\n",
       "   'ah',\n",
       "   ',',\n",
       "   'yes',\n",
       "   ',',\n",
       "   'here',\n",
       "   'we',\n",
       "   'have',\n",
       "   'a',\n",
       "   'bad',\n",
       "   ',',\n",
       "   'bad',\n",
       "   ',',\n",
       "   'bad',\n",
       "   'movie',\n",
       "   '.',\n",
       "   \"'\",\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'feel', 'sanitised', 'and', 'stagey', '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'replete',\n",
       "   'with',\n",
       "   'stereotypical',\n",
       "   'familial',\n",
       "   'quandaries',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]', 'quirky', 'comedy', '[SEP]'],\n",
       "  ['[CLS]', 'unrewarding', '[SEP]'],\n",
       "  ['[CLS]', 'old-hat', '[SEP]']]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_info(idxs, sst2_data_raw, targets, model_out_list, raw_attr_list, conti_attr_list, raw_input_list, \n",
    "          fname=f'../MethodOutputs/{file_name_base}_out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db9de1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_HTML(cur_file_name=f'{file_name_base}_SST2_BERT.ipynb',\n",
    "              out_file_name=f'{file_name_base}_SST2_BERT.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe07dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
