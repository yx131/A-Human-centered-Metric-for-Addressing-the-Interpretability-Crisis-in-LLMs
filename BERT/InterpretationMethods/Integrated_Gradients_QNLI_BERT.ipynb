{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806fb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, pickle\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '../../Utils')\n",
    "from global_constants import gpu_device\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from BERT_models import BERT_QNLI_MODEL\n",
    "\n",
    "from _utils import sample_random_glue_qnli, get_continuation_mapping, \\\n",
    "                    get_continuous_attributions, get_continuous_raw_inputs, \\\n",
    "                    collect_info_for_metric, save_info, download_HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0e3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/user/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad9e709640e492da6f1405c5bdc09a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e6d0a9c9270b6822.arrow\n",
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-c46c5e78d2bda808.arrow\n"
     ]
    }
   ],
   "source": [
    "qnli_data_raw, targets, idxs = sample_random_glue_qnli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca0a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_QNLI_MODEL()\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92a7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some containers to save some info\n",
    "model_out_list, raw_attr_list, conti_attr_list, raw_input_list = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad13b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a26c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IG = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cc7ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_record(raw_datum, target): #raw_datum expected to be a tuple/list of 2 sentences\n",
    "    #tokenizer operations\n",
    "    tokenized = tokenizer(raw_datum, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mappings = tokenized['offset_mapping']\n",
    "    #concatenate the two offset_mappings together because they are fed in together\n",
    "    conti_map = get_continuation_mapping(offset_mappings[0]) + get_continuation_mapping(offset_mappings[1])\n",
    "    #change the first input_id of the second sentence to be the last input_id of the 1st sentence (i.e. an [END] token))\n",
    "    tokenized_input_ids = tokenized['input_ids'][0] + \\\n",
    "                        [tokenized['input_ids'][1][i] if i != 0 else tokenized['input_ids'][0][-1] \\\n",
    "                         for i in range(len(tokenized['input_ids'][1]))]\n",
    "    input_ids = torch.tensor(tokenized_input_ids).unsqueeze(0).to(gpu_device)\n",
    "    detokenized = [t.replace('#', '') for t in tokenizer.convert_ids_to_tokens(input_ids[0])]\n",
    "    \n",
    "    #feeding input forward \n",
    "    input_emb = model.get_embeddings(input_ids)\n",
    "    pred_prob = model(input_emb).item()\n",
    "\n",
    "     #categorizing results\n",
    "    pred_class = 'No Entailment' if pred_prob < 0.5 else 'Entailment' \n",
    "    true_class = 'No Entailment' if target < 0.5 else 'Entailment' \n",
    "    \n",
    "    #attribution algorithm working\n",
    "    attribution, delta = IG.attribute(input_emb, return_convergence_delta=True)\n",
    "    word_attributions = attribution.squeeze(0).sum(dim=1)\n",
    "#     word_attributions = attr_normalizing_func(word_attributions)\n",
    "    word_attributions /= torch.norm(word_attributions)\n",
    "    attr_score = torch.sum(word_attributions)\n",
    "    attr_class = 'No Entailment' if attr_score < 0.5 else 'Entailment'\n",
    "    convergence_score = delta\n",
    "    \n",
    "    \n",
    "#     #re-organizing tensors and arrays because words get split down\n",
    "    conti_attr = get_continuous_attributions(conti_map, word_attributions)\n",
    "    raw_input = get_continuous_raw_inputs(conti_map, detokenized)\n",
    "\n",
    "#     print(f'word attributions {word_attributions}')\n",
    "    print(f'pred_prob {pred_prob}')\n",
    "#     print(f'pred_class {pred_class}')\n",
    "#     print(f'true_class {true_class}')\n",
    "#     print(f'attribution {attribution}')\n",
    "#     print(f'attr_class {attr_class}')\n",
    "#     print(f'attr_score {attr_score}')\n",
    "#     print(f'raw_input {raw_input}')\n",
    "\n",
    "        \n",
    "# #     collect info for metrics later\n",
    "    collect_info_for_metric(model_out_list, pred_prob, raw_attr_list, attribution, conti_attr_list, conti_attr, raw_input_list, raw_input)\n",
    "        \n",
    "    \n",
    "    visual_record = visualization.VisualizationDataRecord(word_attributions=conti_attr,\n",
    "                                                         pred_prob=pred_prob,\n",
    "                                                         pred_class=pred_class,\n",
    "                                                         true_class=true_class,\n",
    "                                                         attr_class=attr_class,\n",
    "                                                         attr_score=attr_score,\n",
    "                                                         raw_input_ids=raw_input,\n",
    "                                                         convergence_score=convergence_score)\n",
    "        \n",
    "        \n",
    "    return visual_record\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0321c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw datum: ['What would a teacher assess the levels of a student on?', 'For example, an experienced teacher and parent described the place of a teacher in learning as follows: \"The real bulk of learning takes place in self-study and problem solving with a lot of feedback around that loop.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.2840,  0.0259, -0.0395,  0.0779, -0.0405, -0.0070,  0.0611, -0.1699,\n",
      "         0.0087,  0.0431, -0.0611, -0.0646, -0.0579,  0.5147,  0.3289, -0.0270,\n",
      "         0.0077, -0.0770, -0.0241, -0.1167, -0.1098, -0.0618, -0.0351,  0.0627,\n",
      "         0.0109, -0.1920, -0.0073,  0.0216, -0.0535,  0.0160, -0.2031,  0.0157,\n",
      "        -0.0993,  0.0044,  0.1890,  0.1058, -0.0249, -0.0162, -0.0582, -0.0810,\n",
      "         0.2040,  0.0276,  0.0214,  0.1012, -0.0306, -0.1019,  0.0800,  0.1731,\n",
      "         0.1302,  0.0663,  0.0319,  0.0387,  0.0539,  0.0117,  0.1007,  0.1069,\n",
      "         0.0069,  0.0419, -0.3844], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2840, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0259, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0395, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0779, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0405, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0070, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0611, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1699, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0087, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0431, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0611, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0612, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.5147, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3289, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0270, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0347, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0241, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1167, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1098, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0618, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0351, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0627, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0109, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1920, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0073, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0216, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0535, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0160, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2031, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0157, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0249, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0162, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0582, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0810, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2040, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0276, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0214, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0333, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0800, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1731, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1302, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0663, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0319, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0387, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0539, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0117, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1007, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1069, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0244, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.3844, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'would', 'a', 'teacher', 'assess', 'the', 'levels', 'of', 'a', 'student', 'on', '?', '[SEP]', '[SEP]', 'for', 'example', ',', 'an', 'experienced', 'teacher', 'and', 'parent', 'described', 'the', 'place', 'of', 'a', 'teacher', 'in', 'learning', 'as', 'follows', ':', '\"', 'the', 'real', 'bulk', 'of', 'learning', 'takes', 'place', 'in', 'self', '-', 'study', 'and', 'problem', 'solving', 'with', 'a', 'lot', 'of', 'feedback', 'around', 'that', 'loop', '.', '[SEP]']\n",
      "len conti_raw 52\n",
      "conti_raw ['[CLS]', 'what', 'would', 'a', 'teacher', 'assess', 'the', 'levels', 'of', 'a', 'student', 'on?', '[SEP]', '[SEP]', 'for', 'example,', 'an', 'experienced', 'teacher', 'and', 'parent', 'described', 'the', 'place', 'of', 'a', 'teacher', 'in', 'learning', 'as', 'follows:', '\"the', 'real', 'bulk', 'of', 'learning', 'takes', 'place', 'in', 'self-study', 'and', 'problem', 'solving', 'with', 'a', 'lot', 'of', 'feedback', 'around', 'that', 'loop.', '[SEP]']\n",
      "pred_prob 0.6311678290367126\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.63)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.83</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> teacher                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> assess                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> levels                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> student                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> example,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> experienced                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> teacher                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> parent                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> described                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> place                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> teacher                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> learning                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> follows:                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> real                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bulk                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> learning                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> takes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> place                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> self-study                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> problem                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> solving                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lot                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> feedback                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> around                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> loop.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What company created Doctor Who?', 'Who character by BBC Television in the early 1960s, a myriad of stories have been published about Doctor Who, in different media: apart from the actual television episodes that continue to be produced by the BBC, there have also been novels, comics, short stories, audio books, radio plays, interactive video games, game books, webcasts, DVD extras, and even stage performances.']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.0192, -0.0350,  0.0374,  0.1147, -0.4170, -0.0890,  0.3477,  0.4633,\n",
      "         0.2496,  0.0664, -0.1533, -0.0220,  0.1108, -0.0588, -0.0202, -0.0021,\n",
      "        -0.0071, -0.0988, -0.0401,  0.0224, -0.0710,  0.0202, -0.0237,  0.0138,\n",
      "        -0.0187, -0.0729, -0.0927, -0.3317, -0.1865, -0.0901, -0.0379, -0.0204,\n",
      "         0.0586,  0.0449, -0.0094,  0.0214,  0.0258,  0.0102,  0.0413, -0.0073,\n",
      "         0.0149, -0.0364,  0.0446,  0.0535,  0.1144,  0.1299,  0.0482,  0.2066,\n",
      "         0.0211,  0.0858,  0.1037,  0.0709,  0.0705,  0.0490,  0.0114, -0.0533,\n",
      "        -0.0025, -0.0016, -0.0250,  0.0114,  0.0055, -0.0709,  0.0204,  0.0341,\n",
      "         0.0125,  0.0191,  0.0373,  0.0163,  0.0335,  0.0106,  0.0414, -0.0771,\n",
      "         0.0089, -0.0436, -0.0007, -0.0050,  0.0203,  0.0023, -0.0662,  0.0150,\n",
      "         0.0578,  0.0505,  0.0106,  0.0904,  0.0079,  0.0945], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0192, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0350, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0374, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1147, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.4170, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1294, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4633, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2496, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0664, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1533, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0220, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1108, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0588, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0202, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0021, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0071, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0695, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0224, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0710, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0202, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0237, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0138, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0729, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0927, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.3317, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0379, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0204, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0518, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0094, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0214, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0258, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0102, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0413, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0073, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0364, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0446, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0535, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1144, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1299, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0482, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1139, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0858, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1037, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0709, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0705, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0302, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0279, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0016, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0068, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0055, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0253, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0341, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0158, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0373, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0163, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0221, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0414, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0341, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0033, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0023, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0256, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0578, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0505, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0106, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0491, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0945, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'company', 'created', 'doctor', 'who', '?', '[SEP]', '[SEP]', 'who', 'character', 'by', 'bbc', 'television', 'in', 'the', 'early', '1960s', ',', 'a', 'myriad', 'of', 'stories', 'have', 'been', 'published', 'about', 'doctor', 'who', ',', 'in', 'different', 'media', ':', 'apart', 'from', 'the', 'actual', 'television', 'episodes', 'that', 'continue', 'to', 'be', 'produced', 'by', 'the', 'bbc', ',', 'there', 'have', 'also', 'been', 'novels', ',', 'comics', ',', 'short', 'stories', ',', 'audio', 'books', ',', 'radio', 'plays', ',', 'interactive', 'video', 'games', ',', 'game', 'books', ',', 'web', 'cast', 's', ',', 'dvd', 'extras', ',', 'and', 'even', 'stage', 'performances', '.', '[SEP]']\n",
      "len conti_raw 69\n",
      "conti_raw ['[CLS]', 'what', 'company', 'created', 'doctor', 'who?', '[SEP]', '[SEP]', 'who', 'character', 'by', 'bbc', 'television', 'in', 'the', 'early', '1960s,', 'a', 'myriad', 'of', 'stories', 'have', 'been', 'published', 'about', 'doctor', 'who,', 'in', 'different', 'media:', 'apart', 'from', 'the', 'actual', 'television', 'episodes', 'that', 'continue', 'to', 'be', 'produced', 'by', 'the', 'bbc,', 'there', 'have', 'also', 'been', 'novels,', 'comics,', 'short', 'stories,', 'audio', 'books,', 'radio', 'plays,', 'interactive', 'video', 'games,', 'game', 'books,', 'webcasts,', 'dvd', 'extras,', 'and', 'even', 'stage', 'performances.', '[SEP]']\n",
      "pred_prob 0.7619203925132751\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.76)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> company                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> created                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> character                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bbc                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> television                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> early                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1960s,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> myriad                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stories                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> published                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> different                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> media:                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apart                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> actual                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> television                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> episodes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> continue                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> produced                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bbc,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> there                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> novels,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> comics,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> short                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stories,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> audio                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> books,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> radio                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plays,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interactive                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> video                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> games,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> books,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> webcasts,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dvd                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> extras,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> even                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performances.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What was the name of the Media Day event for Super Bowl 50?', \"The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night.\"]\n",
      "GT target: 1\n",
      "word attr tensor([-0.1667, -0.1750, -0.1299,  0.0269,  0.0601,  0.0701,  0.0734, -0.3130,\n",
      "        -0.0842, -0.0530,  0.1080,  0.0060,  0.1990, -0.0330, -0.1485,  0.2995,\n",
      "         0.3755,  0.0283,  0.2545, -0.0048,  0.0042, -0.4663, -0.0588,  0.0370,\n",
      "        -0.0009,  0.0143, -0.1311,  0.0057,  0.0379,  0.0101, -0.0616, -0.1305,\n",
      "         0.1308,  0.0333,  0.0149,  0.1633, -0.0426, -0.0676, -0.0988,  0.0502,\n",
      "         0.0308, -0.0152, -0.0695,  0.0762,  0.0937,  0.0349,  0.1512,  0.1078,\n",
      "         0.0442,  0.0408,  0.0249,  0.0952,  0.1265,  0.1413], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1667, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1750, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1299, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0269, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0601, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0701, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0734, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.3130, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0842, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0530, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1080, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0060, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1990, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0908, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2995, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3755, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0283, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0645, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.4663, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0009, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0143, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1311, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0057, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0379, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0101, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0616, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1305, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1308, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0333, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0603, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0676, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0988, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0502, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0308, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0152, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0695, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0762, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1078, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0442, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0408, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0249, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1413, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'was', 'the', 'name', 'of', 'the', 'media', 'day', 'event', 'for', 'super', 'bowl', '50', '?', '[SEP]', '[SEP]', 'the', 'game', \"'\", 's', 'media', 'day', ',', 'which', 'was', 'typically', 'held', 'on', 'the', 'tuesday', 'afternoon', 'prior', 'to', 'the', 'game', ',', 'was', 'moved', 'to', 'the', 'monday', 'evening', 'and', 're', '-', 'branded', 'as', 'super', 'bowl', 'opening', 'night', '.', '[SEP]']\n",
      "len conti_raw 46\n",
      "conti_raw ['[CLS]', 'what', 'was', 'the', 'name', 'of', 'the', 'media', 'day', 'event', 'for', 'super', 'bowl', '50?', '[SEP]', '[SEP]', 'the', \"game's\", 'media', 'day,', 'which', 'was', 'typically', 'held', 'on', 'the', 'tuesday', 'afternoon', 'prior', 'to', 'the', 'game,', 'was', 'moved', 'to', 'the', 'monday', 'evening', 'and', 're-branded', 'as', 'super', 'bowl', 'opening', 'night.', '[SEP]']\n",
      "pred_prob 0.7368832230567932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.74)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.72</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> name                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> media                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> day                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> event                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> super                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bowl                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 50?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> media                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> day,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> typically                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> held                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tuesday                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> afternoon                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> prior                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> moved                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> monday                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> evening                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> re-branded                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> super                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bowl                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> opening                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> night.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['How many Doctor Who soundtracks have been released since 2005?', 'The fourth was released on 4 October 2010 as a two disc special edition and contained music from the 20082010 specials (The Next Doctor to End of Time Part 2).']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.2470,  0.0160, -0.0582, -0.2237, -0.0535, -0.0021,  0.0200,  0.0925,\n",
      "         0.0226, -0.0960,  0.1181,  0.1263,  0.3179,  0.0800,  0.0066, -0.5453,\n",
      "         0.0857,  0.0140,  0.0693,  0.1211,  0.0441,  0.1958,  0.0930,  0.0447,\n",
      "         0.0316,  0.0126,  0.0469, -0.0032,  0.0383, -0.0880, -0.0533,  0.0194,\n",
      "         0.0513,  0.0431,  0.1114,  0.0169, -0.0341,  0.0801,  0.0521,  0.0123,\n",
      "        -0.0848,  0.0770, -0.0138,  0.0017,  0.0136,  0.0817,  0.1124,  0.0527,\n",
      "         0.0462,  0.5196], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2470, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0160, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0582, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2237, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0535, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0021, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0200, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0925, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0226, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0960, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1222, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3179, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0800, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0066, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.5453, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0857, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0140, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0693, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1211, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0441, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1958, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0930, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0447, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0316, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0126, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0469, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0032, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0383, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0880, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0533, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0194, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0513, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0341, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0661, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0123, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0848, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0770, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0138, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0017, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0136, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0817, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0643, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.5196, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'how', 'many', 'doctor', 'who', 'soundtracks', 'have', 'been', 'released', 'since', '2005', '?', '[SEP]', '[SEP]', 'the', 'fourth', 'was', 'released', 'on', '4', 'october', '2010', 'as', 'a', 'two', 'disc', 'special', 'edition', 'and', 'contained', 'music', 'from', 'the', '2008', '', '2010', 'specials', '(', 'the', 'next', 'doctor', 'to', 'end', 'of', 'time', 'part', '2', ')', '.', '[SEP]']\n",
      "len conti_raw 44\n",
      "conti_raw ['[CLS]', 'how', 'many', 'doctor', 'who', 'soundtracks', 'have', 'been', 'released', 'since', '2005?', '[SEP]', '[SEP]', 'the', 'fourth', 'was', 'released', 'on', '4', 'october', '2010', 'as', 'a', 'two', 'disc', 'special', 'edition', 'and', 'contained', 'music', 'from', 'the', '20082010', 'specials', '(the', 'next', 'doctor', 'to', 'end', 'of', 'time', 'part', '2).', '[SEP]']\n",
      "pred_prob 0.8182130455970764\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.82)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.88</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> soundtracks                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> released                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> since                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2005?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fourth                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> released                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 4                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> october                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2010                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> special                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> edition                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> contained                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> music                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 20082010                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> specials                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> next                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> end                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> part                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2).                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"What is the name of the country's longest continuously running student film society?\", 'Students at the University of Chicago run over 400 clubs and organizations known as Recognized Student Organizations (RSOs).']\n",
      "GT target: 0\n",
      "word attr tensor([-0.1352, -0.0044,  0.1199,  0.1279,  0.3796,  0.0690,  0.0608,  0.0192,\n",
      "        -0.0293,  0.0353,  0.0043, -0.0066,  0.0039, -0.0422, -0.0470, -0.1921,\n",
      "         0.1815,  0.3194,  0.2170,  0.0763,  0.2299,  0.0833,  0.1006,  0.0680,\n",
      "         0.1914,  0.3408,  0.1541,  0.2854,  0.1822, -0.0497,  0.1528,  0.0342,\n",
      "         0.1635,  0.0726, -0.0133,  0.3065,  0.1410, -0.0311,  0.0412,  0.0096,\n",
      "         0.0320,  0.1570], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1352, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0044, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1199, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1279, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3796, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0690, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0608, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0152, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0043, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0066, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0039, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0422, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0470, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3194, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2170, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0763, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2299, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0833, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1006, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0680, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1914, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3408, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1541, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2854, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1822, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0497, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1528, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0342, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1635, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0726, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.3065, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0304, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1570, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'is', 'the', 'name', 'of', 'the', 'country', \"'\", 's', 'longest', 'continuously', 'running', 'student', 'film', 'society', '?', '[SEP]', '[SEP]', 'students', 'at', 'the', 'university', 'of', 'chicago', 'run', 'over', '400', 'clubs', 'and', 'organizations', 'known', 'as', 'recognized', 'student', 'organizations', '(', 'rs', 'os', ')', '.', '[SEP]']\n",
      "len conti_raw 35\n",
      "conti_raw ['[CLS]', 'what', 'is', 'the', 'name', 'of', 'the', \"country's\", 'longest', 'continuously', 'running', 'student', 'film', 'society?', '[SEP]', '[SEP]', 'students', 'at', 'the', 'university', 'of', 'chicago', 'run', 'over', '400', 'clubs', 'and', 'organizations', 'known', 'as', 'recognized', 'student', 'organizations', '(rsos).', '[SEP]']\n",
      "pred_prob 0.9198501110076904\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.92)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>3.81</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> name                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> country's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> longest                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> continuously                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> running                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> student                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> society?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> students                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> at                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> university                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chicago                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> run                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> over                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 400                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clubs                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> organizations                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> known                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recognized                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> student                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> organizations                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (rsos).                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['How many times has the South Florida/Miami area hosted the Super Bowl?', 'The South Florida/Miami area has previously hosted the event 10 times (tied for most with New Orleans), with the most recent one being Super Bowl XLIV in 2010.']\n",
      "GT target: 1\n",
      "word attr tensor([ 4.0527e-04,  9.2501e-02,  9.3217e-02, -2.5911e-01,  1.4960e-01,\n",
      "         8.2904e-02, -5.6716e-03, -3.3650e-02,  6.0343e-03, -2.9225e-02,\n",
      "         7.9439e-02,  3.4508e-01,  5.6496e-02, -7.1825e-02, -1.3204e-01,\n",
      "        -3.4905e-02,  5.1371e-01,  2.7981e-01,  9.1404e-02,  1.0758e-02,\n",
      "        -9.0789e-02,  4.4840e-02,  1.9659e-02,  4.2070e-02,  9.9307e-02,\n",
      "        -5.1174e-02,  2.3187e-01,  7.9979e-02, -3.2047e-02, -8.0813e-02,\n",
      "        -1.3460e-01,  6.9023e-02, -3.1578e-02,  1.3928e-01,  1.2995e-01,\n",
      "        -3.5256e-02,  9.0615e-02,  3.2348e-01, -4.7914e-02,  2.8908e-02,\n",
      "         5.1457e-02,  1.1181e-01,  8.0123e-02, -4.1949e-02,  3.7031e-02,\n",
      "         7.3949e-02, -4.1112e-03, -1.5565e-01, -6.0088e-02, -1.0763e-01,\n",
      "         1.0722e-01,  1.6208e-01,  6.1383e-02, -1.0444e-02], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0925, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0932, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2591, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1496, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0829, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0057, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0215, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0794, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3451, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0565, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0718, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0835, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.5137, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2798, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0914, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0108, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0017, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0421, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0993, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0512, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2319, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0800, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0320, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0808, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1346, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0187, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1393, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1299, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0353, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0906, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0833, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0515, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1118, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0801, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0419, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0370, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0739, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0041, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1557, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0839, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1072, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0104, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'how', 'many', 'times', 'has', 'the', 'south', 'florida', '/', 'miami', 'area', 'hosted', 'the', 'super', 'bowl', '?', '[SEP]', '[SEP]', 'the', 'south', 'florida', '/', 'miami', 'area', 'has', 'previously', 'hosted', 'the', 'event', '10', 'times', '(', 'tied', 'for', 'most', 'with', 'new', 'orleans', ')', ',', 'with', 'the', 'most', 'recent', 'one', 'being', 'super', 'bowl', 'xl', 'iv', 'in', '2010', '.', '[SEP]']\n",
      "len conti_raw 44\n",
      "conti_raw ['[CLS]', 'how', 'many', 'times', 'has', 'the', 'south', 'florida/miami', 'area', 'hosted', 'the', 'super', 'bowl?', '[SEP]', '[SEP]', 'the', 'south', 'florida/miami', 'area', 'has', 'previously', 'hosted', 'the', 'event', '10', 'times', '(tied', 'for', 'most', 'with', 'new', 'orleans),', 'with', 'the', 'most', 'recent', 'one', 'being', 'super', 'bowl', 'xliv', 'in', '2010.', '[SEP]']\n",
      "pred_prob 0.8666946887969971\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.87)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>2.33</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> south                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> florida/miami                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> area                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hosted                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> super                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bowl?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> south                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> florida/miami                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> area                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> previously                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hosted                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> event                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 10                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (tied                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> new                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> orleans),                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recent                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> being                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> super                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bowl                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> xliv                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2010.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What is different about Paulinella chromatophora?', 'It is not clear whether that symbiont is closely related to the ancestral chloroplast of other eukaryotes.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.1127,  0.0425, -0.0368, -0.0994, -0.0841,  0.0633,  0.1001,  0.0042,\n",
      "        -0.0014,  0.0070,  0.1159,  0.0216,  0.0797,  0.7625,  0.0335,  0.0472,\n",
      "        -0.0103,  0.0053,  0.0445, -0.2762,  0.0603, -0.1165, -0.0849, -0.0804,\n",
      "        -0.0535,  0.0401, -0.0079,  0.1688, -0.0634, -0.0157,  0.0090, -0.0723,\n",
      "        -0.0380, -0.0392,  0.0211, -0.0521, -0.0456, -0.0138, -0.0474, -0.0132,\n",
      "         0.0781,  0.0402, -0.1006,  0.4093], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1127, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0425, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0368, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0994, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0841, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0817, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0603, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.7625, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0335, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0472, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0103, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0053, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0445, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2762, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0603, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0720, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0401, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0079, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1688, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0634, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0157, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0090, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0326, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0456, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0138, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0343, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4093, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'is', 'different', 'about', 'pauline', 'lla', 'ch', 'rom', 'ato', 'ph', 'ora', '?', '[SEP]', '[SEP]', 'it', 'is', 'not', 'clear', 'whether', 'that', 'sy', 'mb', 'ion', 't', 'is', 'closely', 'related', 'to', 'the', 'ancestral', 'ch', 'lor', 'op', 'las', 't', 'of', 'other', 'eu', 'kar', 'yo', 'tes', '.', '[SEP]']\n",
      "len conti_raw 27\n",
      "conti_raw ['[CLS]', 'what', 'is', 'different', 'about', 'paulinella', 'chromatophora?', '[SEP]', '[SEP]', 'it', 'is', 'not', 'clear', 'whether', 'that', 'symbiont', 'is', 'closely', 'related', 'to', 'the', 'ancestral', 'chloroplast', 'of', 'other', 'eukaryotes.', '[SEP]']\n",
      "pred_prob 0.5214384198188782\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.52)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.91</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> different                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paulinella                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chromatophora?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clear                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> whether                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> symbiont                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> closely                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> related                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ancestral                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chloroplast                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eukaryotes.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"Who played Doctor Who on stage in the 70's?\", 'Doctor Who has appeared on stage numerous times.']\n",
      "GT target: 0\n",
      "word attr tensor([-0.2265,  0.4146, -0.0236, -0.3764,  0.3059,  0.0813,  0.0484,  0.0834,\n",
      "         0.0110, -0.0536,  0.0537,  0.0239,  0.4705,  0.2123,  0.0962, -0.2423,\n",
      "         0.0347, -0.0818, -0.1340,  0.0030,  0.0236, -0.0538, -0.2090, -0.2736,\n",
      "         0.1906], device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.2265, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.4146, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0236, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.3764, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.3059, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0813, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0484, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0834, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0110, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2123, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0962, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2423, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0347, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0818, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1340, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0030, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0236, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0538, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1906, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'played', 'doctor', 'who', 'on', 'stage', 'in', 'the', '70', \"'\", 's', '?', '[SEP]', '[SEP]', 'doctor', 'who', 'has', 'appeared', 'on', 'stage', 'numerous', 'times', '.', '[SEP]']\n",
      "len conti_raw 21\n",
      "conti_raw ['[CLS]', 'who', 'played', 'doctor', 'who', 'on', 'stage', 'in', 'the', \"70's?\", '[SEP]', '[SEP]', 'doctor', 'who', 'has', 'appeared', 'on', 'stage', 'numerous', 'times.', '[SEP]']\n",
      "pred_prob 0.8973642587661743\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.90)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> played                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 70's?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doctor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> appeared                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> numerous                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Who do clinical pharmacists work with much of the time?', 'Clinical pharmacists often collaborate with physicians and other healthcare professionals to improve pharmaceutical care.']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.2233,  0.2497, -0.0596, -0.0393, -0.0494, -0.1047,  0.0213, -0.1706,\n",
      "         0.1146, -0.1451,  0.2807, -0.0472, -0.0264,  0.1079,  0.2293,  0.5092,\n",
      "         0.3025, -0.0464, -0.0216, -0.0644,  0.0701, -0.1592, -0.2230,  0.1137,\n",
      "         0.0875, -0.0391, -0.0697,  0.0970, -0.0833,  0.0532, -0.0229,  0.0858,\n",
      "        -0.0258, -0.0435, -0.0743,  0.4000], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2233, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2497, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0596, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0393, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0992, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1146, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1451, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2807, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0472, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0264, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1686, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.5092, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3025, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0464, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0728, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.2230, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1137, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0875, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0391, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0697, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0970, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0833, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0532, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0229, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0858, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0258, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0589, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4000, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'do', 'clinical', 'ph', 'arm', 'ac', 'ists', 'work', 'with', 'much', 'of', 'the', 'time', '?', '[SEP]', '[SEP]', 'clinical', 'ph', 'arm', 'ac', 'ists', 'often', 'collaborate', 'with', 'physicians', 'and', 'other', 'healthcare', 'professionals', 'to', 'improve', 'pharmaceutical', 'care', '.', '[SEP]']\n",
      "len conti_raw 28\n",
      "conti_raw ['[CLS]', 'who', 'do', 'clinical', 'pharmacists', 'work', 'with', 'much', 'of', 'the', 'time?', '[SEP]', '[SEP]', 'clinical', 'pharmacists', 'often', 'collaborate', 'with', 'physicians', 'and', 'other', 'healthcare', 'professionals', 'to', 'improve', 'pharmaceutical', 'care.', '[SEP]']\n",
      "pred_prob 0.8218711018562317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.82)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.43</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> do                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clinical                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pharmacists                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> work                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> much                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clinical                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pharmacists                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> often                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> collaborate                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> physicians                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> healthcare                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> professionals                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> improve                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pharmaceutical                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> care.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['In which county does Jacksonville reside?', 'It is the county seat of Duval County, with which the city government consolidated in 1968.']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.1379, -0.1506,  0.2918, -0.0029,  0.0624,  0.2530,  0.1181, -0.0221,\n",
      "         0.4741,  0.3296,  0.0977, -0.1990,  0.0737,  0.2000,  0.1946,  0.0632,\n",
      "        -0.2533, -0.0034,  0.0591,  0.1256, -0.0534,  0.1549,  0.3392,  0.1874,\n",
      "         0.0055,  0.1237,  0.0157, -0.0800,  0.1965], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1379, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1506, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2918, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0029, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0624, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2530, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0480, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4741, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3296, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0977, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1990, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0737, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2000, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1946, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0632, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2533, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0279, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1256, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0534, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1549, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3392, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1874, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0055, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1237, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0321, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1965, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'in', 'which', 'county', 'does', 'jacksonville', 'reside', '?', '[SEP]', '[SEP]', 'it', 'is', 'the', 'county', 'seat', 'of', 'duval', 'county', ',', 'with', 'which', 'the', 'city', 'government', 'consolidated', 'in', '1968', '.', '[SEP]']\n",
      "len conti_raw 26\n",
      "conti_raw ['[CLS]', 'in', 'which', 'county', 'does', 'jacksonville', 'reside?', '[SEP]', '[SEP]', 'it', 'is', 'the', 'county', 'seat', 'of', 'duval', 'county,', 'with', 'which', 'the', 'city', 'government', 'consolidated', 'in', '1968.', '[SEP]']\n",
      "pred_prob 0.8931249976158142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.89)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>2.74</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> county                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> does                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> jacksonville                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reside?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> county                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seat                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> duval                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> county,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> city                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> government                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> consolidated                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1968.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Who did Genghis Khan charge with finding and punishing the Shah?', 'Genghis Khan ordered the wholesale massacre of many of the civilians, enslaved the rest of the population and executed Inalchuq by pouring molten silver into his ears and eyes, as retribution for his actions.']\n",
      "GT target: 0\n",
      "word attr tensor([ 2.2959e-01,  4.5153e-01,  1.2564e-01, -6.1879e-03,  1.2689e-01,\n",
      "         2.1177e-02,  1.9863e-01, -5.8850e-02,  2.5757e-01, -1.7715e-01,\n",
      "         2.3384e-02, -2.5735e-01, -4.9690e-03, -3.9086e-02,  2.4535e-01,\n",
      "        -2.6692e-01,  1.8093e-01, -2.7490e-02,  6.7585e-03,  2.4060e-01,\n",
      "        -2.8978e-02,  2.9409e-02, -6.1822e-02, -4.2260e-02, -4.7545e-02,\n",
      "        -1.1679e-01, -4.5656e-02,  1.7886e-02,  3.6339e-02, -9.9099e-03,\n",
      "        -9.8769e-02, -8.4420e-02,  2.8451e-01, -2.3797e-03, -7.8200e-02,\n",
      "         1.5251e-02, -5.2732e-03, -7.8123e-02,  5.1345e-02,  1.2754e-01,\n",
      "        -1.3492e-01, -1.6109e-01,  9.0064e-02,  2.7838e-02,  9.1439e-02,\n",
      "        -1.4781e-02, -9.6117e-02,  9.8008e-02,  7.2309e-02,  6.5534e-02,\n",
      "        -9.6035e-03, -4.3756e-04, -7.3477e-02, -6.0947e-02, -1.5501e-02,\n",
      "        -6.8809e-02, -6.4648e-03,  1.8314e-02, -4.8796e-02, -8.6961e-02,\n",
      "         9.1945e-02], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2296, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4515, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1256, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1986, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0588, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2576, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1771, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0234, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1312, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0391, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1809, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0275, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0294, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0618, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0423, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0475, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1168, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0457, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0179, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0363, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0099, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0916, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2845, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0024, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0782, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0153, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0053, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0781, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0513, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1275, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0914, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0148, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0961, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0980, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0723, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0655, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0096, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0004, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0672, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0155, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0688, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0065, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0183, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0679, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0919, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'did', 'gen', 'ghi', 's', 'khan', 'charge', 'with', 'finding', 'and', 'punish', 'ing', 'the', 'shah', '?', '[SEP]', '[SEP]', 'gen', 'ghi', 's', 'khan', 'ordered', 'the', 'wholesale', 'massacre', 'of', 'many', 'of', 'the', 'civilians', ',', 'enslaved', 'the', 'rest', 'of', 'the', 'population', 'and', 'executed', 'ina', 'lch', 'u', 'q', 'by', 'pouring', 'molten', 'silver', 'into', 'his', 'ears', 'and', 'eyes', ',', 'as', 'retribution', 'for', 'his', 'actions', '.', '[SEP]']\n",
      "len conti_raw 49\n",
      "conti_raw ['[CLS]', 'who', 'did', 'genghis', 'khan', 'charge', 'with', 'finding', 'and', 'punishing', 'the', 'shah?', '[SEP]', '[SEP]', 'genghis', 'khan', 'ordered', 'the', 'wholesale', 'massacre', 'of', 'many', 'of', 'the', 'civilians,', 'enslaved', 'the', 'rest', 'of', 'the', 'population', 'and', 'executed', 'inalchuq', 'by', 'pouring', 'molten', 'silver', 'into', 'his', 'ears', 'and', 'eyes,', 'as', 'retribution', 'for', 'his', 'actions.', '[SEP]']\n",
      "pred_prob 0.7466775178909302\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.75)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.91</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> genghis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> khan                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> charge                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> finding                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> punishing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> shah?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> genghis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> khan                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ordered                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> wholesale                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> massacre                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> civilians,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enslaved                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rest                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> population                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> executed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> inalchuq                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pouring                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> molten                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> silver                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> into                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ears                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eyes,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> retribution                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> actions.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What entity enforces the Charter of Fundamental Rights of the European Union?', 'In effect, after the Lisbon Treaty, the Charter and the Convention now co-exist under European Union law, though the former is enforced by the European Court of Justice in relation to European Union measures, and the latter by the European Court of Human Rights in relation to measures by member states.']\n",
      "GT target: 1\n",
      "word attr tensor([ 6.2101e-02, -1.2580e-01,  3.3339e-01, -5.7018e-01,  3.6367e-02,\n",
      "         5.6001e-02, -1.4770e-01, -1.6413e-02,  6.9668e-02, -5.9408e-03,\n",
      "        -9.5813e-05,  2.6742e-02,  9.6789e-02,  2.3918e-02,  6.5213e-02,\n",
      "         2.6139e-01,  1.7391e-01,  4.5838e-02,  5.0508e-02, -2.5596e-02,\n",
      "        -4.1419e-02,  7.4366e-02, -1.0616e-01,  6.8651e-02,  1.5325e-03,\n",
      "         1.3733e-02, -2.1161e-01,  6.6769e-03,  4.4545e-02, -3.7256e-02,\n",
      "        -2.7968e-02,  6.5954e-02,  2.7809e-02, -2.7213e-02, -1.7538e-03,\n",
      "         6.6933e-02, -3.9583e-02,  4.3497e-02,  1.8895e-02,  9.4995e-02,\n",
      "         7.0003e-02, -2.8957e-01,  2.0580e-01, -2.7235e-01, -6.9253e-03,\n",
      "         7.0275e-02,  9.6899e-02, -1.8701e-02, -3.1090e-02, -1.9001e-02,\n",
      "        -7.0768e-03, -1.0455e-02, -1.4779e-02,  1.2113e-01, -2.8117e-02,\n",
      "        -4.3203e-02, -2.3147e-03,  1.3808e-02,  4.0905e-02, -1.6701e-01,\n",
      "         2.9085e-02,  5.8449e-02,  8.4940e-02,  7.8194e-03, -1.8690e-02,\n",
      "        -3.4964e-02,  1.2185e-02, -5.9763e-03, -2.0162e-02,  2.3576e-02,\n",
      "         2.5973e-02, -9.2909e-03, -4.5816e-02,  4.6755e-02, -8.6037e-02,\n",
      "        -5.2381e-02], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0621, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1258, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.3334, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2669, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0560, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1477, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0164, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0697, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0059, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-9.5813e-05, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0267, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0968, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0446, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2614, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1739, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0458, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0414, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0744, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1062, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0351, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0137, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2116, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0067, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0445, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0373, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0280, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0098, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0018, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0669, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0396, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0312, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0950, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0700, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2896, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2058, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2724, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0069, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0703, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0969, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0311, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0190, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0071, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0105, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0148, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1211, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0281, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0228, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0138, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0409, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1670, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0291, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0584, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0849, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0078, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0350, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0122, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0060, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0202, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0236, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0260, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0093, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0458, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0196, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0524, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'entity', 'enforce', 's', 'the', 'charter', 'of', 'fundamental', 'rights', 'of', 'the', 'european', 'union', '?', '[SEP]', '[SEP]', 'in', 'effect', ',', 'after', 'the', 'lisbon', 'treaty', ',', 'the', 'charter', 'and', 'the', 'convention', 'now', 'co', '-', 'exist', 'under', 'european', 'union', 'law', ',', 'though', 'the', 'former', 'is', 'enforced', 'by', 'the', 'european', 'court', 'of', 'justice', 'in', 'relation', 'to', 'european', 'union', 'measures', ',', 'and', 'the', 'latter', 'by', 'the', 'european', 'court', 'of', 'human', 'rights', 'in', 'relation', 'to', 'measures', 'by', 'member', 'states', '.', '[SEP]']\n",
      "len conti_raw 67\n",
      "conti_raw ['[CLS]', 'what', 'entity', 'enforces', 'the', 'charter', 'of', 'fundamental', 'rights', 'of', 'the', 'european', 'union?', '[SEP]', '[SEP]', 'in', 'effect,', 'after', 'the', 'lisbon', 'treaty,', 'the', 'charter', 'and', 'the', 'convention', 'now', 'co-exist', 'under', 'european', 'union', 'law,', 'though', 'the', 'former', 'is', 'enforced', 'by', 'the', 'european', 'court', 'of', 'justice', 'in', 'relation', 'to', 'european', 'union', 'measures,', 'and', 'the', 'latter', 'by', 'the', 'european', 'court', 'of', 'human', 'rights', 'in', 'relation', 'to', 'measures', 'by', 'member', 'states.', '[SEP]']\n",
      "pred_prob 0.3837520182132721\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.38)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.17</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> entity                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enforces                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> charter                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fundamental                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rights                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> european                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> union?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> effect,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> after                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lisbon                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> treaty,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> charter                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> convention                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> now                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> co-exist                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> under                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> european                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> union                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> law,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> though                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> former                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enforced                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> european                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> court                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> justice                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> relation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> european                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> union                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> measures,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> latter                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> european                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> court                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> human                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rights                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> relation                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> measures                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> member                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> states.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"Most of the museum's collection had been returned by which year?\", 'Before the return of the collections after the war, the Britain Can Make It exhibition was held between September and November 1946, attracting nearly a million and a half visitors.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.0918,  0.0394, -0.0162,  0.0102, -0.1053, -0.0041, -0.0215, -0.1178,\n",
      "         0.0257,  0.0239, -0.0143, -0.0514,  0.8558, -0.0184,  0.2358,  0.1767,\n",
      "         0.1154, -0.0187,  0.0206,  0.0019, -0.0310,  0.0365, -0.0316, -0.0393,\n",
      "         0.0247,  0.0066, -0.0559,  0.0106,  0.0796,  0.0246,  0.0049,  0.0231,\n",
      "        -0.0943, -0.0302,  0.0623, -0.0745,  0.1461,  0.0510, -0.0721, -0.1532,\n",
      "        -0.0175, -0.0997, -0.0221,  0.0065,  0.0308,  0.0169, -0.0064,  0.0326,\n",
      "        -0.0119, -0.0619,  0.1460], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0918, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0394, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0162, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0102, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1178, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0257, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0239, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0143, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0514, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.8558, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1087, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1767, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1154, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0206, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0019, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0310, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0365, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0316, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0393, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0247, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0247, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0106, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0796, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0246, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0049, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0231, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0943, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0302, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0623, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0745, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1461, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0510, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0721, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0853, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0997, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0221, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0308, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0169, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0064, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0326, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0369, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1460, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'most', 'of', 'the', 'museum', \"'\", 's', 'collection', 'had', 'been', 'returned', 'by', 'which', 'year', '?', '[SEP]', '[SEP]', 'before', 'the', 'return', 'of', 'the', 'collections', 'after', 'the', 'war', ',', 'the', 'britain', 'can', 'make', 'it', 'exhibition', 'was', 'held', 'between', 'september', 'and', 'november', '1946', ',', 'attracting', 'nearly', 'a', 'million', 'and', 'a', 'half', 'visitors', '.', '[SEP]']\n",
      "len conti_raw 45\n",
      "conti_raw ['[CLS]', 'most', 'of', 'the', \"museum's\", 'collection', 'had', 'been', 'returned', 'by', 'which', 'year?', '[SEP]', '[SEP]', 'before', 'the', 'return', 'of', 'the', 'collections', 'after', 'the', 'war,', 'the', 'britain', 'can', 'make', 'it', 'exhibition', 'was', 'held', 'between', 'september', 'and', 'november', '1946,', 'attracting', 'nearly', 'a', 'million', 'and', 'a', 'half', 'visitors.', '[SEP]']\n",
      "pred_prob 0.8580110669136047\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.86)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.13</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> museum's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> collection                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> had                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> returned                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 58%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> year?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> before                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> return                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> collections                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> after                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> war,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> britain                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> can                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> make                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exhibition                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> held                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> september                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> november                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1946,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> attracting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nearly                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> million                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> half                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> visitors.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Within the 30 days how many digiboxes had been sold?', \"Within 30 days, over 100,000 digiboxes had been sold, which help bolstered BSkyB's decision to give away free digiboxes and minidishes from May 1999.\"]\n",
      "GT target: 1\n",
      "word attr tensor([ 0.0194, -0.1353,  0.0726,  0.0377, -0.0210,  0.0043,  0.0078, -0.0585,\n",
      "        -0.0611,  0.0404, -0.1260, -0.0393, -0.0952, -0.0355,  0.0271,  0.6750,\n",
      "         0.4620, -0.2061,  0.0107,  0.0056, -0.0684, -0.0137,  0.0142, -0.0407,\n",
      "         0.0402, -0.1141, -0.0617,  0.0483, -0.0985, -0.0537, -0.0320,  0.0091,\n",
      "         0.0255,  0.0371,  0.1698,  0.0244, -0.0239,  0.0945,  0.0994,  0.1241,\n",
      "         0.0507,  0.0060, -0.0033, -0.1135,  0.0229,  0.0755, -0.0390,  0.0280,\n",
      "        -0.0635,  0.0405,  0.0622, -0.0391, -0.0443,  0.0480,  0.0083,  0.0098,\n",
      "         0.0091,  0.0099,  0.1376,  0.0885,  0.2263], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0194, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1353, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0726, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0377, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0210, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0043, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0078, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0679, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0393, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0952, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0042, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.6750, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4620, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2061, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0107, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0314, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0137, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0135, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0591, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0537, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0320, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0173, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0371, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1698, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0201, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1135, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0229, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0755, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0390, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0280, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0069, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0443, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0189, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0091, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0099, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1130, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2263, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'within', 'the', '30', 'days', 'how', 'many', 'dig', 'ib', 'ox', 'es', 'had', 'been', 'sold', '?', '[SEP]', '[SEP]', 'within', '30', 'days', ',', 'over', '100', ',', '000', 'dig', 'ib', 'ox', 'es', 'had', 'been', 'sold', ',', 'which', 'help', 'bo', 'lster', 'ed', 'bs', 'ky', 'b', \"'\", 's', 'decision', 'to', 'give', 'away', 'free', 'dig', 'ib', 'ox', 'es', 'and', 'mini', 'dis', 'hes', 'from', 'may', '1999', '.', '[SEP]']\n",
      "len conti_raw 38\n",
      "conti_raw ['[CLS]', 'within', 'the', '30', 'days', 'how', 'many', 'digiboxes', 'had', 'been', 'sold?', '[SEP]', '[SEP]', 'within', '30', 'days,', 'over', '100,000', 'digiboxes', 'had', 'been', 'sold,', 'which', 'help', 'bolstered', \"bskyb's\", 'decision', 'to', 'give', 'away', 'free', 'digiboxes', 'and', 'minidishes', 'from', 'may', '1999.', '[SEP]']\n",
      "pred_prob 0.7189930081367493\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.72)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.29</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> within                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 30                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> days                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> digiboxes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> had                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sold?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> within                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 30                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> days,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> over                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 100,000                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> digiboxes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> had                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sold,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> help                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bolstered                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bskyb's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decision                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> give                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> away                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> free                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> digiboxes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> minidishes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> may                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1999.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['The receptors on a killer T cell must bind to how many MHC: antigen complexes in order to activate the cell?', \"The MHC:antigen complex is also recognized by the helper cell's CD4 co-receptor, which recruits molecules inside the T cell (e.g., Lck) that are responsible for the T cell's activation.\"]\n",
      "GT target: 0\n",
      "word attr tensor([ 2.4964e-01,  1.4252e-02, -8.0942e-02,  5.2945e-02, -3.5137e-02,\n",
      "         3.1200e-02,  9.2377e-02,  5.9336e-02,  2.7388e-02,  2.9436e-01,\n",
      "         9.9476e-03,  1.5625e-01, -4.7127e-02,  1.0581e-02,  3.2911e-03,\n",
      "        -1.5913e-01, -5.3753e-02,  2.7684e-02, -4.7659e-02, -5.7724e-02,\n",
      "         1.0792e-01,  2.7733e-01,  6.3919e-02,  5.7503e-02,  2.4111e-01,\n",
      "         3.3736e-01,  9.2265e-02,  6.3863e-02,  1.1040e-01, -3.7805e-02,\n",
      "        -1.5850e-01,  6.0111e-02,  5.6531e-02, -8.1526e-02,  2.4192e-02,\n",
      "        -2.0975e-01,  1.2208e-01,  4.3282e-02,  9.5052e-03, -1.1096e-01,\n",
      "         4.3314e-02,  6.2947e-02,  2.5935e-02, -5.9706e-02,  1.8347e-01,\n",
      "        -2.5246e-03,  2.9854e-02, -5.0812e-02,  5.8472e-02,  1.5410e-01,\n",
      "         9.5608e-02,  1.4791e-01,  3.2752e-02,  3.7278e-02,  8.2335e-02,\n",
      "         6.4435e-02, -2.8327e-02, -1.6155e-01, -4.8606e-02,  1.9017e-01,\n",
      "        -8.4805e-02, -1.0663e-02,  8.7837e-02,  1.3214e-02,  2.6049e-02,\n",
      "         2.0744e-01, -1.3072e-04,  3.9435e-02,  1.4055e-01,  7.0957e-02,\n",
      "        -1.8477e-02,  4.4418e-02,  1.0336e-01,  4.3236e-02,  7.2092e-03,\n",
      "        -3.6709e-02,  2.6989e-01], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2496, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0143, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0809, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0529, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0351, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0312, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0924, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0593, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0274, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2944, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0099, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1563, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0471, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0761, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0538, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0277, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0477, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0577, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1079, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2773, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0639, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1493, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3374, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0923, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0639, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0565, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0815, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0242, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2097, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1221, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0433, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0507, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0395, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0619, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0199, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1541, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0956, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1479, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0328, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0373, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0823, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0644, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2074, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0001, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0394, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1406, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0710, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0185, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0586, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0147, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2699, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'the', 'receptors', 'on', 'a', 'killer', 't', 'cell', 'must', 'bind', 'to', 'how', 'many', 'm', 'hc', ':', 'antigen', 'complexes', 'in', 'order', 'to', 'activate', 'the', 'cell', '?', '[SEP]', '[SEP]', 'the', 'm', 'hc', ':', 'antigen', 'complex', 'is', 'also', 'recognized', 'by', 'the', 'help', 'er', 'cell', \"'\", 's', 'cd', '4', 'co', '-', 'receptor', ',', 'which', 'recruits', 'molecules', 'inside', 'the', 't', 'cell', '(', 'e', '.', 'g', '.', ',', 'lc', 'k', ')', 'that', 'are', 'responsible', 'for', 'the', 't', 'cell', \"'\", 's', 'activation', '.', '[SEP]']\n",
      "len conti_raw 54\n",
      "conti_raw ['[CLS]', 'the', 'receptors', 'on', 'a', 'killer', 't', 'cell', 'must', 'bind', 'to', 'how', 'many', 'mhc:', 'antigen', 'complexes', 'in', 'order', 'to', 'activate', 'the', 'cell?', '[SEP]', '[SEP]', 'the', 'mhc:antigen', 'complex', 'is', 'also', 'recognized', 'by', 'the', 'helper', \"cell's\", 'cd4', 'co-receptor,', 'which', 'recruits', 'molecules', 'inside', 'the', 't', 'cell', '(e.g.,', 'lck)', 'that', 'are', 'responsible', 'for', 'the', 't', \"cell's\", 'activation.', '[SEP]']\n",
      "pred_prob 0.9368581175804138\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.94)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>3.38</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> receptors                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> killer                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> t                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cell                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> must                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bind                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mhc:                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> antigen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> complexes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> order                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> activate                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cell?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mhc:antigen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> complex                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recognized                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> helper                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cell's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cd4                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> co-receptor,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recruits                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> molecules                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> inside                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> t                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cell                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (e.g.,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lck)                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> responsible                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> t                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cell's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> activation.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"How much did Westinghouse pay for Tesla's designs?\", \"In July 1888, Brown and Peck negotiated a licensing deal with George Westinghouse for Tesla's polyphase induction motor and transformer designs for $60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor.\"]\n",
      "GT target: 1\n",
      "word attr tensor([-2.2523e-02, -7.5996e-02,  2.3606e-01,  6.8518e-02, -8.3733e-02,\n",
      "         3.7705e-03, -2.7743e-02,  2.8854e-01,  1.0809e-01,  1.2810e-01,\n",
      "        -7.1850e-03, -7.6983e-03, -6.4708e-03, -6.3559e-03,  5.2962e-01,\n",
      "         3.2720e-01, -6.5541e-02,  4.7409e-02, -2.8472e-01, -6.2360e-02,\n",
      "         1.8218e-02,  8.8779e-03, -2.0483e-03,  1.3838e-01,  5.0561e-02,\n",
      "        -1.0935e-01,  1.7769e-01,  7.8202e-02,  2.8604e-02, -6.0679e-02,\n",
      "        -2.8537e-03, -2.7625e-02,  9.4289e-02,  5.6760e-02,  3.4307e-02,\n",
      "         2.8506e-02,  1.0573e-02,  3.0943e-02, -2.6233e-03, -7.0198e-02,\n",
      "        -2.4808e-02,  9.1062e-02,  1.7874e-01,  1.2089e-03, -7.6339e-03,\n",
      "         1.0206e-01,  6.6651e-02, -2.2305e-01, -3.2384e-02, -1.0987e-02,\n",
      "        -3.8905e-03,  8.6404e-03,  9.2625e-02, -2.6886e-02,  1.6680e-02,\n",
      "         2.6879e-02, -5.2287e-02,  1.5642e-02, -5.1698e-02, -7.4037e-02,\n",
      "        -9.4920e-02,  2.1351e-03,  6.6917e-02,  1.5493e-02,  1.0879e-01,\n",
      "         3.9391e-02,  3.6806e-02,  1.0078e-01, -6.6751e-02, -4.9319e-04,\n",
      "         2.5778e-01], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0225, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0760, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2361, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0685, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0339, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2885, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1081, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0264, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0064, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.5296, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3272, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0655, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0474, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1735, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0182, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0089, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0020, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1384, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0506, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1094, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1777, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0782, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0286, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0297, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0943, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0370, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0091, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0702, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0248, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0911, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0900, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0076, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1021, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0331, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0039, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0086, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0926, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0269, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0167, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0269, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0523, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0156, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0669, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0155, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1088, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0394, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0368, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1008, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0336, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2578, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'how', 'much', 'did', 'west', 'ing', 'house', 'pay', 'for', 'tesla', \"'\", 's', 'designs', '?', '[SEP]', '[SEP]', 'in', 'july', '1888', ',', 'brown', 'and', 'peck', 'negotiated', 'a', 'licensing', 'deal', 'with', 'george', 'west', 'ing', 'house', 'for', 'tesla', \"'\", 's', 'poly', 'pha', 'se', 'induction', 'motor', 'and', 'transform', 'er', 'designs', 'for', '$', '60', ',', '000', 'in', 'cash', 'and', 'stock', 'and', 'a', 'royalty', 'of', '$', '2', '.', '50', 'per', 'ac', 'horsepower', 'produced', 'by', 'each', 'motor', '.', '[SEP]']\n",
      "len conti_raw 51\n",
      "conti_raw ['[CLS]', 'how', 'much', 'did', 'westinghouse', 'pay', 'for', \"tesla's\", 'designs?', '[SEP]', '[SEP]', 'in', 'july', '1888,', 'brown', 'and', 'peck', 'negotiated', 'a', 'licensing', 'deal', 'with', 'george', 'westinghouse', 'for', \"tesla's\", 'polyphase', 'induction', 'motor', 'and', 'transformer', 'designs', 'for', '$60,000', 'in', 'cash', 'and', 'stock', 'and', 'a', 'royalty', 'of', '$2.50', 'per', 'ac', 'horsepower', 'produced', 'by', 'each', 'motor.', '[SEP]']\n",
      "pred_prob 0.7738507390022278\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.77)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>2.13</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> much                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> westinghouse                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pay                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tesla's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> designs?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> july                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1888,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> brown                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> peck                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> negotiated                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> licensing                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> deal                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> george                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> westinghouse                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tesla's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> polyphase                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> induction                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> motor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> transformer                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> designs                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> $60,000                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cash                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stock                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> royalty                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> $2.50                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> per                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ac                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> horsepower                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> produced                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> each                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> motor.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"What type of movies were produced in Jacksonville's 30 studios?\", 'One converted movie studio site, Norman Studios, remains in Arlington; It has been converted to the Jacksonville Silent Film Museum at Norman Studios.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.2738, -0.2909, -0.4096,  0.0570,  0.0048, -0.0192, -0.1916, -0.0582,\n",
      "         0.0941, -0.0450,  0.0378, -0.1778, -0.0944,  0.0641,  0.1717,  0.0899,\n",
      "         0.0925, -0.1061, -0.1176,  0.0141,  0.0529, -0.0597, -0.0014, -0.1023,\n",
      "         0.0015, -0.0511,  0.1286,  0.5649,  0.0384,  0.1266, -0.0166,  0.1101,\n",
      "         0.0550,  0.1238,  0.0018, -0.0723, -0.0400, -0.1121, -0.0136,  0.0728,\n",
      "        -0.0935, -0.0392,  0.2283, -0.0999], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2738, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2909, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.4096, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0570, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0048, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0192, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1916, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0582, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0312, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1778, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0151, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1717, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0899, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0925, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1061, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1176, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0141, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0034, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0014, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0504, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0511, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1286, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3017, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1266, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0166, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1101, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0550, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1238, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0018, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0723, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0400, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1121, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0136, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0728, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0935, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0945, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0999, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'type', 'of', 'movies', 'were', 'produced', 'in', 'jacksonville', \"'\", 's', '30', 'studios', '?', '[SEP]', '[SEP]', 'one', 'converted', 'movie', 'studio', 'site', ',', 'norman', 'studios', ',', 'remains', 'in', 'arlington', ';', 'it', 'has', 'been', 'converted', 'to', 'the', 'jacksonville', 'silent', 'film', 'museum', 'at', 'norman', 'studios', '.', '[SEP]']\n",
      "len conti_raw 37\n",
      "conti_raw ['[CLS]', 'what', 'type', 'of', 'movies', 'were', 'produced', 'in', \"jacksonville's\", '30', 'studios?', '[SEP]', '[SEP]', 'one', 'converted', 'movie', 'studio', 'site,', 'norman', 'studios,', 'remains', 'in', 'arlington;', 'it', 'has', 'been', 'converted', 'to', 'the', 'jacksonville', 'silent', 'film', 'museum', 'at', 'norman', 'studios.', '[SEP]']\n",
      "pred_prob 0.575401782989502\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.58)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.19</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> type                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movies                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> produced                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> jacksonville's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 30                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> studios?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> converted                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> studio                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> site,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> norman                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> studios,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> remains                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> arlington;                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> converted                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> jacksonville                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> silent                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> museum                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> at                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> norman                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> studios.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What voyager said that Mombasa was a great harbour and moored small crafts and great ships?', 'The Swahili built Mombasa into a major port city and established trade links with other nearby city-states, as well as commercial centres in Persia, Arabia, and even India.']\n",
      "GT target: 0\n",
      "word attr tensor([-0.0402, -0.0861,  0.5651,  0.1167,  0.0604,  0.0605,  0.1506,  0.0190,\n",
      "         0.1351,  0.0510,  0.0688, -0.0181, -0.0256, -0.0334, -0.0232,  0.0201,\n",
      "         0.0414, -0.0386,  0.1084,  0.0697,  0.0813,  0.4307,  0.3878,  0.0806,\n",
      "        -0.0721,  0.0032,  0.0514,  0.0865,  0.0379,  0.0939,  0.0494, -0.0272,\n",
      "        -0.0140,  0.0376,  0.0400,  0.0476, -0.0331, -0.0281,  0.0554, -0.0936,\n",
      "         0.0140,  0.0512, -0.0549,  0.0544,  0.0173, -0.0614, -0.0097,  0.0501,\n",
      "        -0.0009,  0.0294, -0.0121, -0.0915, -0.0102,  0.1891, -0.0137,  0.0665,\n",
      "         0.0122,  0.0964,  0.0864,  0.0136, -0.0373,  0.3139], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0402, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0861, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.5651, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1167, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0604, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0623, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1351, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0510, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0688, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0181, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0256, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0283, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0201, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0414, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0386, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1084, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0755, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4307, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3878, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0806, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0085, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0865, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0576, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0272, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0140, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0376, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0400, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0476, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0331, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0281, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0554, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0936, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0140, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0512, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0549, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0112, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0501, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0009, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0294, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0121, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0915, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0102, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0877, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0964, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0864, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0118, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3139, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'voyager', 'said', 'that', 'mom', 'bas', 'a', 'was', 'a', 'great', 'harbour', 'and', 'moore', 'd', 'small', 'crafts', 'and', 'great', 'ships', '?', '[SEP]', '[SEP]', 'the', 'sw', 'ah', 'ili', 'built', 'mom', 'bas', 'a', 'into', 'a', 'major', 'port', 'city', 'and', 'established', 'trade', 'links', 'with', 'other', 'nearby', 'city', '-', 'states', ',', 'as', 'well', 'as', 'commercial', 'centres', 'in', 'persia', ',', 'arabia', ',', 'and', 'even', 'india', '.', '[SEP]']\n",
      "len conti_raw 48\n",
      "conti_raw ['[CLS]', 'what', 'voyager', 'said', 'that', 'mombasa', 'was', 'a', 'great', 'harbour', 'and', 'moored', 'small', 'crafts', 'and', 'great', 'ships?', '[SEP]', '[SEP]', 'the', 'swahili', 'built', 'mombasa', 'into', 'a', 'major', 'port', 'city', 'and', 'established', 'trade', 'links', 'with', 'other', 'nearby', 'city-states,', 'as', 'well', 'as', 'commercial', 'centres', 'in', 'persia,', 'arabia,', 'and', 'even', 'india.', '[SEP]']\n",
      "pred_prob 0.9446513652801514\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.94)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>3.12</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> voyager                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mombasa                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> harbour                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> moored                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> small                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> crafts                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ships?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> swahili                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> built                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mombasa                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> into                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> major                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> port                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> city                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> established                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> trade                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> links                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nearby                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> city-states,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> well                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> commercial                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> centres                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> persia,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> arabia,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> even                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> india.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Where is Energiprojekt AB based?', 'Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.2099, -0.3623, -0.0615,  0.1064, -0.0185,  0.0095,  0.0199,  0.0399,\n",
      "         0.0551,  0.2203, -0.1018,  0.0158,  0.4777,  0.3637, -0.0725,  0.0239,\n",
      "         0.0024, -0.0038, -0.0027,  0.1255, -0.0125, -0.1959, -0.0845,  0.2172,\n",
      "         0.0548, -0.0672, -0.0597,  0.0317,  0.0685,  0.0065,  0.1380,  0.0863,\n",
      "        -0.0820, -0.1593, -0.0028,  0.0871,  0.0300,  0.0651,  0.0016,  0.0139,\n",
      "         0.0075, -0.2081, -0.0056, -0.0740,  0.0250,  0.0848, -0.0078,  0.1619,\n",
      "         0.0542,  0.0022,  0.3094], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2099, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.3623, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0615, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2203, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0430, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4777, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3637, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0725, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0239, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0619, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0125, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1959, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0845, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2172, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0548, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0672, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0597, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0317, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0375, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1380, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0863, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0820, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1593, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0028, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0585, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0651, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0016, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0139, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0075, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2081, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0056, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0740, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0250, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0848, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0078, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1619, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0282, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3094, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'where', 'is', 'en', 'er', 'gi', 'pro', 'je', 'kt', 'ab', 'based', '?', '[SEP]', '[SEP]', 'although', 'the', 'rec', 'ip', 'ro', 'cating', 'steam', 'engine', 'is', 'no', 'longer', 'in', 'widespread', 'commercial', 'use', ',', 'various', 'companies', 'are', 'exploring', 'or', 'exploit', 'ing', 'the', 'potential', 'of', 'the', 'engine', 'as', 'an', 'alternative', 'to', 'internal', 'combustion', 'engines', '.', '[SEP]']\n",
      "len conti_raw 39\n",
      "conti_raw ['[CLS]', 'where', 'is', 'energiprojekt', 'ab', 'based?', '[SEP]', '[SEP]', 'although', 'the', 'reciprocating', 'steam', 'engine', 'is', 'no', 'longer', 'in', 'widespread', 'commercial', 'use,', 'various', 'companies', 'are', 'exploring', 'or', 'exploiting', 'the', 'potential', 'of', 'the', 'engine', 'as', 'an', 'alternative', 'to', 'internal', 'combustion', 'engines.', '[SEP]']\n",
      "pred_prob 0.8502684831619263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.85)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.53</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> where                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> energiprojekt                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ab                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> based?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> although                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reciprocating                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> steam                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> engine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> no                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> longer                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> widespread                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> commercial                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> use,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> various                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> companies                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exploring                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exploiting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> potential                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> engine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alternative                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> internal                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> combustion                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> engines.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Who other than Tesla did Westinghouse consider for the patents?', \"Westinghouse looked into getting a patent on a similar commutator-less, rotating magnetic field-based induction motor presented in a paper in March 1888 by the Italian physicist Galileo Ferraris, but decided Tesla's patent would probably control the market.\"]\n",
      "GT target: 1\n",
      "word attr tensor([-0.2286, -0.0293,  0.2587,  0.0106,  0.4359,  0.2197, -0.1141, -0.0178,\n",
      "        -0.1164, -0.1839, -0.0289,  0.0765,  0.1331, -0.0980,  0.3410,  0.2082,\n",
      "        -0.1830, -0.0408, -0.2235, -0.1244,  0.0981,  0.0445,  0.0281,  0.1237,\n",
      "         0.0326,  0.0518,  0.0206, -0.0169,  0.0250, -0.0563,  0.0413, -0.0234,\n",
      "        -0.0229,  0.1076,  0.0513, -0.0032,  0.0591, -0.0096,  0.0267,  0.0607,\n",
      "        -0.0358, -0.0204,  0.0148,  0.0176,  0.0251, -0.0292,  0.0512,  0.1087,\n",
      "         0.1649, -0.0138, -0.0750, -0.1360, -0.0353, -0.0118, -0.0136, -0.0491,\n",
      "         0.1635, -0.0432, -0.0071, -0.0063,  0.1326,  0.1959, -0.1712,  0.1025,\n",
      "         0.0756, -0.0924, -0.0747,  0.0876], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.2286, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0293, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2587, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0106, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4359, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2197, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0912, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1839, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0289, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0765, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0176, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3410, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2082, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1677, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1244, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0981, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0445, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0281, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1237, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0326, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0518, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0206, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0154, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1076, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0513, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0092, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0267, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0607, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0358, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0204, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0148, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0176, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0251, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0292, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0512, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1087, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1649, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0138, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0750, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1360, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0186, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0491, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1635, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0157, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1326, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1959, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1712, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1025, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0756, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0836, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0876, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'other', 'than', 'tesla', 'did', 'west', 'ing', 'house', 'consider', 'for', 'the', 'patents', '?', '[SEP]', '[SEP]', 'west', 'ing', 'house', 'looked', 'into', 'getting', 'a', 'patent', 'on', 'a', 'similar', 'com', 'mut', 'ator', '-', 'less', ',', 'rotating', 'magnetic', 'field', '-', 'based', 'induction', 'motor', 'presented', 'in', 'a', 'paper', 'in', 'march', '1888', 'by', 'the', 'italian', 'physicist', 'galileo', 'ferrari', 's', ',', 'but', 'decided', 'tesla', \"'\", 's', 'patent', 'would', 'probably', 'control', 'the', 'market', '.', '[SEP]']\n",
      "len conti_raw 51\n",
      "conti_raw ['[CLS]', 'who', 'other', 'than', 'tesla', 'did', 'westinghouse', 'consider', 'for', 'the', 'patents?', '[SEP]', '[SEP]', 'westinghouse', 'looked', 'into', 'getting', 'a', 'patent', 'on', 'a', 'similar', 'commutator-less,', 'rotating', 'magnetic', 'field-based', 'induction', 'motor', 'presented', 'in', 'a', 'paper', 'in', 'march', '1888', 'by', 'the', 'italian', 'physicist', 'galileo', 'ferraris,', 'but', 'decided', \"tesla's\", 'patent', 'would', 'probably', 'control', 'the', 'market.', '[SEP]']\n",
      "pred_prob 0.7733870148658752\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.77)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.26</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> than                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tesla                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> westinghouse                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> consider                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> patents?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> westinghouse                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> looked                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> into                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> getting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> patent                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> similar                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> commutator-less,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rotating                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> magnetic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> field-based                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> induction                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> motor                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> presented                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paper                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> march                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1888                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> italian                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> physicist                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> galileo                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ferraris,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decided                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tesla's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> patent                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> probably                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> control                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> market.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What separates the neuroimmune system and peripheral immune system in humans?', 'In humans, the bloodbrain barrier, bloodcerebrospinal fluid barrier, and similar fluidbrain barriers separate the peripheral immune system from the neuroimmune system which protects the brain.']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.7354,  0.1096,  0.0127, -0.0187, -0.0615, -0.0594, -0.0252, -0.0966,\n",
      "        -0.0240,  0.0431,  0.0030, -0.0735, -0.0827,  0.0326,  0.0126, -0.1034,\n",
      "         0.2522,  0.2207,  0.1242,  0.0403,  0.0012,  0.0196,  0.0464, -0.0247,\n",
      "        -0.0641, -0.0369,  0.0808,  0.0499, -0.0174, -0.0282, -0.0304, -0.0330,\n",
      "        -0.0312,  0.0051, -0.0612, -0.0555,  0.0721, -0.0369,  0.1060,  0.0470,\n",
      "        -0.0486, -0.0591, -0.0063, -0.0529,  0.1518, -0.0721, -0.0251, -0.0663,\n",
      "         0.0395, -0.0142, -0.0446, -0.0436, -0.0102,  0.0076, -0.1207, -0.0037,\n",
      "         0.0411,  0.0387,  0.0600,  0.0035,  0.0311, -0.0731,  0.3864],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.7354, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1096, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0127, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0187, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0431, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0030, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0735, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0827, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0326, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0126, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0744, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2207, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1242, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0403, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0104, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0464, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0653, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0369, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0555, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0176, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1060, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0470, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0301, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0529, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1518, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0721, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0251, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0663, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0395, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0142, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0446, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0345, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0411, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0387, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0600, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0035, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0210, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3864, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'separates', 'the', 'ne', 'uro', 'im', 'mun', 'e', 'system', 'and', 'peripheral', 'immune', 'system', 'in', 'humans', '?', '[SEP]', '[SEP]', 'in', 'humans', ',', 'the', 'blood', '', 'brain', 'barrier', ',', 'blood', '', 'ce', 're', 'bro', 'sp', 'inal', 'fluid', 'barrier', ',', 'and', 'similar', 'fluid', '', 'brain', 'barriers', 'separate', 'the', 'peripheral', 'immune', 'system', 'from', 'the', 'ne', 'uro', 'im', 'mun', 'e', 'system', 'which', 'protects', 'the', 'brain', '.', '[SEP]']\n",
      "len conti_raw 40\n",
      "conti_raw ['[CLS]', 'what', 'separates', 'the', 'neuroimmune', 'system', 'and', 'peripheral', 'immune', 'system', 'in', 'humans?', '[SEP]', '[SEP]', 'in', 'humans,', 'the', 'bloodbrain', 'barrier,', 'bloodcerebrospinal', 'fluid', 'barrier,', 'and', 'similar', 'fluidbrain', 'barriers', 'separate', 'the', 'peripheral', 'immune', 'system', 'from', 'the', 'neuroimmune', 'system', 'which', 'protects', 'the', 'brain.', '[SEP]']\n",
      "pred_prob 0.8900112509727478\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.89)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.17</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> separates                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> neuroimmune                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> peripheral                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immune                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bloodbrain                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> barrier,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bloodcerebrospinal                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fluid                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> barrier,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> similar                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fluidbrain                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> barriers                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> separate                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> peripheral                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immune                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> neuroimmune                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> protects                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> brain.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"What did Kublai's government have to balance between?\", \"Kublai's government after 1262 was a compromise between preserving Mongol interests in China and satisfying the demands of his Chinese subjects.\"]\n",
      "GT target: 1\n",
      "word attr tensor([-0.1237,  0.0944,  0.0523,  0.1007, -0.0976, -0.1462,  0.0260, -0.0462,\n",
      "        -0.2106, -0.1569, -0.2192,  0.0167, -0.2830,  0.1112,  0.1346,  0.1134,\n",
      "         0.0122, -0.0885, -0.0847,  0.0357, -0.0478, -0.2262, -0.1424, -0.0590,\n",
      "         0.0618, -0.1352, -0.0264,  0.0149, -0.3018, -0.1683, -0.0835, -0.1690,\n",
      "        -0.0934, -0.0811, -0.0902, -0.1925, -0.0799, -0.1813, -0.1111,  0.1011,\n",
      "        -0.1759, -0.3035, -0.3009,  0.2845], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1237, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0944, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0523, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0347, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.2106, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1569, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2192, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0167, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0859, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1346, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1134, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0303, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.2262, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1424, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0014, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1352, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0264, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.3018, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1683, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0835, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1690, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0934, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0811, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0902, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1925, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0799, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1813, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1111, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1011, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1759, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.3022, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2845, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'did', 'ku', 'bla', 'i', \"'\", 's', 'government', 'have', 'to', 'balance', 'between', '?', '[SEP]', '[SEP]', 'ku', 'bla', 'i', \"'\", 's', 'government', 'after', '126', '2', 'was', 'a', 'compromise', 'between', 'preserving', 'mongol', 'interests', 'in', 'china', 'and', 'satisfying', 'the', 'demands', 'of', 'his', 'chinese', 'subjects', '.', '[SEP]']\n",
      "len conti_raw 33\n",
      "conti_raw ['[CLS]', 'what', 'did', \"kublai's\", 'government', 'have', 'to', 'balance', 'between?', '[SEP]', '[SEP]', \"kublai's\", 'government', 'after', '1262', 'was', 'a', 'compromise', 'between', 'preserving', 'mongol', 'interests', 'in', 'china', 'and', 'satisfying', 'the', 'demands', 'of', 'his', 'chinese', 'subjects.', '[SEP]']\n",
      "pred_prob 0.13032764196395874\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.13)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>-3.27</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kublai's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> government                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balance                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kublai's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> government                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> after                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1262                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> compromise                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> preserving                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mongol                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interests                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> china                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> satisfying                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> demands                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chinese                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> subjects.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"What did Gasquet's book blame the plague on?\", 'The historian Francis Aidan Gasquet wrote about the \\'Great Pestilence\\' in 1893 and suggested that \"it would appear to be some form of the ordinary Eastern or bubonic plague\".']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.2990,  0.0759,  0.0366, -0.1301, -0.2942, -0.0128,  0.0320,  0.2230,\n",
      "        -0.3770,  0.1521,  0.1131,  0.0294,  0.0612,  0.2886,  0.1321, -0.0005,\n",
      "         0.0983,  0.0535, -0.1313,  0.0677, -0.2685, -0.0105,  0.0068,  0.0290,\n",
      "        -0.0134,  0.0306, -0.0114, -0.0127,  0.0960,  0.0131,  0.0281, -0.1943,\n",
      "        -0.1114,  0.0708,  0.1018,  0.1043,  0.0565, -0.0558, -0.0448,  0.0267,\n",
      "         0.0443,  0.1514,  0.0056,  0.0932,  0.0713,  0.0108, -0.0791,  0.0699,\n",
      "         0.1147,  0.2333, -0.0278, -0.2253,  0.1092,  0.0799,  0.2718],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2990, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0759, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0366, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2230, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.3770, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1521, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1131, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0453, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2886, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1321, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0005, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0983, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0535, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1313, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1004, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0105, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0068, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0290, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0086, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0276, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0281, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1943, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1114, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0708, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1018, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0804, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0558, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0448, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0267, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0443, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1514, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0056, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0932, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0713, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0108, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0791, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0699, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0731, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2718, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'did', 'gas', 'quet', \"'\", 's', 'book', 'blame', 'the', 'plague', 'on', '?', '[SEP]', '[SEP]', 'the', 'historian', 'francis', 'aidan', 'gas', 'quet', 'wrote', 'about', 'the', \"'\", 'great', 'pest', 'ile', 'nce', \"'\", 'in', '1893', 'and', 'suggested', 'that', '\"', 'it', 'would', 'appear', 'to', 'be', 'some', 'form', 'of', 'the', 'ordinary', 'eastern', 'or', 'bu', 'bon', 'ic', 'plague', '\"', '.', '[SEP]']\n",
      "len conti_raw 41\n",
      "conti_raw ['[CLS]', 'what', 'did', \"gasquet's\", 'book', 'blame', 'the', 'plague', 'on?', '[SEP]', '[SEP]', 'the', 'historian', 'francis', 'aidan', 'gasquet', 'wrote', 'about', 'the', \"'great\", \"pestilence'\", 'in', '1893', 'and', 'suggested', 'that', '\"it', 'would', 'appear', 'to', 'be', 'some', 'form', 'of', 'the', 'ordinary', 'eastern', 'or', 'bubonic', 'plague\".', '[SEP]']\n",
      "pred_prob 0.7593749761581421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.76)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.48</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> gasquet's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> book                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> blame                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plague                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> historian                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> francis                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> aidan                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> gasquet                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> wrote                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 'great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pestilence'                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1893                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> suggested                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> appear                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> form                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ordinary                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eastern                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bubonic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plague\".                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Who shared sideline duties with Evan Washburn?', 'In the United States, the game was televised by CBS, as part of a cycle between the three main broadcast television partners of the NFL.']\n",
      "GT target: 0\n",
      "word attr tensor([-2.7169e-02,  4.9507e-01,  1.2849e-01,  9.0164e-02,  1.1487e-04,\n",
      "         2.1983e-01,  2.0255e-01, -1.4569e-01,  2.6678e-02, -8.2250e-02,\n",
      "         8.7064e-03,  3.9535e-01,  4.0473e-01,  1.7755e-01,  5.8628e-02,\n",
      "        -1.1142e-01,  4.8731e-02,  3.6169e-02,  6.5628e-02,  5.9319e-02,\n",
      "         1.6481e-01, -1.0576e-02,  1.5389e-01,  1.6557e-01,  6.7165e-02,\n",
      "         1.6001e-01,  1.3444e-02,  3.8200e-02, -1.4265e-02, -1.8916e-01,\n",
      "         5.2515e-02,  1.0172e-01,  7.6534e-02,  1.4789e-01,  5.5396e-02,\n",
      "         1.8605e-02,  2.1943e-02,  8.1251e-02,  9.9153e-02, -4.0032e-02,\n",
      "         6.9098e-03,  1.5327e-01], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0272, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.4951, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1285, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0451, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2198, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2025, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1457, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0095, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3953, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4047, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1776, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0586, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1114, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0656, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0593, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1648, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0106, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1539, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1164, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1600, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0134, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0382, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0143, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1892, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0525, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1017, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0765, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1479, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0554, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0186, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0219, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0813, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0992, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0166, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1533, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'shared', 'side', 'line', 'duties', 'with', 'evan', 'wash', 'burn', '?', '[SEP]', '[SEP]', 'in', 'the', 'united', 'states', ',', 'the', 'game', 'was', 'televised', 'by', 'cbs', ',', 'as', 'part', 'of', 'a', 'cycle', 'between', 'the', 'three', 'main', 'broadcast', 'television', 'partners', 'of', 'the', 'nfl', '.', '[SEP]']\n",
      "len conti_raw 36\n",
      "conti_raw ['[CLS]', 'who', 'shared', 'sideline', 'duties', 'with', 'evan', 'washburn?', '[SEP]', '[SEP]', 'in', 'the', 'united', 'states,', 'the', 'game', 'was', 'televised', 'by', 'cbs,', 'as', 'part', 'of', 'a', 'cycle', 'between', 'the', 'three', 'main', 'broadcast', 'television', 'partners', 'of', 'the', 'nfl.', '[SEP]']\n",
      "pred_prob 0.9370878338813782\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.94)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>3.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> shared                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sideline                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> duties                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> evan                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> washburn?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> united                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> states,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> game                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> televised                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cbs,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> part                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cycle                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> three                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> main                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> broadcast                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> television                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> partners                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nfl.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Who was added to party as Washington went on the way?', 'Washington left with a small party, picking up along the way Jacob Van Braam as an interpreter; Christopher Gist, a company surveyor working in the area; and a few Mingo led by Tanaghrisson.']\n",
      "GT target: 1\n",
      "word attr tensor([-0.2057,  0.1332,  0.0483, -0.0112,  0.0325, -0.0341,  0.0233,  0.1219,\n",
      "         0.0861,  0.0651,  0.0438,  0.0951,  0.0281,  0.5027,  0.4220,  0.1576,\n",
      "        -0.0069,  0.0100,  0.0371, -0.0500,  0.0432,  0.0585, -0.0175,  0.0644,\n",
      "         0.0310,  0.0016,  0.0763, -0.0632,  0.0261, -0.0065, -0.0275,  0.0445,\n",
      "        -0.0044, -0.0234, -0.0062,  0.0053,  0.0618, -0.0194,  0.0176,  0.0253,\n",
      "         0.0666, -0.3408, -0.0321,  0.0078,  0.0325, -0.0670,  0.0026,  0.1613,\n",
      "         0.0738,  0.0721, -0.0072, -0.0019,  0.0172,  0.1202,  0.0722,  0.0838,\n",
      "         0.0521,  0.0754, -0.0221,  0.4552], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.2057, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1332, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0483, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0112, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0325, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0341, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0233, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1219, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0861, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0651, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0438, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0616, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.5027, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4220, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1576, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0069, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0100, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0371, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0500, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0508, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0175, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0644, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0310, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0016, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0763, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0632, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0261, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0170, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0445, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0044, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0148, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0053, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0194, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0253, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0666, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.3408, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0321, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0078, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0325, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0322, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1613, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0738, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0721, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0045, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0172, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1202, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0241, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4552, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'was', 'added', 'to', 'party', 'as', 'washington', 'went', 'on', 'the', 'way', '?', '[SEP]', '[SEP]', 'washington', 'left', 'with', 'a', 'small', 'party', ',', 'picking', 'up', 'along', 'the', 'way', 'jacob', 'van', 'bra', 'am', 'as', 'an', 'interpreter', ';', 'christopher', 'gi', 'st', ',', 'a', 'company', 'surveyor', 'working', 'in', 'the', 'area', ';', 'and', 'a', 'few', 'ming', 'o', 'led', 'by', 'tan', 'agh', 'ris', 'son', '.', '[SEP]']\n",
      "len conti_raw 48\n",
      "conti_raw ['[CLS]', 'who', 'was', 'added', 'to', 'party', 'as', 'washington', 'went', 'on', 'the', 'way?', '[SEP]', '[SEP]', 'washington', 'left', 'with', 'a', 'small', 'party,', 'picking', 'up', 'along', 'the', 'way', 'jacob', 'van', 'braam', 'as', 'an', 'interpreter;', 'christopher', 'gist,', 'a', 'company', 'surveyor', 'working', 'in', 'the', 'area;', 'and', 'a', 'few', 'mingo', 'led', 'by', 'tanaghrisson.', '[SEP]']\n",
      "pred_prob 0.8867630362510681\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.89)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>2.61</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> added                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> party                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> washington                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> went                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> way?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> washington                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> left                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> small                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> party,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> picking                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> up                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> along                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> way                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> jacob                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> van                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> braam                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interpreter;                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> christopher                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> gist,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> company                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> surveyor                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> working                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> area;                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> few                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mingo                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> led                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tanaghrisson.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What did Queen Elizabeth II open in Newcastle in 1981?', \"It was opened in five phases between 1980 and 1984, and was Britain's first urban light rail transit system; two extensions were opened in 1991 and 2002.\"]\n",
      "GT target: 0\n",
      "word attr tensor([ 0.0752,  0.0570,  0.0249, -0.0011, -0.1955, -0.0735, -0.0530,  0.1177,\n",
      "         0.0955,  0.0742, -0.1956,  0.4609,  0.0688,  0.0204,  0.1170,  0.0095,\n",
      "         0.1159,  0.0699,  0.0091,  0.4135,  0.0817, -0.1003,  0.0625, -0.1035,\n",
      "         0.0770,  0.0418, -0.0696, -0.0424,  0.0100, -0.0578,  0.2511,  0.0146,\n",
      "         0.0916,  0.0933,  0.3663,  0.2439,  0.0450, -0.0031, -0.0548, -0.0842,\n",
      "         0.0622,  0.1113,  0.1486,  0.2153, -0.0181, -0.0226, -0.1891],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0752, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0570, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0249, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0011, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1955, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0735, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0530, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1177, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0955, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0742, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1327, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0688, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0204, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1170, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0095, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1159, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0699, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0091, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4135, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0817, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1003, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0625, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0133, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0418, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0696, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0370, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2511, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0146, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0916, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0933, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3663, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1444, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0031, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0548, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0842, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0622, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1113, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1486, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2153, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0204, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1891, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'did', 'queen', 'elizabeth', 'ii', 'open', 'in', 'newcastle', 'in', '1981', '?', '[SEP]', '[SEP]', 'it', 'was', 'opened', 'in', 'five', 'phases', 'between', '1980', 'and', '1984', ',', 'and', 'was', 'britain', \"'\", 's', 'first', 'urban', 'light', 'rail', 'transit', 'system', ';', 'two', 'extensions', 'were', 'opened', 'in', '1991', 'and', '2002', '.', '[SEP]']\n",
      "len conti_raw 41\n",
      "conti_raw ['[CLS]', 'what', 'did', 'queen', 'elizabeth', 'ii', 'open', 'in', 'newcastle', 'in', '1981?', '[SEP]', '[SEP]', 'it', 'was', 'opened', 'in', 'five', 'phases', 'between', '1980', 'and', '1984,', 'and', 'was', \"britain's\", 'first', 'urban', 'light', 'rail', 'transit', 'system;', 'two', 'extensions', 'were', 'opened', 'in', '1991', 'and', '2002.', '[SEP]']\n",
      "pred_prob 0.8243213295936584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.82)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>2.38</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> queen                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> elizabeth                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ii                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> open                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> newcastle                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1981?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> opened                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> five                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> phases                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1980                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1984,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> britain's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> first                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> urban                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> light                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rail                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> transit                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system;                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> extensions                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> opened                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1991                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2002.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What writing inspired the name Great Yuan?', 'Furthermore, the Yuan is sometimes known as the \"Empire of the Great Khan\" or \"Khanate of the Great Khan\", which particularly appeared on some Yuan maps, since Yuan emperors held the nominal title of Great Khan.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.1030,  0.0269,  0.4436,  0.2488,  0.0679,  0.1240,  0.0034,  0.1357,\n",
      "         0.1201,  0.2334,  0.1410,  0.0835, -0.0339,  0.0612,  0.0488,  0.0438,\n",
      "         0.0591,  0.0340,  0.0480,  0.0464,  0.0174,  0.0574, -0.0094,  0.0363,\n",
      "        -0.0045, -0.0199,  0.0083,  0.0417,  0.0388,  0.0234, -0.0099, -0.0275,\n",
      "         0.0193, -0.0189,  0.0671,  0.0040, -0.0107,  0.0815, -0.0243, -0.0435,\n",
      "         0.0552,  0.1064,  0.0681, -0.0411, -0.0165,  0.0289, -0.7134,  0.0365,\n",
      "         0.0555,  0.0709, -0.0096,  0.0099, -0.0177, -0.0087, -0.0534, -0.0465,\n",
      "         0.0835], device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1030, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0269, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4436, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2488, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0679, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1240, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0034, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1279, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2334, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1410, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0248, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0612, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0488, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0438, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0591, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0340, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0480, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0464, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0374, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0094, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0363, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0045, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0417, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0106, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0275, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0193, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0189, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0124, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0815, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0243, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0435, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0552, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1064, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0681, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0288, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0289, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.7134, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0365, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0555, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0709, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0096, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0099, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0177, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0087, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0500, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0835, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'writing', 'inspired', 'the', 'name', 'great', 'yuan', '?', '[SEP]', '[SEP]', 'furthermore', ',', 'the', 'yuan', 'is', 'sometimes', 'known', 'as', 'the', '\"', 'empire', 'of', 'the', 'great', 'khan', '\"', 'or', '\"', 'khan', 'ate', 'of', 'the', 'great', 'khan', '\"', ',', 'which', 'particularly', 'appeared', 'on', 'some', 'yuan', 'maps', ',', 'since', 'yuan', 'emperors', 'held', 'the', 'nominal', 'title', 'of', 'great', 'khan', '.', '[SEP]']\n",
      "len conti_raw 47\n",
      "conti_raw ['[CLS]', 'what', 'writing', 'inspired', 'the', 'name', 'great', 'yuan?', '[SEP]', '[SEP]', 'furthermore,', 'the', 'yuan', 'is', 'sometimes', 'known', 'as', 'the', '\"empire', 'of', 'the', 'great', 'khan\"', 'or', '\"khanate', 'of', 'the', 'great', 'khan\",', 'which', 'particularly', 'appeared', 'on', 'some', 'yuan', 'maps,', 'since', 'yuan', 'emperors', 'held', 'the', 'nominal', 'title', 'of', 'great', 'khan.', '[SEP]']\n",
      "pred_prob 0.9457733035087585\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.95)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.87</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> writing                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> inspired                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> name                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> yuan?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> furthermore,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> yuan                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sometimes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> known                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"empire                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> khan\"                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"khanate                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> khan\",                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> particularly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> appeared                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> yuan                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> maps,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> since                    </font></mark><mark style=\"background-color: hsl(0, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> yuan                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> emperors                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> held                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nominal                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> title                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> khan.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What happened to the East India Trading Company in 1767?', 'In 1599 the British East India Company was established and was chartered by Queen Elizabeth in the following year.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.3074,  0.4086,  0.0794,  0.1430,  0.0582, -0.2965, -0.0007,  0.2979,\n",
      "        -0.0163, -0.0870, -0.3683, -0.1824, -0.1266,  0.0700,  0.4109,  0.1339,\n",
      "         0.1247,  0.0113,  0.0776, -0.2274,  0.0342, -0.0809, -0.0537,  0.0427,\n",
      "        -0.1246, -0.0314,  0.0494,  0.0872,  0.0469, -0.0287,  0.0491,  0.0776,\n",
      "         0.0064, -0.0395, -0.0580,  0.1360], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.3074, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4086, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0794, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1430, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0582, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2965, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0007, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2979, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0163, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0870, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2753, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1266, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0700, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4109, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1293, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0113, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0776, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2274, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0342, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0809, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0537, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0427, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1246, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0314, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0494, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0872, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0469, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0287, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0491, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0776, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0064, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0487, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1360, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'happened', 'to', 'the', 'east', 'india', 'trading', 'company', 'in', '1767', '?', '[SEP]', '[SEP]', 'in', '159', '9', 'the', 'british', 'east', 'india', 'company', 'was', 'established', 'and', 'was', 'chartered', 'by', 'queen', 'elizabeth', 'in', 'the', 'following', 'year', '.', '[SEP]']\n",
      "len conti_raw 33\n",
      "conti_raw ['[CLS]', 'what', 'happened', 'to', 'the', 'east', 'india', 'trading', 'company', 'in', '1767?', '[SEP]', '[SEP]', 'in', '1599', 'the', 'british', 'east', 'india', 'company', 'was', 'established', 'and', 'was', 'chartered', 'by', 'queen', 'elizabeth', 'in', 'the', 'following', 'year.', '[SEP]']\n",
      "pred_prob 0.7392330169677734\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.74)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.93</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> happened                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> east                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> india                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> trading                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> company                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1767?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1599                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> british                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> east                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> india                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> company                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> established                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chartered                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> queen                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> elizabeth                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> following                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> year.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['The principle of faunal succession was developed 100 years before whose theory of evolution?', \"Based on principles laid out by William Smith almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession were developed independently of evolutionary thought.\"]\n",
      "GT target: 1\n",
      "word attr tensor([ 0.3822, -0.1377, -0.2015, -0.1075,  0.1324, -0.0430, -0.4169, -0.0941,\n",
      "         0.0618,  0.0118,  0.0595,  0.2242, -0.1075, -0.1000, -0.0524, -0.0581,\n",
      "         0.1656, -0.0302, -0.1220, -0.1013,  0.0647, -0.1030,  0.0787,  0.0383,\n",
      "         0.0658, -0.0182, -0.1079, -0.0284,  0.0556,  0.0715,  0.1343,  0.1810,\n",
      "         0.1174, -0.0164, -0.0359,  0.0853,  0.1343,  0.1392,  0.2995,  0.1277,\n",
      "         0.0498, -0.1007, -0.0636, -0.0139, -0.1126, -0.0874, -0.2440, -0.0528,\n",
      "         0.0888,  0.0749, -0.0924,  0.0222, -0.2098, -0.1041,  0.0193],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.3822, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1377, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2015, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1075, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0447, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.4169, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0941, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0618, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0118, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0595, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2242, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1075, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1000, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0524, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0537, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0302, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1220, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1013, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0647, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1030, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0787, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0383, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0658, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0182, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1079, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0284, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0556, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0715, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1343, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1810, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1174, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0164, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0359, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0853, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2181, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1277, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0498, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0822, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0139, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1126, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0874, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2440, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0528, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0888, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0749, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0924, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0222, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1569, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0193, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'the', 'principle', 'of', 'fauna', 'l', 'succession', 'was', 'developed', '100', 'years', 'before', 'whose', 'theory', 'of', 'evolution', '?', '[SEP]', '[SEP]', 'based', 'on', 'principles', 'laid', 'out', 'by', 'william', 'smith', 'almost', 'a', 'hundred', 'years', 'before', 'the', 'publication', 'of', 'charles', 'darwin', \"'\", 's', 'theory', 'of', 'evolution', ',', 'the', 'principles', 'of', 'succession', 'were', 'developed', 'independently', 'of', 'evolutionary', 'thought', '.', '[SEP]']\n",
      "len conti_raw 49\n",
      "conti_raw ['[CLS]', 'the', 'principle', 'of', 'faunal', 'succession', 'was', 'developed', '100', 'years', 'before', 'whose', 'theory', 'of', 'evolution?', '[SEP]', '[SEP]', 'based', 'on', 'principles', 'laid', 'out', 'by', 'william', 'smith', 'almost', 'a', 'hundred', 'years', 'before', 'the', 'publication', 'of', 'charles', \"darwin's\", 'theory', 'of', 'evolution,', 'the', 'principles', 'of', 'succession', 'were', 'developed', 'independently', 'of', 'evolutionary', 'thought.', '[SEP]']\n",
      "pred_prob 0.3316514492034912\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.33)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>-0.08</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> principle                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> faunal                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> succession                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> developed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 100                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> years                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> before                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> whose                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> theory                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> evolution?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> based                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> principles                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> laid                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> out                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> william                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> smith                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> almost                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hundred                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> years                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> before                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> publication                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> charles                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> darwin's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> theory                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> evolution,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> principles                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> succession                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> developed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> independently                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> evolutionary                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thought.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['How many times did Luther preach in Halle in 1545 and 1546?', 'In 1545 and 1546 Luther preached three times in the Market Church in Halle, staying with his friend Justus Jonas during Christmas.']\n",
      "GT target: 1\n",
      "word attr tensor([-0.0707, -0.0704,  0.2476, -0.1007,  0.0779,  0.0097, -0.0193,  0.0514,\n",
      "        -0.0444,  0.0486, -0.0127, -0.1371, -0.0987,  0.0521,  0.0674, -0.2404,\n",
      "         0.4918,  0.2281,  0.1623, -0.0338, -0.0821,  0.1575, -0.0246,  0.0771,\n",
      "        -0.0558,  0.5013, -0.0792, -0.0090,  0.0182,  0.0634,  0.2554, -0.0798,\n",
      "         0.0280, -0.0959,  0.1029, -0.0492,  0.0399,  0.0687,  0.0255, -0.0232,\n",
      "         0.0529,  0.0483,  0.0210, -0.0328, -0.2248,  0.1657], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.0707, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0704, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2476, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1007, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0779, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0097, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0193, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0514, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0444, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0486, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0749, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0987, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0903, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4918, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2281, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1623, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0579, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1575, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0262, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0558, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.5013, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0792, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0090, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0182, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0634, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2554, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0798, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0280, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0035, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0492, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0399, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0687, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0255, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0483, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0210, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1288, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1657, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'how', 'many', 'times', 'did', 'luther', 'preach', 'in', 'halle', 'in', '154', '5', 'and', '154', '6', '?', '[SEP]', '[SEP]', 'in', '154', '5', 'and', '154', '6', 'luther', 'preached', 'three', 'times', 'in', 'the', 'market', 'church', 'in', 'halle', ',', 'staying', 'with', 'his', 'friend', 'just', 'us', 'jonas', 'during', 'christmas', '.', '[SEP]']\n",
      "len conti_raw 38\n",
      "conti_raw ['[CLS]', 'how', 'many', 'times', 'did', 'luther', 'preach', 'in', 'halle', 'in', '1545', 'and', '1546?', '[SEP]', '[SEP]', 'in', '1545', 'and', '1546', 'luther', 'preached', 'three', 'times', 'in', 'the', 'market', 'church', 'in', 'halle,', 'staying', 'with', 'his', 'friend', 'justus', 'jonas', 'during', 'christmas.', '[SEP]']\n",
      "pred_prob 0.909346342086792\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.91)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.48</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> how                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> many                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> luther                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> preach                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> halle                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1545                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1546?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1545                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1546                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> luther                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> preached                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> three                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> market                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> church                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> halle,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> staying                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> friend                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> justus                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> jonas                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> during                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> christmas.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What part of the Rhine flows through North Rhine-Westphalia?', 'Here the Rhine flows through the largest conurbation in Germany, the Rhine-Ruhr region.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.4713,  0.0879, -0.0900,  0.0623,  0.0508, -0.1953, -0.0583,  0.0487,\n",
      "        -0.0917,  0.0025,  0.1444,  0.1052,  0.1982,  0.3644,  0.4373,  0.0093,\n",
      "        -0.1058, -0.1067,  0.0434,  0.0881,  0.0246, -0.1121,  0.0620, -0.0541,\n",
      "         0.0809, -0.0055, -0.0560, -0.0309,  0.1692,  0.0589,  0.0711,  0.0414,\n",
      "         0.1437,  0.0125, -0.0371,  0.4289], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4713, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0879, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0900, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0623, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0508, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1953, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0583, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0487, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0917, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3644, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4373, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0093, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1058, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1067, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0434, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0881, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0246, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1121, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0424, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0055, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1692, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0985, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0123, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4289, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'part', 'of', 'the', 'rhine', 'flows', 'through', 'north', 'rhine', '-', 'westphalia', '?', '[SEP]', '[SEP]', 'here', 'the', 'rhine', 'flows', 'through', 'the', 'largest', 'con', 'ur', 'bation', 'in', 'germany', ',', 'the', 'rhine', '-', 'ru', 'hr', 'region', '.', '[SEP]']\n",
      "len conti_raw 26\n",
      "conti_raw ['[CLS]', 'what', 'part', 'of', 'the', 'rhine', 'flows', 'through', 'north', 'rhine-westphalia?', '[SEP]', '[SEP]', 'here', 'the', 'rhine', 'flows', 'through', 'the', 'largest', 'conurbation', 'in', 'germany,', 'the', 'rhine-ruhr', 'region.', '[SEP]']\n",
      "pred_prob 0.7328494191169739\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.73)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>2.26</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> part                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rhine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> flows                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> through                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> north                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rhine-westphalia?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rhine                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> flows                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> through                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> largest                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> conurbation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> germany,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rhine-ruhr                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> region.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What is the most important thing apicoplasts do?', 'The most important apicoplast function is isopentenyl pyrophosphate synthesisin fact, apicomplexans die when something interferes with this apicoplast function, and when apicomplexans are grown in an isopentenyl pyrophosphate-rich medium, they dump the organelle.']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.6529,  0.0386, -0.0527,  0.0726, -0.2934,  0.0565,  0.0127,  0.0789,\n",
      "        -0.0163,  0.0034,  0.0410, -0.0377,  0.0110, -0.0136,  0.1944,  0.0966,\n",
      "         0.0465, -0.0268,  0.1599, -0.0434, -0.0100, -0.0364,  0.0441,  0.1262,\n",
      "        -0.1139,  0.0036, -0.0609, -0.0044,  0.0319, -0.0049, -0.1044, -0.0396,\n",
      "         0.0083, -0.1241,  0.2464,  0.0771,  0.0205,  0.0679, -0.0119,  0.0096,\n",
      "        -0.0485,  0.0234,  0.0969,  0.1729,  0.0231,  0.1594, -0.0618,  0.0232,\n",
      "         0.0470, -0.0412, -0.0395, -0.0169, -0.0714, -0.0777, -0.0144, -0.0766,\n",
      "        -0.0600, -0.0550, -0.0837, -0.1103, -0.0342,  0.0889, -0.0124, -0.0298,\n",
      "        -0.0050,  0.0231,  0.0142, -0.0296,  0.0210, -0.0533, -0.0109, -0.0907,\n",
      "        -0.0296,  0.0157, -0.1404, -0.0080,  0.0428,  0.0543, -0.0197,  0.1029,\n",
      "        -0.0159,  0.1033, -0.0187, -0.1423,  0.0195,  0.1931], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.6529, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0386, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0527, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0726, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2934, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0565, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0127, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0043, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1944, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0966, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0465, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0268, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1599, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0063, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1262, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1139, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0718, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0911, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0280, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0494, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1729, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0231, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1594, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0193, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0470, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0412, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0637, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0455, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0600, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0550, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0116, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0124, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0298, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0050, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0231, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0233, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0173, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1029, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0159, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1033, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0305, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1931, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'is', 'the', 'most', 'important', 'thing', 'api', 'co', 'pl', 'ast', 's', 'do', '?', '[SEP]', '[SEP]', 'the', 'most', 'important', 'api', 'co', 'pl', 'ast', 'function', 'is', 'iso', 'pen', 'ten', 'yl', 'p', 'yr', 'op', 'hos', 'phate', 'synthesis', '', 'in', 'fact', ',', 'api', 'com', 'plex', 'ans', 'die', 'when', 'something', 'interfere', 's', 'with', 'this', 'api', 'co', 'pl', 'ast', 'function', ',', 'and', 'when', 'api', 'com', 'plex', 'ans', 'are', 'grown', 'in', 'an', 'iso', 'pen', 'ten', 'yl', 'p', 'yr', 'op', 'hos', 'phate', '-', 'rich', 'medium', ',', 'they', 'dump', 'the', 'organ', 'elle', '.', '[SEP]']\n",
      "len conti_raw 45\n",
      "conti_raw ['[CLS]', 'what', 'is', 'the', 'most', 'important', 'thing', 'apicoplasts', 'do?', '[SEP]', '[SEP]', 'the', 'most', 'important', 'apicoplast', 'function', 'is', 'isopentenyl', 'pyrophosphate', 'synthesisin', 'fact,', 'apicomplexans', 'die', 'when', 'something', 'interferes', 'with', 'this', 'apicoplast', 'function,', 'and', 'when', 'apicomplexans', 'are', 'grown', 'in', 'an', 'isopentenyl', 'pyrophosphate-rich', 'medium,', 'they', 'dump', 'the', 'organelle.', '[SEP]']\n",
      "pred_prob 0.7642964720726013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.76)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.93</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apicoplasts                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> do?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apicoplast                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> function                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> isopentenyl                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pyrophosphate                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> synthesisin                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fact,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apicomplexans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> die                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> when                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> something                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interferes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apicoplast                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> function,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> when                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apicomplexans                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> grown                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> isopentenyl                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pyrophosphate-rich                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> medium,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dump                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> organelle.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"When did ABC begin airing Dick Clark's New Year's Rockin' Eve?\", \"Since 1974, ABC has generally aired Dick Clark's New Year's Rockin' Eve on New Year's Eve (hosted first by its creator Dick Clark, and later by his successor Ryan Seacrest); the only exception was in 1999, when ABC put it on a one-year hiatus to provide coverage of the international millennium festivities, though Clark's traditional countdown from Times Square was still featured within the coverage.\"]\n",
      "GT target: 1\n",
      "word attr tensor([ 1.8834e-01, -2.4774e-02,  3.3172e-01, -1.8898e-01,  5.4220e-03,\n",
      "         3.3534e-01, -1.0442e-01, -8.9280e-02,  1.3526e-02, -8.1215e-03,\n",
      "        -2.3594e-02, -3.4920e-02, -2.6833e-02,  1.6765e-02, -1.3511e-03,\n",
      "        -4.8490e-02, -1.8720e-02, -2.7964e-02,  6.9490e-02,  5.1542e-02,\n",
      "         3.0926e-02, -5.7189e-02,  2.3070e-01, -5.3094e-02, -1.6827e-01,\n",
      "         2.5971e-02,  7.3590e-02,  3.8012e-01, -3.8763e-02, -9.5009e-02,\n",
      "        -5.0820e-03, -1.6232e-02, -3.1402e-02, -1.8894e-02, -2.0893e-02,\n",
      "         2.2725e-02, -4.6312e-02, -2.2311e-02,  6.3009e-03, -1.2664e-01,\n",
      "        -4.6808e-02,  2.2267e-02, -1.3976e-02,  1.9874e-02,  4.9318e-02,\n",
      "         2.1126e-04,  3.0749e-02,  1.2125e-01,  3.6032e-02, -1.3873e-02,\n",
      "         2.9529e-02,  5.5841e-02, -1.0745e-01, -1.1496e-01, -4.7736e-02,\n",
      "        -3.7776e-02, -7.3638e-02, -7.5610e-02, -1.4236e-02, -6.4421e-03,\n",
      "         9.0953e-02, -6.9353e-02,  1.9419e-01, -4.6288e-02, -9.9068e-02,\n",
      "        -7.4790e-02,  9.8720e-03, -1.3277e-01,  1.6707e-01,  2.1213e-01,\n",
      "        -1.1008e-03,  1.3235e-01, -4.2688e-02, -4.6266e-02, -2.9923e-02,\n",
      "         4.9056e-02,  4.5400e-02,  5.2720e-02, -4.9900e-02, -5.2760e-02,\n",
      "        -2.4635e-02, -1.1191e-01,  7.4324e-03,  4.8762e-02,  9.6750e-02,\n",
      "         1.5124e-02,  6.3536e-02,  3.5251e-02,  1.0665e-01,  6.9625e-03,\n",
      "         3.7070e-02,  1.1520e-01, -1.5369e-01,  7.4008e-02,  6.2559e-02,\n",
      "         2.2193e-02, -6.0682e-02,  2.9027e-02,  4.4254e-03, -4.7933e-02,\n",
      "        -1.4719e-02,  3.7083e-02,  6.2539e-03, -2.3875e-03,  4.5472e-02,\n",
      "        -5.9437e-02, -6.9898e-02,  1.7965e-01], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1883, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0248, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.3317, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1890, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0054, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3353, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1044, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0230, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0236, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0071, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0218, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0208, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0515, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0309, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0572, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0888, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1683, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0260, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0736, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3801, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0388, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0331, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0314, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0014, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0140, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1266, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0468, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0223, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0261, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0760, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0360, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0139, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0295, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0558, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1075, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0814, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0378, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0736, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0756, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0142, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0064, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0910, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0455, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0748, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0099, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1328, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1671, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2121, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0656, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0427, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0463, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0299, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0491, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0454, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0527, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0380, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1119, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0074, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0488, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0967, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0151, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0635, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0353, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1067, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0220, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1152, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0114, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0222, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0607, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0290, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0044, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0479, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0147, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0371, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0063, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0024, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0455, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0647, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1797, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'when', 'did', 'abc', 'begin', 'airing', 'dick', 'clark', \"'\", 's', 'new', 'year', \"'\", 's', 'rock', 'in', \"'\", 'eve', '?', '[SEP]', '[SEP]', 'since', '1974', ',', 'abc', 'has', 'generally', 'aired', 'dick', 'clark', \"'\", 's', 'new', 'year', \"'\", 's', 'rock', 'in', \"'\", 'eve', 'on', 'new', 'year', \"'\", 's', 'eve', '(', 'hosted', 'first', 'by', 'its', 'creator', 'dick', 'clark', ',', 'and', 'later', 'by', 'his', 'successor', 'ryan', 'sea', 'crest', ')', ';', 'the', 'only', 'exception', 'was', 'in', '1999', ',', 'when', 'abc', 'put', 'it', 'on', 'a', 'one', '-', 'year', 'hiatus', 'to', 'provide', 'coverage', 'of', 'the', 'international', 'millennium', 'festivities', ',', 'though', 'clark', \"'\", 's', 'traditional', 'countdown', 'from', 'times', 'square', 'was', 'still', 'featured', 'within', 'the', 'coverage', '.', '[SEP]']\n",
      "len conti_raw 80\n",
      "conti_raw ['[CLS]', 'when', 'did', 'abc', 'begin', 'airing', 'dick', \"clark's\", 'new', \"year's\", \"rockin'\", 'eve?', '[SEP]', '[SEP]', 'since', '1974,', 'abc', 'has', 'generally', 'aired', 'dick', \"clark's\", 'new', \"year's\", \"rockin'\", 'eve', 'on', 'new', \"year's\", 'eve', '(hosted', 'first', 'by', 'its', 'creator', 'dick', 'clark,', 'and', 'later', 'by', 'his', 'successor', 'ryan', 'seacrest);', 'the', 'only', 'exception', 'was', 'in', '1999,', 'when', 'abc', 'put', 'it', 'on', 'a', 'one-year', 'hiatus', 'to', 'provide', 'coverage', 'of', 'the', 'international', 'millennium', 'festivities,', 'though', \"clark's\", 'traditional', 'countdown', 'from', 'times', 'square', 'was', 'still', 'featured', 'within', 'the', 'coverage.', '[SEP]']\n",
      "pred_prob 0.7005867958068848\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.70)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.05</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> when                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> abc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> begin                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> airing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dick                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clark's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> new                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> year's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rockin'                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eve?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> since                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1974,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> abc                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> generally                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> aired                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dick                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clark's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> new                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> year's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rockin'                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eve                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> new                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> year's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eve                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (hosted                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> first                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> creator                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dick                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clark,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> later                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> successor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ryan                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seacrest);                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> only                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exception                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1999,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> when                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> abc                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> put                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one-year                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hiatus                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> provide                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> coverage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> international                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> millennium                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> festivities,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> though                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> clark's                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> traditional                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> countdown                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> square                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> still                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> featured                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> within                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> coverage.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['The Kronenberg Palace had been an exceptional example of what type of architecture?', 'Despite that the Warsaw University of Technology building (18991902) is the most interesting of the late 19th-century architecture.']\n",
      "GT target: 0\n",
      "word attr tensor([ 4.8881e-01,  6.5555e-03,  3.3494e-02,  7.0932e-02, -3.1988e-02,\n",
      "        -1.4285e-01, -4.3108e-03,  1.3585e-02, -3.9543e-02,  4.7000e-04,\n",
      "        -2.1330e-01, -2.1064e-02,  5.9600e-03,  7.6976e-02, -9.2489e-02,\n",
      "         1.0998e-01,  1.7255e-01, -1.1961e-02, -1.3823e-01,  2.9856e-01,\n",
      "         2.0708e-01, -5.0524e-02,  2.5208e-01, -1.2055e-01, -2.4335e-01,\n",
      "         1.1389e-03,  2.5783e-01,  6.7374e-02, -9.9247e-02, -1.2260e-01,\n",
      "        -1.5501e-01, -2.7177e-02, -1.0452e-01, -6.8306e-02, -1.4837e-01,\n",
      "         5.0748e-02, -1.2650e-01, -2.7506e-02, -7.9459e-02,  4.4636e-02,\n",
      "         4.1989e-02, -1.2164e-02,  1.0345e-01, -1.9123e-02,  3.7328e-01],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4888, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0066, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1428, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0043, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0136, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0395, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0211, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0060, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0770, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0925, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0120, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1382, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2986, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2071, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0505, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2521, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1206, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2433, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0011, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2578, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0697, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1045, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0683, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1484, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0507, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1265, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0275, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0795, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0156, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3733, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'the', 'k', 'rone', 'nberg', 'palace', 'had', 'been', 'an', 'exceptional', 'example', 'of', 'what', 'type', 'of', 'architecture', '?', '[SEP]', '[SEP]', 'despite', 'that', 'the', 'warsaw', 'university', 'of', 'technology', 'building', '(', '1899', '', '1902', ')', 'is', 'the', 'most', 'interesting', 'of', 'the', 'late', '19th', '-', 'century', 'architecture', '.', '[SEP]']\n",
      "len conti_raw 35\n",
      "conti_raw ['[CLS]', 'the', 'kronenberg', 'palace', 'had', 'been', 'an', 'exceptional', 'example', 'of', 'what', 'type', 'of', 'architecture?', '[SEP]', '[SEP]', 'despite', 'that', 'the', 'warsaw', 'university', 'of', 'technology', 'building', '(18991902)', 'is', 'the', 'most', 'interesting', 'of', 'the', 'late', '19th-century', 'architecture.', '[SEP]']\n",
      "pred_prob 0.5980805158615112\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.60)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.58</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kronenberg                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> palace                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> had                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exceptional                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> example                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> type                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> architecture?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> despite                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> warsaw                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> university                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> technology                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> building                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (18991902)                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> interesting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> late                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 19th-century                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> architecture.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What was the definition of professionals, for this study?', 'It is important to note, however, that the British study referenced above is the only one of its kind and consisted of \"a random ... probability sample of 2,869 young people between the ages of 18 and 24 in a computer-assisted study\" and that the questions referred to \"sexual abuse with a professional,\" not necessarily a teacher.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.4923,  0.0230, -0.0118,  0.0492, -0.2070, -0.0483, -0.2598, -0.0267,\n",
      "         0.0101, -0.0301,  0.0343,  0.0327,  0.3832,  0.3170,  0.0100, -0.0629,\n",
      "         0.0363, -0.0014, -0.0417, -0.0337,  0.0039, -0.0290, -0.0144, -0.0037,\n",
      "         0.0069, -0.0357,  0.0635, -0.0140, -0.0827, -0.0043, -0.0147,  0.0805,\n",
      "        -0.0264,  0.0705, -0.0547,  0.0183,  0.1147, -0.0093, -0.1043, -0.0143,\n",
      "         0.1244, -0.0522, -0.0551, -0.0921,  0.1915,  0.0762, -0.0103, -0.0051,\n",
      "        -0.0468, -0.0344,  0.0406, -0.0101,  0.0406, -0.1078, -0.0302,  0.0364,\n",
      "        -0.0894, -0.0854, -0.0213, -0.0879, -0.0335, -0.0090,  0.0751, -0.0022,\n",
      "        -0.0330,  0.0210,  0.0033, -0.0081,  0.0278,  0.0026, -0.1783, -0.1450,\n",
      "         0.0781,  0.1274, -0.0066, -0.1436,  0.0781,  0.0678, -0.2084, -0.0125,\n",
      "         0.0700,  0.0394, -0.0118,  0.1068, -0.1446, -0.0350,  0.1132],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.4923, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0230, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0118, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0492, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2070, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0483, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1433, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0101, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0301, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0335, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3832, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3170, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0100, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0629, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0363, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0014, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0377, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0144, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0037, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0069, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0357, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0635, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0140, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0827, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0043, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0147, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0805, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0264, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0705, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0547, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0183, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1147, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0093, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0593, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1244, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0729, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1915, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0762, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0103, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0052, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0101, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0406, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1078, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0302, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0364, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0894, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0854, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0213, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0879, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0335, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0090, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0017, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0122, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0081, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0278, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0026, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1783, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1450, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0781, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0604, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1436, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0781, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0678, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0202, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0394, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0118, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1068, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0898, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1132, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'was', 'the', 'definition', 'of', 'professionals', ',', 'for', 'this', 'study', '?', '[SEP]', '[SEP]', 'it', 'is', 'important', 'to', 'note', ',', 'however', ',', 'that', 'the', 'british', 'study', 'referenced', 'above', 'is', 'the', 'only', 'one', 'of', 'its', 'kind', 'and', 'consisted', 'of', '\"', 'a', 'random', '.', '.', '.', 'probability', 'sample', 'of', '2', ',', '86', '9', 'young', 'people', 'between', 'the', 'ages', 'of', '18', 'and', '24', 'in', 'a', 'computer', '-', 'assisted', 'study', '\"', 'and', 'that', 'the', 'questions', 'referred', 'to', '\"', 'sexual', 'abuse', 'with', 'a', 'professional', ',', '\"', 'not', 'necessarily', 'a', 'teacher', '.', '[SEP]']\n",
      "len conti_raw 70\n",
      "conti_raw ['[CLS]', 'what', 'was', 'the', 'definition', 'of', 'professionals,', 'for', 'this', 'study?', '[SEP]', '[SEP]', 'it', 'is', 'important', 'to', 'note,', 'however,', 'that', 'the', 'british', 'study', 'referenced', 'above', 'is', 'the', 'only', 'one', 'of', 'its', 'kind', 'and', 'consisted', 'of', '\"a', 'random', '...', 'probability', 'sample', 'of', '2,869', 'young', 'people', 'between', 'the', 'ages', 'of', '18', 'and', '24', 'in', 'a', 'computer-assisted', 'study\"', 'and', 'that', 'the', 'questions', 'referred', 'to', '\"sexual', 'abuse', 'with', 'a', 'professional,\"', 'not', 'necessarily', 'a', 'teacher.', '[SEP]']\n",
      "pred_prob 0.42063799500465393\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.42)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.24</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> definition                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> professionals,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> study?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> note,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> however,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> british                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> study                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> referenced                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> above                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> only                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kind                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> consisted                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> random                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ...                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> probability                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sample                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2,869                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> young                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> people                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ages                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 18                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 24                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> computer-assisted                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> study\"                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> questions                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> referred                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"sexual                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> abuse                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> professional,\"                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> necessarily                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> teacher.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['bassett focuses on what to illustrate his idea?', 'To better illustrate this idea, Bassett focuses his analysis of the role of nineteenth-century maps during the \"scramble for Africa\".']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.1967,  0.3358, -0.5492, -0.0484, -0.2672, -0.1420,  0.1136,  0.0380,\n",
      "        -0.0058, -0.0364,  0.2060,  0.1647,  0.1345,  0.0629, -0.0892, -0.0027,\n",
      "         0.0319, -0.1164,  0.2507, -0.2629,  0.0743,  0.0746,  0.0215,  0.0674,\n",
      "         0.0815,  0.0284, -0.3078,  0.0206,  0.0225,  0.1941,  0.0015,  0.0262,\n",
      "         0.0528, -0.0329,  0.0155,  0.0059,  0.0443, -0.0730,  0.1489],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1967, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3358, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.5492, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0484, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.2672, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1420, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1136, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0380, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0211, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2060, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1647, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1345, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0629, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0892, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0027, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2507, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2629, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0743, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0746, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0215, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0674, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0815, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0284, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0606, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1941, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0015, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0262, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0100, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0155, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0239, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1489, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'bassett', 'focuses', 'on', 'what', 'to', 'illustrate', 'his', 'idea', '?', '[SEP]', '[SEP]', 'to', 'better', 'illustrate', 'this', 'idea', ',', 'bassett', 'focuses', 'his', 'analysis', 'of', 'the', 'role', 'of', 'nineteenth', '-', 'century', 'maps', 'during', 'the', '\"', 'scramble', 'for', 'africa', '\"', '.', '[SEP]']\n",
      "len conti_raw 32\n",
      "conti_raw ['[CLS]', 'bassett', 'focuses', 'on', 'what', 'to', 'illustrate', 'his', 'idea?', '[SEP]', '[SEP]', 'to', 'better', 'illustrate', 'this', 'idea,', 'bassett', 'focuses', 'his', 'analysis', 'of', 'the', 'role', 'of', 'nineteenth-century', 'maps', 'during', 'the', '\"scramble', 'for', 'africa\".', '[SEP]']\n",
      "pred_prob 0.5277463793754578\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.53)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.48</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bassett                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> focuses                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> illustrate                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> idea?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> better                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> illustrate                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> idea,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bassett                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> focuses                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> analysis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> role                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nineteenth-century                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> maps                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> during                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"scramble                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> africa\".                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Where did Tesla believe his talents came from?', \"Tesla's mother, uka Tesla (ne Mandi), whose father was also an Orthodox priest,:10 had a talent for making home craft tools, mechanical appliances, and the ability to memorize Serbian epic poems.\"]\n",
      "GT target: 0\n",
      "word attr tensor([ 0.0054, -0.0900,  0.0652,  0.2587, -0.3318,  0.2267, -0.0023,  0.0495,\n",
      "         0.0431,  0.0965,  0.2338,  0.1292,  0.2165,  0.0324,  0.0538,  0.0333,\n",
      "        -0.0503, -0.2292, -0.1199,  0.0228,  0.0513, -0.1727,  0.0667,  0.0946,\n",
      "         0.0681,  0.0138,  0.0333,  0.0240,  0.2147,  0.0328, -0.0584,  0.0387,\n",
      "        -0.0069, -0.1074, -0.0496, -0.0335,  0.2132,  0.1062,  0.0733,  0.1322,\n",
      "        -0.0325,  0.0126,  0.0986,  0.1391,  0.0221, -0.0032,  0.0833,  0.0310,\n",
      "         0.1417,  0.0742,  0.1575,  0.1093,  0.0981,  0.1998, -0.1938,  0.0079,\n",
      "        -0.2707,  0.0559,  0.2976], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0054, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0900, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0652, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2587, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.3318, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2267, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0023, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0495, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0698, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2338, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1292, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0891, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0085, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1745, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0228, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0607, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0441, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0333, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0240, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2147, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0328, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0584, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0387, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2132, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1062, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0733, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1322, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0325, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0126, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0986, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0806, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0032, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0572, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1417, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0742, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1575, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1093, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1490, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1938, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0079, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2976, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'where', 'did', 'tesla', 'believe', 'his', 'talents', 'came', 'from', '?', '[SEP]', '[SEP]', 'tesla', \"'\", 's', 'mother', ',', '', 'uka', 'tesla', '(', 'nee', 'man', 'dic', ')', ',', 'whose', 'father', 'was', 'also', 'an', 'orthodox', 'priest', ',', ':', '10', 'had', 'a', 'talent', 'for', 'making', 'home', 'craft', 'tools', ',', 'mechanical', 'appliances', ',', 'and', 'the', 'ability', 'to', 'memo', 'rize', 'serbian', 'epic', 'poems', '.', '[SEP]']\n",
      "len conti_raw 43\n",
      "conti_raw ['[CLS]', 'where', 'did', 'tesla', 'believe', 'his', 'talents', 'came', 'from?', '[SEP]', '[SEP]', \"tesla's\", 'mother,', 'uka', 'tesla', '(nee', 'mandic),', 'whose', 'father', 'was', 'also', 'an', 'orthodox', 'priest,:10', 'had', 'a', 'talent', 'for', 'making', 'home', 'craft', 'tools,', 'mechanical', 'appliances,', 'and', 'the', 'ability', 'to', 'memorize', 'serbian', 'epic', 'poems.', '[SEP]']\n",
      "pred_prob 0.7914941310882568\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.79)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>2.41</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> where                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tesla                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> believe                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> talents                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> came                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tesla's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mother,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> uka                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tesla                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (nee                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mandic),                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> whose                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> father                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> orthodox                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> priest,:10                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> had                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> talent                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> making                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> home                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> craft                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tools,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mechanical                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> appliances,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ability                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> memorize                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> serbian                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> epic                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> poems.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Who was given the esteemed status of MVP for Super Bowl 50?', 'Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2 sacks, and two forced fumbles.']\n",
      "GT target: 1\n",
      "word attr tensor([ 8.1456e-02,  5.1201e-01,  1.5136e-01,  1.1146e-01,  1.3353e-01,\n",
      "         5.4454e-02, -1.2434e-02,  6.8137e-02,  6.8507e-02,  1.5223e-01,\n",
      "         1.3332e-01, -6.2265e-02,  4.2109e-04,  5.2136e-02,  2.9188e-01,\n",
      "         2.1402e-01,  4.6111e-02,  1.0796e-02,  4.0870e-01, -5.5928e-02,\n",
      "        -4.3655e-02,  1.7187e-01,  2.3270e-01, -8.5719e-02, -4.5537e-02,\n",
      "         9.3391e-02,  1.6510e-02, -3.2621e-02, -3.2378e-02, -5.7554e-02,\n",
      "         6.7031e-02, -5.4672e-03, -4.6625e-02,  4.7458e-02, -3.1618e-02,\n",
      "         1.6563e-02,  1.3519e-01,  8.4665e-03,  5.4917e-02,  2.1767e-01,\n",
      "        -2.2818e-02,  4.8959e-02,  3.5118e-01], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0815, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.5120, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1514, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1115, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1335, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0210, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0681, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0685, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1522, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1333, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0623, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1720, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.2140, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0461, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0108, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4087, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0559, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0437, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1719, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2327, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0857, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0455, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0550, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0326, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0324, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0576, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0308, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0075, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1352, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0085, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0549, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0732, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3512, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'was', 'given', 'the', 'esteem', 'ed', 'status', 'of', 'mvp', 'for', 'super', 'bowl', '50', '?', '[SEP]', '[SEP]', 'denver', 'linebacker', 'von', 'miller', 'was', 'named', 'super', 'bowl', 'mvp', ',', 'recording', 'five', 'solo', 'tackles', ',', '2', '', 'sacks', ',', 'and', 'two', 'forced', 'fumble', 's', '.', '[SEP]']\n",
      "len conti_raw 35\n",
      "conti_raw ['[CLS]', 'who', 'was', 'given', 'the', 'esteemed', 'status', 'of', 'mvp', 'for', 'super', 'bowl', '50?', '[SEP]', '[SEP]', 'denver', 'linebacker', 'von', 'miller', 'was', 'named', 'super', 'bowl', 'mvp,', 'recording', 'five', 'solo', 'tackles,', '2', 'sacks,', 'and', 'two', 'forced', 'fumbles.', '[SEP]']\n",
      "pred_prob 0.9407644867897034\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.94)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>3.42</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> given                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> esteemed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> status                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mvp                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> super                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bowl                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 50?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> denver                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> linebacker                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> von                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> miller                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> named                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> super                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bowl                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mvp,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recording                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> five                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> solo                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tackles,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sacks,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> forced                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fumbles.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What was the percentage of Black or African-Americans living in the city?', 'Hispanic or Latino of any race were 39.9% of the population.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.0898,  0.2295,  0.1167,  0.0989,  0.4904, -0.0731,  0.0265, -0.0266,\n",
      "         0.0250,  0.0332,  0.1232,  0.0084, -0.0075,  0.1304,  0.1215,  0.2569,\n",
      "         0.3549,  0.1655,  0.2247, -0.0667,  0.2172,  0.0124, -0.0520,  0.1992,\n",
      "         0.1489,  0.1542, -0.1783,  0.0866,  0.2848, -0.0122,  0.0727,  0.1768,\n",
      "        -0.1839,  0.1375], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.0898, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2295, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1167, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0989, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.4904, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0731, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0265, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0266, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0761, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0084, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0075, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1304, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1892, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3549, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1655, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2247, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0667, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2172, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0124, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0520, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1992, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1489, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1610, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0122, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0727, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0036, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1375, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'was', 'the', 'percentage', 'of', 'black', 'or', 'african', '-', 'americans', 'living', 'in', 'the', 'city', '?', '[SEP]', '[SEP]', 'hispanic', 'or', 'latino', 'of', 'any', 'race', 'were', '39', '.', '9', '%', 'of', 'the', 'population', '.', '[SEP]']\n",
      "len conti_raw 27\n",
      "conti_raw ['[CLS]', 'what', 'was', 'the', 'percentage', 'of', 'black', 'or', 'african-americans', 'living', 'in', 'the', 'city?', '[SEP]', '[SEP]', 'hispanic', 'or', 'latino', 'of', 'any', 'race', 'were', '39.9%', 'of', 'the', 'population.', '[SEP]']\n",
      "pred_prob 0.9827529191970825\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.98)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>3.39</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> percentage                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> black                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> african-americans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> living                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> city?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hispanic                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> latino                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> any                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> race                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 39.9%                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> population.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What was the result of the 2007 election?', 'With International Criminal Court trial dates in 2013 for both President Kenyatta and Deputy President William Ruto related to the 2007 election aftermath, US President Barack Obama chose not to visit the country during his mid-2013 African trip.']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.3290,  0.2152, -0.0040,  0.0138, -0.2197, -0.0885,  0.0642, -0.1862,\n",
      "        -0.0086,  0.1044, -0.0161,  0.3176, -0.0682, -0.0319,  0.0443, -0.1102,\n",
      "        -0.1969, -0.0563, -0.0458, -0.1757, -0.0595, -0.0581, -0.0889,  0.1329,\n",
      "         0.0418, -0.0571, -0.1488, -0.0804, -0.0731,  0.0295, -0.0977,  0.0085,\n",
      "         0.0179,  0.0293, -0.3137, -0.0289, -0.2204, -0.1219,  0.0282, -0.0403,\n",
      "        -0.1907, -0.1281, -0.1492,  0.0339, -0.1257, -0.0937, -0.0114,  0.0190,\n",
      "         0.0096,  0.1352,  0.0630, -0.0200, -0.0727, -0.0704,  0.0055,  0.0150,\n",
      "         0.4012], device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.3290, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2152, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0040, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0138, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2197, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0885, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0642, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1862, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0479, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0161, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.3176, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0682, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0319, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0443, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1102, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1969, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0563, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0458, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1757, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0595, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0581, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0889, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0873, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0571, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1488, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0804, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0731, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0341, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0085, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0179, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0293, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.3137, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0289, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1711, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0282, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0403, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1907, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1281, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1492, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0339, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1257, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0937, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0114, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0190, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0096, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1352, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0256, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0704, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4012, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'was', 'the', 'result', 'of', 'the', '2007', 'election', '?', '[SEP]', '[SEP]', 'with', 'international', 'criminal', 'court', 'trial', 'dates', 'in', '2013', 'for', 'both', 'president', 'kenya', 'tta', 'and', 'deputy', 'president', 'william', 'ru', 'to', 'related', 'to', 'the', '2007', 'election', 'aftermath', ',', 'us', 'president', 'barack', 'obama', 'chose', 'not', 'to', 'visit', 'the', 'country', 'during', 'his', 'mid', '-', '2013', 'african', 'trip', '.', '[SEP]']\n",
      "len conti_raw 50\n",
      "conti_raw ['[CLS]', 'what', 'was', 'the', 'result', 'of', 'the', '2007', 'election?', '[SEP]', '[SEP]', 'with', 'international', 'criminal', 'court', 'trial', 'dates', 'in', '2013', 'for', 'both', 'president', 'kenyatta', 'and', 'deputy', 'president', 'william', 'ruto', 'related', 'to', 'the', '2007', 'election', 'aftermath,', 'us', 'president', 'barack', 'obama', 'chose', 'not', 'to', 'visit', 'the', 'country', 'during', 'his', 'mid-2013', 'african', 'trip.', '[SEP]']\n",
      "pred_prob 0.3425130546092987\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.34)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>-1.40</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> result                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2007                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> election?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> international                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> criminal                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> court                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> trial                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dates                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2013                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> both                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> president                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kenyatta                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> deputy                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> president                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> william                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ruto                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> related                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2007                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> election                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> aftermath,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> president                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> barack                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> obama                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chose                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> visit                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> country                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> during                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mid-2013                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> african                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> trip.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What equation currently decribes the physics of force.', 'The notion \"force\" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrdinger equation instead of Newtonian equations.']\n",
      "GT target: 1\n",
      "word attr tensor([ 0.1274, -0.0826, -0.5173, -0.1344,  0.1198,  0.0183,  0.0997,  0.0661,\n",
      "        -0.1805, -0.0465,  0.0325, -0.1272,  0.1863,  0.1671, -0.0088, -0.1029,\n",
      "         0.0181,  0.0069,  0.0091,  0.0575,  0.0169, -0.0293, -0.0452,  0.0302,\n",
      "        -0.0380, -0.0588,  0.0117, -0.0131, -0.0753, -0.0431, -0.0148, -0.0492,\n",
      "        -0.0272, -0.1707, -0.0386, -0.0505, -0.0531,  0.0349,  0.0302,  0.0512,\n",
      "        -0.2311,  0.0555,  0.3374,  0.0652,  0.0161,  0.3290,  0.0873, -0.0821,\n",
      "        -0.0294, -0.1359, -0.2691,  0.1074,  0.0370, -0.0981, -0.0766, -0.1393,\n",
      "        -0.0787, -0.1702], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1274, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0826, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.5173, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1344, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0844, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0661, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1805, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0465, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0473, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1863, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1671, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0088, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1029, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0575, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0169, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0293, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0452, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0302, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0484, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0117, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0131, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0753, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0431, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0148, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0492, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0272, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1707, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0386, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0505, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0531, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0349, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0302, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0512, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2311, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0555, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3374, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0652, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0161, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3290, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0747, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.2691, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1074, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0370, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0873, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1090, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1702, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'equation', 'currently', 'dec', 'ri', 'bes', 'the', 'physics', 'of', 'force', '.', '[SEP]', '[SEP]', 'the', 'notion', '\"', 'force', '\"', 'keeps', 'its', 'meaning', 'in', 'quantum', 'mechanics', ',', 'though', 'one', 'is', 'now', 'dealing', 'with', 'operators', 'instead', 'of', 'classical', 'variables', 'and', 'though', 'the', 'physics', 'is', 'now', 'described', 'by', 'the', 'sc', 'hr', 'od', 'inger', 'equation', 'instead', 'of', 'newton', 'ian', 'equations', '.', '[SEP]']\n",
      "len conti_raw 47\n",
      "conti_raw ['[CLS]', 'what', 'equation', 'currently', 'decribes', 'the', 'physics', 'of', 'force.', '[SEP]', '[SEP]', 'the', 'notion', '\"force\"', 'keeps', 'its', 'meaning', 'in', 'quantum', 'mechanics,', 'though', 'one', 'is', 'now', 'dealing', 'with', 'operators', 'instead', 'of', 'classical', 'variables', 'and', 'though', 'the', 'physics', 'is', 'now', 'described', 'by', 'the', 'schrodinger', 'equation', 'instead', 'of', 'newtonian', 'equations.', '[SEP]']\n",
      "pred_prob 0.23880738019943237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.24)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>-1.10</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> equation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> currently                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decribes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> physics                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> force.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> notion                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"force\"                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> keeps                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> meaning                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> quantum                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mechanics,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> though                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> now                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dealing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> operators                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> instead                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> classical                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> variables                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> though                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> physics                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> now                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> described                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> schrodinger                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> equation                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> instead                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> newtonian                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> equations.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What did Iqbal fear would weaken the spiritual foundations of Islam and Muslim society?', 'In his travels to Egypt, Afghanistan, Palestine and Syria, he promoted ideas of greater Islamic political co-operation and unity, calling for the shedding of nationalist differences.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.6096, -0.1007, -0.1174, -0.0233,  0.1616, -0.1619, -0.1633,  0.0837,\n",
      "         0.0534,  0.1987,  0.0250,  0.2174,  0.0187,  0.0185, -0.0029,  0.1249,\n",
      "        -0.0091,  0.3213, -0.0345, -0.0062, -0.0572, -0.1171, -0.0391, -0.1974,\n",
      "        -0.1126, -0.1782,  0.0224, -0.0896, -0.0086, -0.1390,  0.0603, -0.1612,\n",
      "         0.1205, -0.0163, -0.1360,  0.1350,  0.0425, -0.1186,  0.0138, -0.1217,\n",
      "         0.0009, -0.1302, -0.0719,  0.1304, -0.0587,  0.0521,  0.0454, -0.0767,\n",
      "         0.0176,  0.0358,  0.0464,  0.0245,  0.0865], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.6096, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1007, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1174, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0233, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1616, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1619, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1633, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0837, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0534, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1987, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0250, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2174, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0187, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0185, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0610, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0091, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.3213, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0345, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0062, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0572, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1171, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1182, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1454, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0224, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0896, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0738, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0603, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1612, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1205, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0163, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1360, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1350, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0425, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0871, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1011, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1304, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0587, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0521, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0157, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0176, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0358, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0354, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0865, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'did', 'iqbal', 'fear', 'would', 'weaken', 'the', 'spiritual', 'foundations', 'of', 'islam', 'and', 'muslim', 'society', '?', '[SEP]', '[SEP]', 'in', 'his', 'travels', 'to', 'egypt', ',', 'afghanistan', ',', 'palestine', 'and', 'syria', ',', 'he', 'promoted', 'ideas', 'of', 'greater', 'islamic', 'political', 'co', '-', 'operation', 'and', 'unity', ',', 'calling', 'for', 'the', 'shed', 'ding', 'of', 'nationalist', 'differences', '.', '[SEP]']\n",
      "len conti_raw 44\n",
      "conti_raw ['[CLS]', 'what', 'did', 'iqbal', 'fear', 'would', 'weaken', 'the', 'spiritual', 'foundations', 'of', 'islam', 'and', 'muslim', 'society?', '[SEP]', '[SEP]', 'in', 'his', 'travels', 'to', 'egypt,', 'afghanistan,', 'palestine', 'and', 'syria,', 'he', 'promoted', 'ideas', 'of', 'greater', 'islamic', 'political', 'co-operation', 'and', 'unity,', 'calling', 'for', 'the', 'shedding', 'of', 'nationalist', 'differences.', '[SEP]']\n",
      "pred_prob 0.38441193103790283\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.38)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.22</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> iqbal                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fear                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> weaken                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spiritual                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> foundations                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> islam                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> muslim                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> society?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> travels                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> egypt,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> afghanistan,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> palestine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> syria,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> he                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> promoted                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ideas                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> greater                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> islamic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> political                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> co-operation                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> unity,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> calling                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> shedding                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nationalist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> differences.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['In what meeting did Shirley lay out plans for 1756?', 'At a meeting in Albany in December 1755, he laid out his plans for 1756.']\n",
      "GT target: 1\n",
      "word attr tensor([-0.4969,  0.1074, -0.2108,  0.2020, -0.0956, -0.1721,  0.2258,  0.0146,\n",
      "        -0.0609,  0.1606,  0.1534, -0.0391, -0.0105,  0.0815,  0.0562, -0.1531,\n",
      "         0.4672,  0.0685,  0.0832,  0.0798,  0.0068, -0.3049, -0.0057,  0.0716,\n",
      "         0.0079,  0.0721, -0.0556,  0.1454,  0.0024, -0.2096, -0.2552, -0.0740],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.4969, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1074, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2108, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2020, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0956, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1721, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.2258, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0146, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0609, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1606, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0571, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0105, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0815, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0562, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1531, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.4672, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0685, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0832, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0798, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0068, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1553, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0716, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0079, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0721, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0556, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1454, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0024, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2324, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0740, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'in', 'what', 'meeting', 'did', 'shirley', 'lay', 'out', 'plans', 'for', '1756', '?', '[SEP]', '[SEP]', 'at', 'a', 'meeting', 'in', 'albany', 'in', 'december', '1755', ',', 'he', 'laid', 'out', 'his', 'plans', 'for', '1756', '.', '[SEP]']\n",
      "len conti_raw 29\n",
      "conti_raw ['[CLS]', 'in', 'what', 'meeting', 'did', 'shirley', 'lay', 'out', 'plans', 'for', '1756?', '[SEP]', '[SEP]', 'at', 'a', 'meeting', 'in', 'albany', 'in', 'december', '1755,', 'he', 'laid', 'out', 'his', 'plans', 'for', '1756.', '[SEP]']\n",
      "pred_prob 0.6875017881393433\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.69)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>-0.14</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> meeting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> shirley                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lay                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> out                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1756?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> at                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> meeting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> albany                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> december                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1755,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> he                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> laid                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> out                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> his                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1756.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: [\"In autoimmune disorders, the immune system doesn't distinguish between what types of cells?\", 'One of the functions of specialized cells (located in the thymus and bone marrow) is to present young lymphocytes with self antigens produced throughout the body and to eliminate those cells that recognize self-antigens, preventing autoimmunity.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.5808,  0.0383,  0.0430, -0.0292, -0.0386, -0.0412,  0.0336,  0.0080,\n",
      "         0.0700,  0.0325,  0.0662,  0.0674, -0.0276,  0.0166,  0.0013,  0.0434,\n",
      "         0.1035,  0.0658, -0.0124,  0.0869,  0.0974,  0.3903,  0.3483,  0.1945,\n",
      "         0.0080,  0.0783, -0.0881, -0.0765, -0.0644, -0.0329, -0.0095, -0.0486,\n",
      "        -0.0417,  0.0263, -0.0330,  0.0394,  0.0036,  0.0014, -0.0895, -0.0332,\n",
      "         0.0141,  0.0682,  0.0714, -0.0074, -0.0012, -0.0110,  0.0029,  0.0187,\n",
      "        -0.0753, -0.0271,  0.0065,  0.0076,  0.0275, -0.0686, -0.0059,  0.0460,\n",
      "        -0.0101, -0.0107, -0.0394,  0.0534, -0.0700, -0.0290,  0.0762, -0.0728,\n",
      "        -0.0261, -0.0065, -0.0234, -0.0511,  0.0021, -0.0101, -0.0426, -0.0403,\n",
      "        -0.0385, -0.0680,  0.4374], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.5808, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0383, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0285, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0208, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0700, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0325, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0662, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0183, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0013, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0434, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1035, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0658, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0124, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0921, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3903, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3483, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1945, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0080, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0783, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0881, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0765, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0644, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0329, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0291, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0417, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0263, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0032, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0036, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0014, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0613, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0141, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0682, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0714, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0074, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0085, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0753, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0271, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0070, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0275, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0686, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0059, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0460, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0101, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0107, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0394, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0534, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0700, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0290, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0762, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0021, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0520, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4374, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'in', 'auto', 'im', 'mun', 'e', 'disorders', ',', 'the', 'immune', 'system', 'doesn', \"'\", 't', 'distinguish', 'between', 'what', 'types', 'of', 'cells', '?', '[SEP]', '[SEP]', 'one', 'of', 'the', 'functions', 'of', 'specialized', 'cells', '(', 'located', 'in', 'the', 'thy', 'mus', 'and', 'bone', 'marrow', ')', 'is', 'to', 'present', 'young', 'l', 'ym', 'ph', 'ocytes', 'with', 'self', 'antigen', 's', 'produced', 'throughout', 'the', 'body', 'and', 'to', 'eliminate', 'those', 'cells', 'that', 'recognize', 'self', '-', 'antigen', 's', ',', 'preventing', 'auto', 'im', 'mun', 'ity', '.', '[SEP]']\n",
      "len conti_raw 53\n",
      "conti_raw ['[CLS]', 'in', 'autoimmune', 'disorders,', 'the', 'immune', 'system', \"doesn't\", 'distinguish', 'between', 'what', 'types', 'of', 'cells?', '[SEP]', '[SEP]', 'one', 'of', 'the', 'functions', 'of', 'specialized', 'cells', '(located', 'in', 'the', 'thymus', 'and', 'bone', 'marrow)', 'is', 'to', 'present', 'young', 'lymphocytes', 'with', 'self', 'antigens', 'produced', 'throughout', 'the', 'body', 'and', 'to', 'eliminate', 'those', 'cells', 'that', 'recognize', 'self-antigens,', 'preventing', 'autoimmunity.', '[SEP]']\n",
      "pred_prob 0.8891812562942505\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.89)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.88</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> autoimmune                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disorders,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immune                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doesn't                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> distinguish                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> between                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> types                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cells?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> functions                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> specialized                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cells                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (located                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> thymus                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bone                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> marrow)                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> present                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> young                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lymphocytes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> self                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> antigens                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> produced                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> throughout                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> body                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eliminate                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> those                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cells                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recognize                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> self-antigens,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> preventing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> autoimmunity.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Which husband and wife modern furniture design team are represented in the V&A furniture collection?', 'One of the finest pieces of continental furniture in the collection is the Rococo Augustus Rex Bureau Cabinet dated c1750 from Germany, with especially fine marquetry and ormolu mounts.']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.1311,  0.9117, -0.1416,  0.0170, -0.0611, -0.0367,  0.0091,  0.0231,\n",
      "         0.0189, -0.0191,  0.0139,  0.0125,  0.0534,  0.0431,  0.0340,  0.0177,\n",
      "        -0.0183,  0.0143,  0.2096,  0.1659,  0.0831,  0.0371,  0.0059,  0.0419,\n",
      "         0.0065, -0.0370, -0.0196,  0.0747, -0.0152, -0.0103,  0.0211, -0.0387,\n",
      "        -0.0282,  0.0149,  0.0025,  0.0544,  0.0100,  0.0095,  0.0157, -0.0196,\n",
      "         0.0078,  0.0079, -0.0066, -0.0039, -0.0041,  0.0016, -0.0331, -0.0027,\n",
      "        -0.0760, -0.0107,  0.0026, -0.0228, -0.0581, -0.0185,  0.0436, -0.0524,\n",
      "         0.0217, -0.0230, -0.0107,  0.0673], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1311, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.9117, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1416, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0170, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0611, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0367, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0091, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0231, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0189, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0191, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0139, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0125, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0534, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0281, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0183, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1119, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1659, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0831, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0371, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0059, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0419, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0065, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0370, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0196, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0747, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0152, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0103, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0211, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0387, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0282, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0284, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0100, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0095, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0157, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0196, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0078, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0016, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0041, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0158, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0027, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0760, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0107, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0341, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0185, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0087, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0168, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0673, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'which', 'husband', 'and', 'wife', 'modern', 'furniture', 'design', 'team', 'are', 'represented', 'in', 'the', 'v', '&', 'a', 'furniture', 'collection', '?', '[SEP]', '[SEP]', 'one', 'of', 'the', 'finest', 'pieces', 'of', 'continental', 'furniture', 'in', 'the', 'collection', 'is', 'the', 'roc', 'oco', 'augustus', 'rex', 'bureau', 'cabinet', 'dated', 'c1', '75', '0', 'from', 'germany', ',', 'with', 'especially', 'fine', 'mar', 'quet', 'ry', 'and', 'or', 'mo', 'lu', 'mounts', '.', '[SEP]']\n",
      "len conti_raw 48\n",
      "conti_raw ['[CLS]', 'which', 'husband', 'and', 'wife', 'modern', 'furniture', 'design', 'team', 'are', 'represented', 'in', 'the', 'v&a', 'furniture', 'collection?', '[SEP]', '[SEP]', 'one', 'of', 'the', 'finest', 'pieces', 'of', 'continental', 'furniture', 'in', 'the', 'collection', 'is', 'the', 'rococo', 'augustus', 'rex', 'bureau', 'cabinet', 'dated', 'c1750', 'from', 'germany,', 'with', 'especially', 'fine', 'marquetry', 'and', 'ormolu', 'mounts.', '[SEP]']\n",
      "pred_prob 0.918213963508606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.92)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>1.44</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 55%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> which                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> husband                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> wife                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> modern                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> furniture                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> design                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> team                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> represented                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> v&a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> furniture                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> collection?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> finest                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pieces                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> continental                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> furniture                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> collection                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rococo                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> augustus                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rex                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bureau                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cabinet                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dated                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> c1750                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> germany,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> especially                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> marquetry                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ormolu                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mounts.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Why did oil start getting priced in terms of gold?', \"Because oil was priced in dollars, oil producers' real income decreased.\"]\n",
      "GT target: 1\n",
      "word attr tensor([ 0.2406, -0.4784,  0.1780, -0.1842, -0.0702,  0.0872, -0.2531,  0.0178,\n",
      "        -0.1301, -0.0940, -0.0563,  0.1649, -0.1392, -0.1274, -0.3146, -0.0064,\n",
      "         0.1265,  0.0112,  0.0517,  0.0961, -0.1986, -0.1502, -0.1866,  0.0160,\n",
      "         0.1382,  0.0907, -0.4646, -0.0255,  0.1050], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2406, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.4784, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1780, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1842, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0702, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0872, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2531, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0178, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1301, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0940, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0543, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1392, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1274, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.3146, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0064, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1265, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0112, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0517, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0512, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1502, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0853, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1382, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0907, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2451, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1050, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'why', 'did', 'oil', 'start', 'getting', 'priced', 'in', 'terms', 'of', 'gold', '?', '[SEP]', '[SEP]', 'because', 'oil', 'was', 'priced', 'in', 'dollars', ',', 'oil', 'producers', \"'\", 'real', 'income', 'decreased', '.', '[SEP]']\n",
      "len conti_raw 25\n",
      "conti_raw ['[CLS]', 'why', 'did', 'oil', 'start', 'getting', 'priced', 'in', 'terms', 'of', 'gold?', '[SEP]', '[SEP]', 'because', 'oil', 'was', 'priced', 'in', 'dollars,', 'oil', \"producers'\", 'real', 'income', 'decreased.', '[SEP]']\n",
      "pred_prob 0.3373945653438568\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.34)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>-1.56</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> why                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> oil                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> start                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> getting                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> priced                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> terms                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> gold?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> because                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> oil                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> priced                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dollars,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> oil                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> producers'                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> real                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> income                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decreased.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['Who did Britain exploit in India?', 'Although a substantial number of colonies had been designed to provide economic profit and to ship resources to home ports in the seventeenth and eighteenth centuries, Fieldhouse suggests that in the nineteenth and twentieth centuries in places such as Africa and Asia, this idea is not necessarily valid:']\n",
      "GT target: 0\n",
      "word attr tensor([ 0.1960,  0.1059, -0.0494, -0.0909, -0.1049,  0.0178, -0.1496,  0.0892,\n",
      "         0.5796,  0.2992,  0.2050, -0.0379,  0.0086, -0.0779, -0.0386, -0.0127,\n",
      "        -0.0864, -0.0838, -0.1851,  0.0106, -0.0522, -0.0109, -0.0284, -0.0719,\n",
      "        -0.0655,  0.0411, -0.0056, -0.0240, -0.0439,  0.0098, -0.0382,  0.0169,\n",
      "        -0.0207,  0.0190,  0.2372,  0.0972, -0.0367, -0.0753,  0.0107,  0.0552,\n",
      "         0.0435, -0.0379,  0.0124,  0.1060, -0.0008,  0.0501,  0.1192,  0.0808,\n",
      "         0.0580, -0.0148,  0.0125,  0.0950,  0.0816,  0.1229,  0.0584,  0.0768,\n",
      "        -0.0453, -0.0107,  0.1680, -0.1486, -0.2495,  0.0502, -0.2783],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1960, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1059, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0494, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0909, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1049, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0178, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0302, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.5796, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2992, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2050, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0379, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0086, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0779, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0386, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0127, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0864, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0838, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1851, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0106, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0522, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0109, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0284, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0719, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0655, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0411, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0056, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0240, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0439, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0098, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0382, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0169, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0207, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0190, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2372, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0303, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0323, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0552, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0435, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0379, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0124, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1060, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0008, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0501, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1192, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0808, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0580, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0148, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0125, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0950, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0816, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0907, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0768, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0453, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0107, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1680, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1486, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0997, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.2783, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'who', 'did', 'britain', 'exploit', 'in', 'india', '?', '[SEP]', '[SEP]', 'although', 'a', 'substantial', 'number', 'of', 'colonies', 'had', 'been', 'designed', 'to', 'provide', 'economic', 'profit', 'and', 'to', 'ship', 'resources', 'to', 'home', 'ports', 'in', 'the', 'seventeenth', 'and', 'eighteenth', 'centuries', ',', 'field', 'house', 'suggests', 'that', 'in', 'the', 'nineteenth', 'and', 'twentieth', 'centuries', 'in', 'places', 'such', 'as', 'africa', 'and', 'asia', ',', 'this', 'idea', 'is', 'not', 'necessarily', 'valid', ':', '[SEP]']\n",
      "len conti_raw 58\n",
      "conti_raw ['[CLS]', 'who', 'did', 'britain', 'exploit', 'in', 'india?', '[SEP]', '[SEP]', 'although', 'a', 'substantial', 'number', 'of', 'colonies', 'had', 'been', 'designed', 'to', 'provide', 'economic', 'profit', 'and', 'to', 'ship', 'resources', 'to', 'home', 'ports', 'in', 'the', 'seventeenth', 'and', 'eighteenth', 'centuries,', 'fieldhouse', 'suggests', 'that', 'in', 'the', 'nineteenth', 'and', 'twentieth', 'centuries', 'in', 'places', 'such', 'as', 'africa', 'and', 'asia,', 'this', 'idea', 'is', 'not', 'necessarily', 'valid:', '[SEP]']\n",
      "pred_prob 0.6999862194061279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.70)</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.96</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> britain                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exploit                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> india?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> although                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> substantial                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> number                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> colonies                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> had                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> designed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> provide                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> economic                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> profit                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ship                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> resources                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> home                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ports                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seventeenth                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> eighteenth                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> centuries,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fieldhouse                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> suggests                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nineteenth                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> twentieth                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> centuries                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> places                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> such                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> africa                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> asia,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> idea                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> necessarily                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> valid:                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What popular environmentalist is also a university alumni member?', 'In science, alumni include astronomers Carl Sagan, a prominent contributor to the scientific research of extraterrestrial life, and Edwin Hubble, known for \"Hubble\\'s Law\", NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA, experimental physicist Luis Alvarez, popular environmentalist David Suzuki, balloonist Jeannette Piccard, biologists Ernest Everett']\n",
      "GT target: 1\n",
      "word attr tensor([ 2.3877e-01, -1.6756e-02,  8.9161e-02,  4.5562e-02, -2.1262e-02,\n",
      "        -9.6301e-03,  1.1707e-03,  1.5272e-02,  1.0234e-03, -2.5577e-02,\n",
      "        -1.2245e-02, -8.2443e-02,  3.9939e-01,  3.4574e-01, -3.1161e-02,\n",
      "        -1.5933e-02, -8.1549e-02,  2.3925e-02, -2.1663e-03, -1.7001e-01,\n",
      "        -7.2143e-02, -2.6815e-02, -3.3958e-02, -6.4889e-02, -8.7949e-03,\n",
      "         2.4875e-02, -1.4876e-02, -2.8729e-02,  3.7285e-03,  3.5986e-03,\n",
      "         1.3701e-03, -3.2384e-02,  1.1905e-02, -2.0039e-02, -1.0863e-02,\n",
      "        -2.4874e-02,  1.2113e-02, -4.1088e-02, -4.5342e-02,  3.7765e-03,\n",
      "         4.1262e-02, -1.9404e-02, -1.8642e-02, -9.6630e-03, -1.3798e-02,\n",
      "         1.0392e-02,  4.6996e-02,  1.0295e-02, -1.3114e-02,  8.3502e-03,\n",
      "         7.6426e-03,  9.7152e-03, -1.3163e-02,  2.5301e-02,  2.1474e-02,\n",
      "        -2.0896e-02, -3.5003e-02, -5.8041e-02, -2.4324e-02, -1.3657e-02,\n",
      "        -1.9189e-02, -1.6177e-02, -1.3810e-02,  1.6352e-02, -1.4544e-02,\n",
      "         2.1257e-02, -6.3010e-05,  2.2900e-02,  4.6365e-02,  2.8735e-03,\n",
      "         1.2045e-03,  1.0970e-02, -9.3871e-03,  1.6107e-02,  2.1268e-03,\n",
      "        -2.7783e-02, -2.9819e-02, -1.2627e-02, -3.1508e-02,  7.5007e-03,\n",
      "        -5.7976e-02, -1.2750e-02, -9.3339e-03,  4.9175e-03, -4.2886e-02,\n",
      "        -1.6093e-02,  6.7530e-03,  1.1192e-02,  5.2788e-02,  1.0718e-01,\n",
      "         1.2437e-01,  1.0650e-01,  7.7185e-02,  1.2237e-01,  3.9253e-02,\n",
      "         8.7102e-02, -8.1044e-02, -2.8266e-02, -2.9765e-02,  8.6326e-03,\n",
      "        -2.3713e-02, -9.6083e-02, -1.3243e-01, -3.2386e-02, -5.9382e-02,\n",
      "        -5.8631e-01,  3.2737e-01], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.2388, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0168, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0892, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0121, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0096, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0012, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0153, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0256, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0473, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3994, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.3457, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0312, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0487, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0239, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0022, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1700, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0721, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0476, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0088, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0249, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0149, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0287, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0037, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0036, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0014, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0324, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0162, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0145, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0453, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0038, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0039, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0097, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0138, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0022, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0253, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0215, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0209, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0157, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0213, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0114, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0464, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0029, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0012, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0110, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0094, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0161, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0170, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0315, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0075, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0580, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0127, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0022, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0429, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0161, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0068, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0320, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1072, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1154, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0772, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0808, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0030, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0290, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0518, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0824, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0594, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.5863, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.3274, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'popular', 'environmental', 'ist', 'is', 'also', 'a', 'university', 'alumni', 'member', '?', '[SEP]', '[SEP]', 'in', 'science', ',', 'alumni', 'include', 'astronomers', 'carl', 'saga', 'n', ',', 'a', 'prominent', 'contributor', 'to', 'the', 'scientific', 'research', 'of', 'extra', 'ter', 'rest', 'rial', 'life', ',', 'and', 'edwin', 'hub', 'ble', ',', 'known', 'for', '\"', 'hub', 'ble', \"'\", 's', 'law', '\"', ',', 'nasa', 'astronaut', 'john', 'm', '.', 'gr', 'un', 'sf', 'eld', ',', 'genetic', 'ist', 'james', 'watson', ',', 'best', 'known', 'as', 'one', 'of', 'the', 'co', '-', 'discover', 'ers', 'of', 'the', 'structure', 'of', 'dna', ',', 'experimental', 'physicist', 'luis', 'alvarez', ',', 'popular', 'environmental', 'ist', 'david', 'suzuki', ',', 'balloon', 'ist', 'jeanne', 'tte', 'pic', 'card', ',', 'biologist', 's', 'ernest', 'everett', '[SEP]']\n",
      "len conti_raw 71\n",
      "conti_raw ['[CLS]', 'what', 'popular', 'environmentalist', 'is', 'also', 'a', 'university', 'alumni', 'member?', '[SEP]', '[SEP]', 'in', 'science,', 'alumni', 'include', 'astronomers', 'carl', 'sagan,', 'a', 'prominent', 'contributor', 'to', 'the', 'scientific', 'research', 'of', 'extraterrestrial', 'life,', 'and', 'edwin', 'hubble,', 'known', 'for', '\"hubble\\'s', 'law\",', 'nasa', 'astronaut', 'john', 'm.', 'grunsfeld,', 'geneticist', 'james', 'watson,', 'best', 'known', 'as', 'one', 'of', 'the', 'co-discoverers', 'of', 'the', 'structure', 'of', 'dna,', 'experimental', 'physicist', 'luis', 'alvarez,', 'popular', 'environmentalist', 'david', 'suzuki,', 'balloonist', 'jeannette', 'piccard,', 'biologists', 'ernest', 'everett', '[SEP]']\n",
      "pred_prob 0.31498053669929504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.31)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.11</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> popular                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> environmentalist                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> university                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alumni                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> member?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> science,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alumni                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> astronomers                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> carl                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sagan,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> prominent                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> contributor                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scientific                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> research                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> extraterrestrial                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> life,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> edwin                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hubble,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> known                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"hubble's                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> law\",                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nasa                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> astronaut                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> john                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> m.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> grunsfeld,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> geneticist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> james                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watson,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> known                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> co-discoverers                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> structure                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dna,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> experimental                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> physicist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> luis                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alvarez,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> popular                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> environmentalist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> david                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> suzuki,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> balloonist                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> jeannette                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> piccard,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> biologists                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ernest                    </font></mark><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> everett                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['What publication did Philip Howard work for?', 'Responding to the findings of the survey in The Times newspaper, journalist Philip Howard maintained that, \"to compare the violence of Dr Who, sired by a horse-laugh out of a nightmare, with the more realistic violence of other television series, where actors who look like human beings bleed paint that looks like blood, is like comparing Monopoly with the property market in London: both are fantasies, but one is meant to be taken seriously.\"']\n",
      "GT target: 1\n",
      "word attr tensor([-1.9666e-01,  5.4284e-02, -7.3028e-02,  3.8330e-02, -2.3856e-01,\n",
      "        -4.5005e-01, -2.5867e-02,  7.4719e-03,  1.8391e-02,  3.0038e-01,\n",
      "         1.8569e-01, -1.0384e-01,  3.2095e-02,  8.0662e-03, -9.1042e-03,\n",
      "        -2.5309e-02, -4.2670e-03, -5.1384e-02,  1.5334e-01, -2.5176e-02,\n",
      "         5.8094e-03, -5.4397e-02, -3.2025e-03,  3.6207e-03, -2.6859e-01,\n",
      "        -5.5939e-01, -2.3831e-02,  6.9080e-03,  2.4054e-03,  1.0105e-01,\n",
      "        -2.1856e-02, -1.0057e-01,  4.3463e-02,  8.7109e-02,  8.9489e-03,\n",
      "         2.0144e-02, -1.4594e-02, -7.5326e-04, -3.2716e-03,  5.6003e-03,\n",
      "         6.1915e-03,  1.6579e-02,  2.3669e-02,  1.9252e-02,  3.3464e-02,\n",
      "        -1.4878e-03, -4.8736e-03,  9.0818e-02, -1.6583e-05, -5.4579e-03,\n",
      "         2.6805e-02,  4.4317e-02,  1.7308e-02,  3.1914e-02,  2.8417e-03,\n",
      "         6.7044e-02,  9.2866e-02, -6.2171e-03,  2.6844e-02,  9.5227e-03,\n",
      "         1.1631e-01,  3.2150e-02,  4.3423e-03, -4.8414e-03,  6.2234e-03,\n",
      "         2.5264e-02,  1.0370e-02, -4.1331e-02, -1.2642e-03, -1.1641e-02,\n",
      "        -7.6675e-04, -1.1978e-02, -1.7751e-02, -5.4153e-02, -1.4707e-02,\n",
      "        -2.8597e-02, -1.5473e-02, -1.0251e-03, -1.6544e-02,  7.0087e-02,\n",
      "         2.5232e-02,  6.3341e-03,  1.4782e-02, -7.5041e-03,  5.3946e-02,\n",
      "         3.5439e-02,  9.2828e-02,  1.7582e-02,  2.5468e-02,  6.0091e-02,\n",
      "        -8.1128e-03,  3.2909e-02,  4.4268e-02,  1.4788e-02,  3.7645e-04,\n",
      "        -1.0161e-02,  3.7659e-02,  3.5191e-02,  1.6463e-01], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(-0.1967, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0543, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0730, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0383, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2386, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.4501, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0259, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0129, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.3004, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1857, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1038, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0321, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0081, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0091, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0253, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0043, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0514, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.1533, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0252, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0058, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0288, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0036, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2686, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.5594, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0238, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0047, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0396, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1006, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0435, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0871, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0089, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0201, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0033, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0056, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0062, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0197, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0335, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0015, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0049, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0454, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0055, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0268, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0443, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0173, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0319, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0028, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0670, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0929, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0103, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0095, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.1163, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0322, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0043, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0048, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0062, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0253, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0104, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0413, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0013, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0116, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0008, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0542, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0147, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0286, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0155, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0010, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0165, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0701, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0252, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0063, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0036, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0539, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0354, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0552, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0255, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0601, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0081, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0329, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0443, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0148, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0245, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1646, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'what', 'publication', 'did', 'philip', 'howard', 'work', 'for', '?', '[SEP]', '[SEP]', 'responding', 'to', 'the', 'findings', 'of', 'the', 'survey', 'in', 'the', 'times', 'newspaper', ',', 'journalist', 'philip', 'howard', 'maintained', 'that', ',', '\"', 'to', 'compare', 'the', 'violence', 'of', 'dr', 'who', ',', 'sired', 'by', 'a', 'horse', '-', 'laugh', 'out', 'of', 'a', 'nightmare', ',', 'with', 'the', 'more', 'realistic', 'violence', 'of', 'other', 'television', 'series', ',', 'where', 'actors', 'who', 'look', 'like', 'human', 'beings', 'bleed', 'paint', 'that', 'looks', 'like', 'blood', ',', 'is', 'like', 'comparing', 'monopoly', 'with', 'the', 'property', 'market', 'in', 'london', ':', 'both', 'are', 'fantasies', ',', 'but', 'one', 'is', 'meant', 'to', 'be', 'taken', 'seriously', '.', '\"', '[SEP]']\n",
      "len conti_raw 85\n",
      "conti_raw ['[CLS]', 'what', 'publication', 'did', 'philip', 'howard', 'work', 'for?', '[SEP]', '[SEP]', 'responding', 'to', 'the', 'findings', 'of', 'the', 'survey', 'in', 'the', 'times', 'newspaper,', 'journalist', 'philip', 'howard', 'maintained', 'that,', '\"to', 'compare', 'the', 'violence', 'of', 'dr', 'who,', 'sired', 'by', 'a', 'horse-laugh', 'out', 'of', 'a', 'nightmare,', 'with', 'the', 'more', 'realistic', 'violence', 'of', 'other', 'television', 'series,', 'where', 'actors', 'who', 'look', 'like', 'human', 'beings', 'bleed', 'paint', 'that', 'looks', 'like', 'blood,', 'is', 'like', 'comparing', 'monopoly', 'with', 'the', 'property', 'market', 'in', 'london:', 'both', 'are', 'fantasies,', 'but', 'one', 'is', 'meant', 'to', 'be', 'taken', 'seriously.\"', '[SEP]']\n",
      "pred_prob 0.7388923764228821\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>Entailment (0.74)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>0.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> publication                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> did                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> philip                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> howard                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> work                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> responding                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> findings                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> survey                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> newspaper,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> journalist                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> philip                    </font></mark><mark style=\"background-color: hsl(0, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> howard                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> maintained                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \"to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> compare                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> violence                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dr                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sired                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> horse-laugh                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> out                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nightmare,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> more                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> realistic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> violence                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> television                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> series,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> where                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> actors                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> who                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> look                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> like                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> human                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> beings                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bleed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> paint                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> looks                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> like                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> blood,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> like                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> comparing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> monopoly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> property                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> market                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> london:                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> both                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fantasies,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> meant                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> taken                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seriously.\"                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "Raw datum: ['If the apparant force of two fermions is attractive, what is the spin function?', 'If two identical fermions (e.g. electrons) have a symmetric spin function (e.g. parallel spins) the spatial variables must be antisymmetric (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel spins the position variables must be symmetric (i.e. the apparent force must be attractive).']\n",
      "GT target: 1\n",
      "word attr tensor([ 1.7350e-01, -4.0283e-02,  1.5873e-02, -2.6940e-02, -3.5854e-02,\n",
      "         5.2562e-03, -1.6931e-03, -2.7646e-02,  1.8703e-02, -9.2734e-04,\n",
      "         4.3806e-02,  1.8653e-02,  7.1348e-03, -3.0114e-01, -5.7346e-02,\n",
      "         3.4843e-02, -1.8558e-02,  3.0540e-02,  7.3845e-02, -2.9883e-01,\n",
      "         2.0921e-02,  4.1419e-01,  2.4256e-01, -1.6625e-02, -7.9738e-03,\n",
      "        -1.7185e-02, -6.7436e-03,  1.4287e-02,  9.6808e-03,  2.9435e-02,\n",
      "        -2.9806e-02, -6.1813e-03,  1.8068e-02, -2.5829e-02, -4.1190e-02,\n",
      "        -1.0389e-02, -1.3112e-02,  5.2038e-02, -4.1392e-01,  1.0434e-02,\n",
      "        -2.6314e-01,  3.1500e-02,  1.4851e-02,  2.0234e-02,  1.2950e-02,\n",
      "         1.2447e-02, -2.4840e-02, -5.1034e-02, -1.3630e-02,  1.7180e-02,\n",
      "        -7.2408e-02, -1.9981e-02, -1.3476e-01, -1.5211e-02,  1.6580e-02,\n",
      "        -1.3038e-02,  2.5052e-02,  2.4784e-02, -3.0064e-02, -2.2992e-02,\n",
      "        -7.9471e-03, -3.0686e-02, -2.1093e-02, -1.0570e-02,  6.6367e-02,\n",
      "         1.7130e-04, -8.7008e-03, -1.8345e-02, -4.7829e-03, -9.9327e-03,\n",
      "         7.3042e-02,  1.5057e-02, -3.0578e-02, -9.6778e-03, -1.6275e-03,\n",
      "        -1.3784e-02,  4.9533e-03,  2.1605e-02,  1.9065e-03, -2.4967e-02,\n",
      "        -1.6471e-02, -7.8544e-03,  4.4184e-04, -7.4955e-02, -1.6029e-02,\n",
      "        -4.1453e-03, -1.1746e-03, -5.2254e-02, -2.2685e-02, -6.5384e-03,\n",
      "        -9.2627e-03, -1.6040e-02, -2.0187e-02, -1.4864e-02, -1.8110e-01,\n",
      "         3.3448e-02, -1.9177e-02, -1.3403e-01, -1.2968e-01, -2.0604e-02,\n",
      "        -1.5956e-01,  2.5490e-02, -1.2106e-02,  6.9467e-03,  1.6425e-02,\n",
      "        -3.6731e-02,  5.8807e-02,  2.3921e-01,  9.7573e-02,  6.7479e-02,\n",
      "         2.1172e-02, -1.9899e-01,  9.6878e-02,  5.1668e-02,  9.7021e-02],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "conti attr [tensor(0.1735, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0403, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0159, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0131, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0017, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0276, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0187, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0200, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0071, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1792, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0348, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0186, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0305, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0738, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.1390, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.4142, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2426, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0166, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0080, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0172, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0067, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0092, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0258, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0131, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0520, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.4139, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0104, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.2631, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0149, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0248, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0323, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0172, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0724, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0200, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1348, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0152, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0191, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0225, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0106, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0664, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0087, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0183, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0048, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0099, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0730, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0151, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0306, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0097, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0016, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0138, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0133, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0140, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0079, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0455, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0251, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.0065, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0156, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(-0.1811, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(0.0334, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(-0.0192, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1340, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1297, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0206, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.1596, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<UnbindBackward0>), tensor(-0.0126, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0588, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.2392, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0976, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0675, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0212, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>), tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.0970, device='cuda:0', dtype=torch.float64, grad_fn=<UnbindBackward0>)]\n",
      "detokenized ['[CLS]', 'if', 'the', 'app', 'aran', 't', 'force', 'of', 'two', 'fe', 'rmi', 'ons', 'is', 'attractive', ',', 'what', 'is', 'the', 'spin', 'function', '?', '[SEP]', '[SEP]', 'if', 'two', 'identical', 'fe', 'rmi', 'ons', '(', 'e', '.', 'g', '.', 'electrons', ')', 'have', 'a', 'symmetric', 'spin', 'function', '(', 'e', '.', 'g', '.', 'parallel', 'spins', ')', 'the', 'spatial', 'variables', 'must', 'be', 'anti', 'sy', 'mme', 'tric', '(', 'i', '.', 'e', '.', 'they', 'exclude', 'each', 'other', 'from', 'their', 'places', 'much', 'as', 'if', 'there', 'was', 'a', 'rep', 'ulsive', 'force', ')', ',', 'and', 'vice', 'versa', ',', 'i', '.', 'e', '.', 'for', 'anti', 'para', 'lle', 'l', 'spins', 'the', 'position', 'variables', 'must', 'be', 'symmetric', '(', 'i', '.', 'e', '.', 'the', 'apparent', 'force', 'must', 'be', 'attractive', ')', '.', '[SEP]']\n",
      "len conti_raw 74\n",
      "conti_raw ['[CLS]', 'if', 'the', 'apparant', 'force', 'of', 'two', 'fermions', 'is', 'attractive,', 'what', 'is', 'the', 'spin', 'function?', '[SEP]', '[SEP]', 'if', 'two', 'identical', 'fermions', '(e.g.', 'electrons)', 'have', 'a', 'symmetric', 'spin', 'function', '(e.g.', 'parallel', 'spins)', 'the', 'spatial', 'variables', 'must', 'be', 'antisymmetric', '(i.e.', 'they', 'exclude', 'each', 'other', 'from', 'their', 'places', 'much', 'as', 'if', 'there', 'was', 'a', 'repulsive', 'force),', 'and', 'vice', 'versa,', 'i.e.', 'for', 'antiparallel', 'spins', 'the', 'position', 'variables', 'must', 'be', 'symmetric', '(i.e.', 'the', 'apparent', 'force', 'must', 'be', 'attractive).', '[SEP]']\n",
      "pred_prob 0.3203403353691101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment (0.32)</b></text></td><td><text style=\"padding-right:2em\"><b>No Entailment</b></text></td><td><text style=\"padding-right:2em\"><b>-1.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> if                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apparant                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> force                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fermions                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> attractive,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spin                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> function?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> if                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> identical                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fermions                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (e.g.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> electrons)                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> symmetric                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spin                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> function                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (e.g.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> parallel                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spins)                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spatial                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> variables                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> must                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> antisymmetric                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (i.e.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exclude                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> each                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> other                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> their                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> places                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> much                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> if                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> there                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> repulsive                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> force),                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> vice                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> versa,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i.e.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> antiparallel                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spins                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> position                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> variables                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> must                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> symmetric                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> (i.e.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> apparent                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> force                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> must                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> attractive).                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n"
     ]
    }
   ],
   "source": [
    "for i, (datum_raw, target) in enumerate(zip(qnli_data_raw, targets), start=1):\n",
    "#     example_1 = \"How many times has the South Florida/Miami area hosted the Super Bowl?\"\n",
    "#     example_2 = \"The South Florida/Miami area has previously hosted the event 10 times (tied for most with New Orleans), with the most recent one being Super Bowl XLIV in 2010.\"\n",
    "    \n",
    "#     example_1 = \"When did the third Digimon series begin?\"\n",
    "#     example_2 = \"Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.\"\n",
    "#     datum_raw, target = [example_1, example_2], 1\n",
    "    print(f'Raw datum: {datum_raw}') #datum expected to be a list of 2 sentences\n",
    "    print(f'GT target: {target}')\n",
    "    visual_record=generate_record(datum_raw, target)\n",
    "    print(visualization.visualize_text([visual_record])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ca5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_base = 'Integrated_Gradients'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce01966e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices': [4013,\n",
       "  2281,\n",
       "  2771,\n",
       "  5109,\n",
       "  1852,\n",
       "  2016,\n",
       "  642,\n",
       "  3912,\n",
       "  4427,\n",
       "  2330,\n",
       "  4581,\n",
       "  2611,\n",
       "  5186,\n",
       "  2700,\n",
       "  4920,\n",
       "  4335,\n",
       "  1246,\n",
       "  2811,\n",
       "  5290,\n",
       "  5385,\n",
       "  4774,\n",
       "  653,\n",
       "  2816,\n",
       "  916,\n",
       "  3297,\n",
       "  3459,\n",
       "  1475,\n",
       "  1287,\n",
       "  4548,\n",
       "  1476,\n",
       "  2328,\n",
       "  2208,\n",
       "  2772,\n",
       "  5301,\n",
       "  869,\n",
       "  4968,\n",
       "  4456,\n",
       "  4711,\n",
       "  1929,\n",
       "  634,\n",
       "  3774,\n",
       "  5389,\n",
       "  497,\n",
       "  4901,\n",
       "  4602,\n",
       "  831,\n",
       "  1292,\n",
       "  1087,\n",
       "  3159,\n",
       "  3492],\n",
       " 'raw_data': [['What would a teacher assess the levels of a student on?',\n",
       "   'For example, an experienced teacher and parent described the place of a teacher in learning as follows: \"The real bulk of learning takes place in self-study and problem solving with a lot of feedback around that loop.'],\n",
       "  ['What company created Doctor Who?',\n",
       "   'Who character by BBC Television in the early 1960s, a myriad of stories have been published about Doctor Who, in different media: apart from the actual television episodes that continue to be produced by the BBC, there have also been novels, comics, short stories, audio books, radio plays, interactive video games, game books, webcasts, DVD extras, and even stage performances.'],\n",
       "  ['What was the name of the Media Day event for Super Bowl 50?',\n",
       "   \"The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night.\"],\n",
       "  ['How many Doctor Who soundtracks have been released since 2005?',\n",
       "   'The fourth was released on 4 October 2010 as a two disc special edition and contained music from the 20082010 specials (The Next Doctor to End of Time Part 2).'],\n",
       "  [\"What is the name of the country's longest continuously running student film society?\",\n",
       "   'Students at the University of Chicago run over 400 clubs and organizations known as Recognized Student Organizations (RSOs).'],\n",
       "  ['How many times has the South Florida/Miami area hosted the Super Bowl?',\n",
       "   'The South Florida/Miami area has previously hosted the event 10 times (tied for most with New Orleans), with the most recent one being Super Bowl XLIV in 2010.'],\n",
       "  ['What is different about Paulinella chromatophora?',\n",
       "   'It is not clear whether that symbiont is closely related to the ancestral chloroplast of other eukaryotes.'],\n",
       "  [\"Who played Doctor Who on stage in the 70's?\",\n",
       "   'Doctor Who has appeared on stage numerous times.'],\n",
       "  ['Who do clinical pharmacists work with much of the time?',\n",
       "   'Clinical pharmacists often collaborate with physicians and other healthcare professionals to improve pharmaceutical care.'],\n",
       "  ['In which county does Jacksonville reside?',\n",
       "   'It is the county seat of Duval County, with which the city government consolidated in 1968.'],\n",
       "  ['Who did Genghis Khan charge with finding and punishing the Shah?',\n",
       "   'Genghis Khan ordered the wholesale massacre of many of the civilians, enslaved the rest of the population and executed Inalchuq by pouring molten silver into his ears and eyes, as retribution for his actions.'],\n",
       "  ['What entity enforces the Charter of Fundamental Rights of the European Union?',\n",
       "   'In effect, after the Lisbon Treaty, the Charter and the Convention now co-exist under European Union law, though the former is enforced by the European Court of Justice in relation to European Union measures, and the latter by the European Court of Human Rights in relation to measures by member states.'],\n",
       "  [\"Most of the museum's collection had been returned by which year?\",\n",
       "   'Before the return of the collections after the war, the Britain Can Make It exhibition was held between September and November 1946, attracting nearly a million and a half visitors.'],\n",
       "  ['Within the 30 days how many digiboxes had been sold?',\n",
       "   \"Within 30 days, over 100,000 digiboxes had been sold, which help bolstered BSkyB's decision to give away free digiboxes and minidishes from May 1999.\"],\n",
       "  ['The receptors on a killer T cell must bind to how many MHC: antigen complexes in order to activate the cell?',\n",
       "   \"The MHC:antigen complex is also recognized by the helper cell's CD4 co-receptor, which recruits molecules inside the T cell (e.g., Lck) that are responsible for the T cell's activation.\"],\n",
       "  [\"How much did Westinghouse pay for Tesla's designs?\",\n",
       "   \"In July 1888, Brown and Peck negotiated a licensing deal with George Westinghouse for Tesla's polyphase induction motor and transformer designs for $60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor.\"],\n",
       "  [\"What type of movies were produced in Jacksonville's 30 studios?\",\n",
       "   'One converted movie studio site, Norman Studios, remains in Arlington; It has been converted to the Jacksonville Silent Film Museum at Norman Studios.'],\n",
       "  ['What voyager said that Mombasa was a great harbour and moored small crafts and great ships?',\n",
       "   'The Swahili built Mombasa into a major port city and established trade links with other nearby city-states, as well as commercial centres in Persia, Arabia, and even India.'],\n",
       "  ['Where is Energiprojekt AB based?',\n",
       "   'Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines.'],\n",
       "  ['Who other than Tesla did Westinghouse consider for the patents?',\n",
       "   \"Westinghouse looked into getting a patent on a similar commutator-less, rotating magnetic field-based induction motor presented in a paper in March 1888 by the Italian physicist Galileo Ferraris, but decided Tesla's patent would probably control the market.\"],\n",
       "  ['What separates the neuroimmune system and peripheral immune system in humans?',\n",
       "   'In humans, the bloodbrain barrier, bloodcerebrospinal fluid barrier, and similar fluidbrain barriers separate the peripheral immune system from the neuroimmune system which protects the brain.'],\n",
       "  [\"What did Kublai's government have to balance between?\",\n",
       "   \"Kublai's government after 1262 was a compromise between preserving Mongol interests in China and satisfying the demands of his Chinese subjects.\"],\n",
       "  [\"What did Gasquet's book blame the plague on?\",\n",
       "   'The historian Francis Aidan Gasquet wrote about the \\'Great Pestilence\\' in 1893 and suggested that \"it would appear to be some form of the ordinary Eastern or bubonic plague\".'],\n",
       "  ['Who shared sideline duties with Evan Washburn?',\n",
       "   'In the United States, the game was televised by CBS, as part of a cycle between the three main broadcast television partners of the NFL.'],\n",
       "  ['Who was added to party as Washington went on the way?',\n",
       "   'Washington left with a small party, picking up along the way Jacob Van Braam as an interpreter; Christopher Gist, a company surveyor working in the area; and a few Mingo led by Tanaghrisson.'],\n",
       "  ['What did Queen Elizabeth II open in Newcastle in 1981?',\n",
       "   \"It was opened in five phases between 1980 and 1984, and was Britain's first urban light rail transit system; two extensions were opened in 1991 and 2002.\"],\n",
       "  ['What writing inspired the name Great Yuan?',\n",
       "   'Furthermore, the Yuan is sometimes known as the \"Empire of the Great Khan\" or \"Khanate of the Great Khan\", which particularly appeared on some Yuan maps, since Yuan emperors held the nominal title of Great Khan.'],\n",
       "  ['What happened to the East India Trading Company in 1767?',\n",
       "   'In 1599 the British East India Company was established and was chartered by Queen Elizabeth in the following year.'],\n",
       "  ['The principle of faunal succession was developed 100 years before whose theory of evolution?',\n",
       "   \"Based on principles laid out by William Smith almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession were developed independently of evolutionary thought.\"],\n",
       "  ['How many times did Luther preach in Halle in 1545 and 1546?',\n",
       "   'In 1545 and 1546 Luther preached three times in the Market Church in Halle, staying with his friend Justus Jonas during Christmas.'],\n",
       "  ['What part of the Rhine flows through North Rhine-Westphalia?',\n",
       "   'Here the Rhine flows through the largest conurbation in Germany, the Rhine-Ruhr region.'],\n",
       "  ['What is the most important thing apicoplasts do?',\n",
       "   'The most important apicoplast function is isopentenyl pyrophosphate synthesisin fact, apicomplexans die when something interferes with this apicoplast function, and when apicomplexans are grown in an isopentenyl pyrophosphate-rich medium, they dump the organelle.'],\n",
       "  [\"When did ABC begin airing Dick Clark's New Year's Rockin' Eve?\",\n",
       "   \"Since 1974, ABC has generally aired Dick Clark's New Year's Rockin' Eve on New Year's Eve (hosted first by its creator Dick Clark, and later by his successor Ryan Seacrest); the only exception was in 1999, when ABC put it on a one-year hiatus to provide coverage of the international millennium festivities, though Clark's traditional countdown from Times Square was still featured within the coverage.\"],\n",
       "  ['The Kronenberg Palace had been an exceptional example of what type of architecture?',\n",
       "   'Despite that the Warsaw University of Technology building (18991902) is the most interesting of the late 19th-century architecture.'],\n",
       "  ['What was the definition of professionals, for this study?',\n",
       "   'It is important to note, however, that the British study referenced above is the only one of its kind and consisted of \"a random ... probability sample of 2,869 young people between the ages of 18 and 24 in a computer-assisted study\" and that the questions referred to \"sexual abuse with a professional,\" not necessarily a teacher.'],\n",
       "  ['bassett focuses on what to illustrate his idea?',\n",
       "   'To better illustrate this idea, Bassett focuses his analysis of the role of nineteenth-century maps during the \"scramble for Africa\".'],\n",
       "  ['Where did Tesla believe his talents came from?',\n",
       "   \"Tesla's mother, uka Tesla (ne Mandi), whose father was also an Orthodox priest,:10 had a talent for making home craft tools, mechanical appliances, and the ability to memorize Serbian epic poems.\"],\n",
       "  ['Who was given the esteemed status of MVP for Super Bowl 50?',\n",
       "   'Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2 sacks, and two forced fumbles.'],\n",
       "  ['What was the percentage of Black or African-Americans living in the city?',\n",
       "   'Hispanic or Latino of any race were 39.9% of the population.'],\n",
       "  ['What was the result of the 2007 election?',\n",
       "   'With International Criminal Court trial dates in 2013 for both President Kenyatta and Deputy President William Ruto related to the 2007 election aftermath, US President Barack Obama chose not to visit the country during his mid-2013 African trip.'],\n",
       "  ['What equation currently decribes the physics of force.',\n",
       "   'The notion \"force\" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrdinger equation instead of Newtonian equations.'],\n",
       "  ['What did Iqbal fear would weaken the spiritual foundations of Islam and Muslim society?',\n",
       "   'In his travels to Egypt, Afghanistan, Palestine and Syria, he promoted ideas of greater Islamic political co-operation and unity, calling for the shedding of nationalist differences.'],\n",
       "  ['In what meeting did Shirley lay out plans for 1756?',\n",
       "   'At a meeting in Albany in December 1755, he laid out his plans for 1756.'],\n",
       "  [\"In autoimmune disorders, the immune system doesn't distinguish between what types of cells?\",\n",
       "   'One of the functions of specialized cells (located in the thymus and bone marrow) is to present young lymphocytes with self antigens produced throughout the body and to eliminate those cells that recognize self-antigens, preventing autoimmunity.'],\n",
       "  ['Which husband and wife modern furniture design team are represented in the V&A furniture collection?',\n",
       "   'One of the finest pieces of continental furniture in the collection is the Rococo Augustus Rex Bureau Cabinet dated c1750 from Germany, with especially fine marquetry and ormolu mounts.'],\n",
       "  ['Why did oil start getting priced in terms of gold?',\n",
       "   \"Because oil was priced in dollars, oil producers' real income decreased.\"],\n",
       "  ['Who did Britain exploit in India?',\n",
       "   'Although a substantial number of colonies had been designed to provide economic profit and to ship resources to home ports in the seventeenth and eighteenth centuries, Fieldhouse suggests that in the nineteenth and twentieth centuries in places such as Africa and Asia, this idea is not necessarily valid:'],\n",
       "  ['What popular environmentalist is also a university alumni member?',\n",
       "   'In science, alumni include astronomers Carl Sagan, a prominent contributor to the scientific research of extraterrestrial life, and Edwin Hubble, known for \"Hubble\\'s Law\", NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA, experimental physicist Luis Alvarez, popular environmentalist David Suzuki, balloonist Jeannette Piccard, biologists Ernest Everett'],\n",
       "  ['What publication did Philip Howard work for?',\n",
       "   'Responding to the findings of the survey in The Times newspaper, journalist Philip Howard maintained that, \"to compare the violence of Dr Who, sired by a horse-laugh out of a nightmare, with the more realistic violence of other television series, where actors who look like human beings bleed paint that looks like blood, is like comparing Monopoly with the property market in London: both are fantasies, but one is meant to be taken seriously.\"'],\n",
       "  ['If the apparant force of two fermions is attractive, what is the spin function?',\n",
       "   'If two identical fermions (e.g. electrons) have a symmetric spin function (e.g. parallel spins) the spatial variables must be antisymmetric (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel spins the position variables must be symmetric (i.e. the apparent force must be attractive).']],\n",
       " 'targets': [0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'model_out_list': [0.6311678290367126,\n",
       "  0.7619203925132751,\n",
       "  0.7368832230567932,\n",
       "  0.8182130455970764,\n",
       "  0.9198501110076904,\n",
       "  0.8666946887969971,\n",
       "  0.5214384198188782,\n",
       "  0.8973642587661743,\n",
       "  0.8218711018562317,\n",
       "  0.8931249976158142,\n",
       "  0.7466775178909302,\n",
       "  0.3837520182132721,\n",
       "  0.8580110669136047,\n",
       "  0.7189930081367493,\n",
       "  0.9368581175804138,\n",
       "  0.7738507390022278,\n",
       "  0.575401782989502,\n",
       "  0.9446513652801514,\n",
       "  0.8502684831619263,\n",
       "  0.7733870148658752,\n",
       "  0.8900112509727478,\n",
       "  0.13032764196395874,\n",
       "  0.7593749761581421,\n",
       "  0.9370878338813782,\n",
       "  0.8867630362510681,\n",
       "  0.8243213295936584,\n",
       "  0.9457733035087585,\n",
       "  0.7392330169677734,\n",
       "  0.3316514492034912,\n",
       "  0.909346342086792,\n",
       "  0.7328494191169739,\n",
       "  0.7642964720726013,\n",
       "  0.7005867958068848,\n",
       "  0.5980805158615112,\n",
       "  0.42063799500465393,\n",
       "  0.5277463793754578,\n",
       "  0.7914941310882568,\n",
       "  0.9407644867897034,\n",
       "  0.9827529191970825,\n",
       "  0.3425130546092987,\n",
       "  0.23880738019943237,\n",
       "  0.38441193103790283,\n",
       "  0.6875017881393433,\n",
       "  0.8891812562942505,\n",
       "  0.918213963508606,\n",
       "  0.3373945653438568,\n",
       "  0.6999862194061279,\n",
       "  0.31498053669929504,\n",
       "  0.7388923764228821,\n",
       "  0.3203403353691101],\n",
       " 'raw_attr_list': [tensor([[[ 4.7879e-04,  7.9672e-04, -9.9270e-05,  ..., -2.1832e-06,\n",
       "            -2.2771e-04, -8.4849e-04],\n",
       "           [-2.1242e-04, -1.7947e-04,  1.2881e-03,  ...,  7.3987e-04,\n",
       "             2.7331e-05, -2.6095e-03],\n",
       "           [ 5.2622e-05, -1.9860e-05, -1.7916e-04,  ..., -2.3207e-04,\n",
       "             1.2641e-03, -1.9704e-03],\n",
       "           ...,\n",
       "           [ 5.6413e-05, -1.7041e-03,  6.5780e-04,  ...,  7.6337e-04,\n",
       "             1.6854e-03, -3.9093e-03],\n",
       "           [ 1.1326e-03, -2.5135e-05, -4.7421e-05,  ...,  1.4768e-04,\n",
       "             9.4941e-04,  1.3124e-03],\n",
       "           [ 2.5846e-04,  1.3389e-04,  5.0420e-04,  ..., -1.7369e-03,\n",
       "            -2.9865e-04,  5.7382e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 3.5981e-05, -7.8835e-04,  4.1830e-04,  ...,  6.4190e-05,\n",
       "            -7.4260e-05, -1.8357e-04],\n",
       "           [-3.4185e-03, -1.3237e-03, -1.0151e-03,  ..., -1.5960e-03,\n",
       "            -4.6136e-04, -3.8397e-04],\n",
       "           [-1.5100e-03,  1.1540e-04,  1.5178e-04,  ...,  7.2103e-04,\n",
       "             1.4008e-03,  4.0968e-04],\n",
       "           ...,\n",
       "           [-7.8445e-05,  6.2807e-06, -3.9530e-05,  ...,  8.1327e-04,\n",
       "            -2.2655e-04, -8.0827e-05],\n",
       "           [-1.5150e-04, -1.2327e-04,  5.4304e-05,  ..., -8.0432e-05,\n",
       "             4.2381e-04,  2.8012e-04],\n",
       "           [ 2.1512e-05, -1.0044e-04,  1.4537e-04,  ...,  5.8458e-04,\n",
       "            -6.2566e-05,  2.8130e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.4184e-04, -1.9833e-03, -3.4513e-04,  ...,  1.5331e-04,\n",
       "             7.0372e-05,  1.2707e-03],\n",
       "           [-5.6355e-03, -5.1160e-03, -8.5810e-03,  ..., -4.3339e-03,\n",
       "            -1.2237e-05, -3.3327e-03],\n",
       "           [ 4.7066e-05,  6.1618e-03,  2.8638e-04,  ..., -8.0145e-05,\n",
       "            -7.7000e-05, -6.1641e-04],\n",
       "           ...,\n",
       "           [ 1.8885e-03,  3.5517e-03, -1.1005e-03,  ..., -1.3839e-04,\n",
       "            -9.4557e-04, -2.2659e-03],\n",
       "           [ 3.5290e-06,  5.6906e-04, -4.7299e-04,  ..., -8.5895e-04,\n",
       "             1.5920e-03,  1.7696e-03],\n",
       "           [ 1.0897e-04,  1.8013e-05,  1.1735e-04,  ..., -3.8802e-04,\n",
       "             3.3059e-04, -8.0884e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 5.5143e-04,  2.5245e-04,  1.0967e-03,  ...,  9.5103e-06,\n",
       "            -1.5215e-04, -7.6240e-04],\n",
       "           [-4.8851e-05, -5.6678e-03,  5.7770e-04,  ...,  1.1452e-03,\n",
       "            -4.2318e-03,  1.9899e-04],\n",
       "           [-6.1214e-04, -1.0746e-03,  7.6064e-04,  ...,  5.9540e-04,\n",
       "            -1.6179e-04,  8.9057e-04],\n",
       "           ...,\n",
       "           [ 3.8022e-04, -1.7827e-04,  1.2720e-04,  ...,  6.7019e-06,\n",
       "            -1.7282e-04,  5.0194e-05],\n",
       "           [-1.3067e-06, -1.3906e-04, -3.6953e-05,  ..., -2.6402e-05,\n",
       "             7.7493e-04, -3.1473e-04],\n",
       "           [-1.3580e-05, -3.8467e-05, -2.6465e-04,  ...,  1.4471e-05,\n",
       "             1.7248e-04,  1.5426e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-7.4738e-05,  8.8715e-05,  4.9428e-04,  ...,  3.2632e-06,\n",
       "            -1.1677e-05,  1.2117e-04],\n",
       "           [ 5.8126e-05, -6.5993e-04,  1.1961e-04,  ..., -1.8282e-04,\n",
       "             7.5704e-05, -2.7257e-04],\n",
       "           [-1.8803e-04,  3.4944e-05,  3.7949e-04,  ..., -1.0799e-04,\n",
       "            -2.9563e-04,  3.8213e-04],\n",
       "           ...,\n",
       "           [ 1.9857e-05,  6.3215e-06, -3.8079e-04,  ..., -1.7144e-05,\n",
       "             1.0652e-04, -2.5175e-05],\n",
       "           [ 4.2994e-06,  7.9232e-05,  6.1519e-06,  ...,  1.0372e-04,\n",
       "             3.9200e-04, -3.2036e-04],\n",
       "           [ 1.1626e-04, -6.1699e-05, -1.6180e-05,  ..., -3.7328e-04,\n",
       "             1.9466e-04,  1.3546e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.5751e-05,  4.8666e-04,  3.9055e-04,  ...,  7.2318e-07,\n",
       "            -4.6597e-05, -2.3779e-04],\n",
       "           [-9.3074e-06, -2.3127e-04,  2.2229e-03,  ..., -9.7519e-05,\n",
       "            -6.5366e-04, -2.4439e-05],\n",
       "           [-2.7561e-04, -4.9051e-04,  5.2855e-05,  ...,  3.7694e-04,\n",
       "             7.1647e-05,  2.6524e-04],\n",
       "           ...,\n",
       "           [ 4.4256e-05, -1.4388e-04, -8.8373e-05,  ..., -1.8158e-05,\n",
       "            -5.9017e-05, -3.2264e-05],\n",
       "           [ 2.4708e-04,  1.9760e-04,  2.9891e-05,  ..., -9.9089e-05,\n",
       "             5.2911e-04, -7.8072e-05],\n",
       "           [ 3.2111e-04, -9.2413e-06, -5.0064e-05,  ..., -3.8843e-04,\n",
       "             7.4832e-05,  2.3289e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.0000e-03, -7.9944e-04, -3.7475e-03,  ..., -1.6213e-04,\n",
       "            -2.9472e-04, -7.3621e-04],\n",
       "           [ 3.5114e-03, -3.0286e-04, -1.1475e-03,  ...,  5.8158e-05,\n",
       "             1.3645e-04, -2.2455e-03],\n",
       "           [-9.9611e-04, -1.4953e-05,  2.4199e-04,  ...,  3.1167e-04,\n",
       "            -7.7265e-04, -1.2331e-04],\n",
       "           ...,\n",
       "           [-2.0808e-04, -3.7061e-05,  6.1320e-05,  ...,  4.2456e-05,\n",
       "            -1.6174e-04, -4.2264e-05],\n",
       "           [ 4.8424e-05,  6.9803e-04,  4.3869e-04,  ...,  1.5554e-04,\n",
       "             8.6046e-04,  6.6218e-04],\n",
       "           [ 1.0034e-03, -1.7774e-04, -4.9881e-05,  ...,  1.3076e-04,\n",
       "             7.6322e-05, -3.4310e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.6082e-04, -1.5711e-03, -4.3526e-04,  ...,  9.1481e-05,\n",
       "            -1.1219e-04,  8.1598e-04],\n",
       "           [-3.3221e-04, -9.9096e-04,  2.2761e-04,  ...,  2.0694e-04,\n",
       "            -1.5406e-03, -4.7208e-04],\n",
       "           [ 3.4192e-04, -3.6169e-03, -6.6481e-05,  ...,  2.0567e-04,\n",
       "            -1.0630e-04,  2.0749e-04],\n",
       "           ...,\n",
       "           [-2.1160e-04, -4.4339e-04, -7.0959e-04,  ...,  4.3714e-04,\n",
       "            -6.1176e-05,  1.2097e-03],\n",
       "           [-1.8673e-04,  1.4578e-03, -2.3313e-04,  ..., -1.6442e-03,\n",
       "             8.2066e-04,  2.0694e-04],\n",
       "           [ 5.0038e-04,  1.4517e-04,  1.3898e-04,  ...,  6.1702e-04,\n",
       "             1.7740e-04, -3.4258e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.4255e-04, -8.8266e-04,  2.4493e-04,  ..., -4.4133e-06,\n",
       "             7.4837e-05,  2.6583e-04],\n",
       "           [-1.6983e-04, -2.1990e-05,  4.3452e-04,  ...,  2.8064e-05,\n",
       "             2.9183e-05,  1.7658e-04],\n",
       "           [ 1.5603e-04, -2.4651e-04, -6.0737e-05,  ...,  4.0897e-05,\n",
       "            -2.1841e-04,  8.6724e-06],\n",
       "           ...,\n",
       "           [ 4.0829e-04,  6.7947e-05, -3.8502e-04,  ...,  6.5205e-08,\n",
       "             1.2221e-04, -2.2796e-04],\n",
       "           [ 7.6814e-05,  1.9339e-04,  5.9567e-05,  ...,  1.7338e-05,\n",
       "             1.3742e-03, -3.9374e-04],\n",
       "           [ 7.8561e-05,  1.5863e-04, -9.2431e-05,  ..., -2.4682e-04,\n",
       "             4.5351e-04, -3.4272e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.1800e-04,  8.7266e-04, -2.7298e-04,  ..., -5.0761e-06,\n",
       "            -3.2811e-05, -2.0050e-04],\n",
       "           [-1.7152e-05, -9.8258e-04,  1.8878e-05,  ...,  6.1898e-04,\n",
       "             8.2013e-05, -6.3738e-04],\n",
       "           [ 3.0077e-05, -1.1020e-03,  2.4423e-03,  ..., -2.4242e-04,\n",
       "            -2.5214e-04, -1.9478e-05],\n",
       "           ...,\n",
       "           [ 4.5099e-04,  6.6271e-05, -5.0042e-05,  ...,  3.6674e-05,\n",
       "            -3.3566e-04,  6.4422e-05],\n",
       "           [ 4.5564e-04,  3.1803e-04, -7.9446e-07,  ...,  1.8863e-05,\n",
       "             6.3314e-04, -2.2710e-04],\n",
       "           [ 5.6651e-04, -4.5134e-05, -3.7721e-05,  ..., -3.4967e-05,\n",
       "            -7.4020e-06, -1.0593e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.8243e-04,  3.5860e-03,  4.9284e-05,  ..., -6.3744e-05,\n",
       "            -4.0767e-04, -1.6408e-03],\n",
       "           [-1.4627e-04, -2.2192e-03,  5.7709e-04,  ..., -2.2047e-03,\n",
       "            -6.2657e-04, -2.3189e-03],\n",
       "           [-1.3784e-05,  1.7710e-03, -2.2778e-03,  ...,  1.2162e-03,\n",
       "             2.8323e-04, -1.2284e-05],\n",
       "           ...,\n",
       "           [-4.8887e-04, -7.8411e-04, -3.6219e-04,  ..., -9.8565e-05,\n",
       "            -6.1872e-04,  2.3337e-07],\n",
       "           [-1.0929e-03, -1.9486e-04,  3.5249e-04,  ...,  8.2756e-05,\n",
       "            -1.7591e-04, -7.7994e-04],\n",
       "           [-1.8083e-04,  6.2918e-04,  6.3905e-05,  ...,  7.9162e-04,\n",
       "            -2.3798e-04,  4.3756e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.8176e-04,  6.8685e-04,  1.5631e-04,  ..., -6.8115e-05,\n",
       "            -7.8050e-06, -3.2380e-04],\n",
       "           [-2.0758e-03, -2.1564e-03,  1.1195e-03,  ..., -2.6232e-03,\n",
       "             1.5360e-03,  1.1540e-03],\n",
       "           [-1.5870e-03, -5.1986e-04,  9.5319e-04,  ...,  7.4076e-04,\n",
       "            -3.0284e-05,  3.5736e-03],\n",
       "           ...,\n",
       "           [-8.4276e-05,  4.4919e-04,  9.4982e-04,  ..., -2.3728e-04,\n",
       "            -4.7959e-04, -9.2136e-05],\n",
       "           [-4.3198e-05, -4.3183e-04,  2.7158e-04,  ...,  3.4166e-04,\n",
       "             6.8198e-04,  4.0432e-04],\n",
       "           [ 1.5411e-04,  1.7168e-04, -1.3865e-04,  ..., -2.5880e-04,\n",
       "             7.1430e-04,  5.3343e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.6121e-04, -7.4568e-04,  5.3223e-04,  ...,  1.8120e-05,\n",
       "            -1.1837e-04,  1.8498e-05],\n",
       "           [-4.3443e-05, -5.6204e-04, -1.9743e-05,  ...,  8.7242e-05,\n",
       "            -5.6453e-04, -6.8775e-05],\n",
       "           [ 3.5858e-05, -4.8217e-04,  2.1330e-05,  ...,  1.3527e-04,\n",
       "            -9.8173e-05, -1.0512e-04],\n",
       "           ...,\n",
       "           [ 1.8286e-04, -1.0703e-05,  7.1194e-05,  ...,  8.8217e-05,\n",
       "            -2.4634e-05,  1.0916e-04],\n",
       "           [ 3.3985e-06,  1.2811e-04, -7.7162e-05,  ..., -2.5462e-04,\n",
       "             3.7169e-05, -3.1346e-04],\n",
       "           [ 1.8981e-05, -1.6359e-05,  3.6689e-05,  ...,  1.9521e-04,\n",
       "             3.4159e-05, -1.3235e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.4141e-05,  6.3908e-04, -6.6315e-06,  ...,  4.4975e-05,\n",
       "            -7.5734e-05, -5.8673e-04],\n",
       "           [-1.6637e-03,  6.4469e-04, -7.2594e-04,  ...,  1.2029e-04,\n",
       "            -9.7011e-04, -7.8442e-04],\n",
       "           [ 3.1863e-04,  2.5403e-04, -1.0709e-05,  ...,  3.0212e-04,\n",
       "             2.5771e-04,  1.0056e-03],\n",
       "           ...,\n",
       "           [-2.2177e-05, -1.6919e-04, -7.3088e-05,  ..., -5.7312e-04,\n",
       "             1.5630e-04, -2.0295e-03],\n",
       "           [ 1.7426e-04,  3.0649e-04,  2.2062e-04,  ...,  1.8434e-05,\n",
       "             1.5188e-03,  4.9448e-04],\n",
       "           [ 1.1527e-04,  4.2232e-05,  4.4551e-05,  ...,  5.2212e-04,\n",
       "             1.9480e-04, -1.2219e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.4711e-04,  3.1633e-04,  5.2262e-05,  ...,  7.0865e-07,\n",
       "             5.0839e-05, -1.2197e-04],\n",
       "           [-6.0625e-05, -4.0346e-05,  9.5722e-06,  ..., -3.6458e-06,\n",
       "            -1.2311e-04,  1.1361e-05],\n",
       "           [-3.0213e-05,  1.6583e-06,  3.5512e-05,  ..., -5.3032e-05,\n",
       "            -1.3269e-04, -4.6245e-05],\n",
       "           ...,\n",
       "           [ 2.1014e-04, -1.5037e-05, -6.1559e-05,  ..., -7.6688e-05,\n",
       "            -2.9761e-05, -5.1321e-05],\n",
       "           [ 1.8641e-05, -5.2850e-06, -1.2103e-06,  ..., -4.0802e-06,\n",
       "             2.4535e-05, -7.4801e-05],\n",
       "           [ 3.5405e-05,  2.0948e-05, -1.0031e-05,  ..., -5.8432e-05,\n",
       "             1.4342e-05, -3.5381e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.7263e-04,  1.5289e-03,  4.6671e-04,  ..., -3.6027e-05,\n",
       "            -1.3861e-04, -5.3677e-04],\n",
       "           [-7.0664e-06,  8.0699e-04,  1.3466e-03,  ..., -6.9141e-04,\n",
       "            -1.8203e-04,  8.7766e-05],\n",
       "           [-3.8860e-04,  8.1426e-04, -8.9027e-05,  ..., -7.7021e-04,\n",
       "             3.3283e-04,  6.2088e-04],\n",
       "           ...,\n",
       "           [ 5.6671e-04, -1.1740e-04,  1.0798e-04,  ...,  8.9458e-05,\n",
       "             4.9206e-05,  5.5202e-05],\n",
       "           [-1.0009e-04,  1.3588e-05, -1.2390e-05,  ...,  8.5298e-05,\n",
       "             3.1603e-05, -6.2771e-04],\n",
       "           [ 1.3746e-04, -1.8762e-04, -5.8462e-05,  ...,  1.7109e-04,\n",
       "             4.5833e-05, -6.6896e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.2661e-03, -9.4625e-04, -6.9652e-04,  ..., -1.2062e-04,\n",
       "            -4.3456e-04, -3.3495e-04],\n",
       "           [-3.0248e-03, -6.0466e-03, -3.7693e-03,  ...,  4.9443e-04,\n",
       "            -3.8372e-04, -3.9668e-03],\n",
       "           [ 6.5269e-05,  3.9395e-03, -5.2619e-04,  ...,  2.4257e-03,\n",
       "            -2.3914e-04, -4.4983e-03],\n",
       "           ...,\n",
       "           [-4.7588e-04, -8.3505e-04,  2.9775e-04,  ..., -3.5160e-04,\n",
       "            -2.3567e-03, -1.5807e-05],\n",
       "           [ 7.1859e-04,  1.8829e-03, -7.2961e-04,  ..., -9.7179e-04,\n",
       "             4.2981e-03,  3.3246e-03],\n",
       "           [-1.0483e-03,  1.9259e-04,  9.3307e-05,  ..., -1.1683e-03,\n",
       "             1.1222e-04, -1.9353e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.3500e-04, -2.4725e-04,  5.1643e-05,  ..., -1.2148e-06,\n",
       "             1.3836e-05,  3.3262e-05],\n",
       "           [-1.4742e-03, -5.6174e-04, -1.3379e-03,  ...,  5.4843e-04,\n",
       "            -6.8622e-04, -5.3839e-03],\n",
       "           [-2.3723e-03, -2.9499e-03,  3.9929e-04,  ..., -5.5300e-04,\n",
       "            -2.9876e-06,  4.3165e-04],\n",
       "           ...,\n",
       "           [ 1.1797e-04, -6.6089e-06, -1.3032e-04,  ..., -2.2351e-04,\n",
       "            -4.9603e-05,  5.1248e-05],\n",
       "           [ 2.2161e-05,  1.1920e-07,  2.5556e-05,  ...,  9.4200e-06,\n",
       "             1.3653e-04, -3.4253e-05],\n",
       "           [ 1.9264e-04,  1.7271e-04, -8.6152e-05,  ..., -3.7817e-04,\n",
       "             4.8404e-05,  3.0060e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.6528e-05, -1.8611e-03,  2.5661e-04,  ..., -8.9654e-06,\n",
       "            -1.6107e-04, -3.0944e-04],\n",
       "           [-3.1616e-03, -6.9514e-04, -2.4113e-03,  ..., -1.1535e-03,\n",
       "            -1.0236e-03,  1.2595e-03],\n",
       "           [-3.2301e-04,  3.1443e-05,  1.6450e-05,  ..., -1.0884e-04,\n",
       "             5.1414e-04, -2.9791e-04],\n",
       "           ...,\n",
       "           [-1.5848e-04, -7.3500e-04, -4.4987e-04,  ...,  1.9178e-05,\n",
       "            -2.1701e-05,  1.6430e-05],\n",
       "           [ 2.6315e-05,  1.2972e-04, -2.9388e-04,  ...,  1.2253e-04,\n",
       "             4.9252e-04, -3.0393e-04],\n",
       "           [ 3.3762e-06,  4.1417e-05, -1.0001e-05,  ...,  1.1456e-04,\n",
       "             1.0846e-04,  1.8104e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.1450e-04,  2.7926e-04, -3.8223e-04,  ...,  1.9340e-05,\n",
       "            -9.1803e-05, -5.4451e-04],\n",
       "           [-4.7833e-05, -6.0827e-04, -1.0097e-04,  ..., -2.1314e-04,\n",
       "             1.3740e-04,  2.7601e-04],\n",
       "           [-1.4776e-03, -1.9123e-04, -1.8599e-04,  ..., -9.1019e-05,\n",
       "             2.8792e-05,  8.6045e-04],\n",
       "           ...,\n",
       "           [ 5.2587e-04, -9.2725e-07, -3.3494e-04,  ..., -2.5524e-04,\n",
       "             6.4182e-04,  1.3168e-07],\n",
       "           [-5.2927e-05, -9.5724e-05, -3.1758e-06,  ...,  1.6043e-06,\n",
       "             3.3177e-04, -3.4935e-04],\n",
       "           [ 2.4039e-04,  2.3510e-06,  2.1331e-04,  ...,  8.2100e-05,\n",
       "            -5.2329e-05, -7.1590e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.2548e-04,  6.4322e-04,  5.8624e-04,  ..., -1.9949e-05,\n",
       "             2.2053e-05, -6.4338e-05],\n",
       "           [ 6.5486e-04, -3.7863e-04,  1.3564e-04,  ..., -3.3538e-04,\n",
       "             1.0260e-05,  3.1191e-04],\n",
       "           [-1.8286e-04,  1.5547e-04,  2.4224e-04,  ...,  4.3171e-05,\n",
       "             3.4126e-03,  8.1756e-05],\n",
       "           ...,\n",
       "           [ 5.2752e-06, -3.7467e-05,  5.8045e-05,  ...,  1.1325e-04,\n",
       "             7.8121e-05,  2.6421e-04],\n",
       "           [-5.7804e-06,  3.3428e-06,  2.4684e-05,  ...,  1.5929e-05,\n",
       "            -7.9349e-05, -1.6696e-04],\n",
       "           [ 2.3430e-04,  3.4361e-04, -6.7637e-05,  ...,  7.7127e-04,\n",
       "            -2.1647e-04, -3.3372e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.0651e-05,  6.3515e-04, -5.3722e-04,  ..., -1.3602e-05,\n",
       "            -3.4433e-05, -5.1195e-04],\n",
       "           [ 1.0133e-03, -2.9096e-04, -2.0267e-04,  ...,  1.6304e-04,\n",
       "             1.4128e-04, -2.7218e-04],\n",
       "           [-1.1005e-04,  3.8867e-04, -2.6978e-04,  ...,  2.5231e-05,\n",
       "            -3.4778e-05,  1.3229e-04],\n",
       "           ...,\n",
       "           [-7.6249e-05, -1.1808e-03,  1.7679e-04,  ...,  7.1188e-05,\n",
       "             5.2002e-04, -1.3985e-05],\n",
       "           [-1.0602e-04, -6.1732e-05,  6.2638e-05,  ...,  1.8875e-06,\n",
       "             7.9640e-04, -1.0338e-03],\n",
       "           [ 3.3591e-04, -1.1052e-04, -1.5858e-04,  ...,  1.1350e-05,\n",
       "             9.8105e-05,  5.1466e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.6591e-04,  6.7042e-04,  6.6633e-05,  ...,  2.5993e-06,\n",
       "            -5.9022e-05, -3.9846e-04],\n",
       "           [ 1.6702e-04, -2.8422e-04, -4.0653e-04,  ..., -1.9754e-05,\n",
       "             1.0897e-04, -1.0280e-03],\n",
       "           [ 9.8469e-04,  3.0946e-04, -1.2055e-04,  ..., -4.9547e-04,\n",
       "             1.3665e-04, -2.2855e-05],\n",
       "           ...,\n",
       "           [ 1.2359e-04, -2.9764e-05, -3.0945e-04,  ...,  5.8114e-05,\n",
       "             3.1996e-06,  6.4295e-05],\n",
       "           [-1.0554e-04,  2.0005e-05, -7.2999e-07,  ..., -1.6830e-05,\n",
       "             6.3190e-04,  7.2954e-05],\n",
       "           [ 1.0306e-04,  7.5478e-05,  2.9196e-07,  ...,  3.1611e-04,\n",
       "             1.7780e-04, -9.9703e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.2779e-04, -8.3179e-04,  2.8409e-04,  ...,  7.9175e-06,\n",
       "            -1.9350e-05,  1.2610e-05],\n",
       "           [-7.9845e-05, -3.6822e-04,  4.3700e-04,  ...,  2.4449e-04,\n",
       "            -1.3273e-04,  2.6649e-04],\n",
       "           [-3.9948e-04, -1.9718e-04, -7.5983e-07,  ...,  7.6089e-05,\n",
       "             1.1980e-04,  7.1954e-05],\n",
       "           ...,\n",
       "           [-3.7292e-05,  3.0622e-05, -2.3411e-04,  ...,  4.0331e-04,\n",
       "            -1.9425e-04,  2.2358e-05],\n",
       "           [-5.3544e-05,  1.0660e-04,  5.8949e-06,  ..., -4.1464e-05,\n",
       "             5.1812e-04, -7.9537e-04],\n",
       "           [ 7.0698e-05, -2.8645e-05, -7.6889e-06,  ...,  1.2083e-04,\n",
       "             7.1250e-05, -1.2877e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-6.7746e-04,  2.8743e-04,  1.1263e-04,  ...,  8.3375e-06,\n",
       "            -1.5005e-05, -1.6561e-04],\n",
       "           [-3.4756e-05, -5.4119e-04,  3.3294e-05,  ..., -1.7021e-04,\n",
       "            -1.0311e-04, -3.0784e-06],\n",
       "           [-6.6947e-06,  7.4623e-04,  5.4688e-05,  ..., -4.6499e-05,\n",
       "             1.0136e-05, -6.6214e-05],\n",
       "           ...,\n",
       "           [-1.5447e-05, -2.2669e-04, -7.7876e-05,  ..., -5.5897e-05,\n",
       "             1.0792e-05,  3.7786e-04],\n",
       "           [-3.9055e-04,  6.3233e-05,  1.7362e-04,  ...,  2.5750e-04,\n",
       "             8.2594e-05, -3.0345e-04],\n",
       "           [ 7.3028e-04,  2.4798e-04, -2.5188e-04,  ...,  2.1845e-04,\n",
       "             4.3365e-05,  2.2092e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.2505e-04, -5.9476e-04,  9.2763e-05,  ..., -5.8482e-07,\n",
       "            -4.3031e-05, -3.6007e-04],\n",
       "           [ 2.4447e-03, -1.1300e-03, -9.6986e-04,  ..., -6.9660e-04,\n",
       "             2.9718e-04, -7.0972e-04],\n",
       "           [ 1.9774e-03,  2.0455e-03,  8.5637e-04,  ..., -6.5616e-04,\n",
       "             2.8275e-05,  9.9711e-04],\n",
       "           ...,\n",
       "           [-1.5474e-03, -3.2654e-04, -4.9304e-06,  ..., -5.7142e-04,\n",
       "            -8.5924e-05,  7.9148e-05],\n",
       "           [ 2.7777e-04,  5.3785e-04,  3.3806e-05,  ..., -5.5465e-05,\n",
       "             1.5162e-03,  1.5704e-04],\n",
       "           [-4.3049e-05, -1.1921e-05, -2.1939e-05,  ..., -4.5429e-04,\n",
       "             3.8209e-04,  3.4988e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.2897e-05, -8.2817e-05, -2.5495e-05,  ..., -6.4653e-06,\n",
       "            -2.7536e-05, -9.1665e-05],\n",
       "           [-2.9234e-04, -4.4046e-05, -4.4003e-04,  ...,  1.7688e-04,\n",
       "            -2.1633e-04,  2.6833e-04],\n",
       "           [-1.2125e-04, -9.2614e-04, -3.7676e-04,  ..., -1.8751e-03,\n",
       "             2.7585e-03, -6.8864e-05],\n",
       "           ...,\n",
       "           [ 1.5185e-04,  4.5537e-04,  6.2280e-05,  ..., -2.4719e-04,\n",
       "             3.9075e-06,  5.2408e-05],\n",
       "           [-1.3022e-05,  1.1365e-04,  1.3222e-06,  ...,  2.6466e-06,\n",
       "             1.3737e-04, -8.7912e-05],\n",
       "           [ 2.2691e-05, -2.4321e-05, -7.3591e-06,  ..., -2.4776e-05,\n",
       "             1.6163e-05,  1.1983e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.2258e-04,  4.1508e-04, -1.4738e-03,  ..., -5.6456e-05,\n",
       "            -1.4971e-04, -7.4619e-04],\n",
       "           [ 3.3016e-03, -1.0161e-03,  2.6561e-03,  ..., -1.1577e-04,\n",
       "             7.5294e-04,  2.7145e-03],\n",
       "           [-1.2080e-04, -7.9225e-05,  3.0733e-04,  ...,  7.7055e-04,\n",
       "            -2.7816e-03,  1.0822e-04],\n",
       "           ...,\n",
       "           [-1.4000e-03,  4.2283e-04,  2.7983e-04,  ...,  2.3135e-06,\n",
       "             3.9164e-04, -1.0420e-03],\n",
       "           [ 7.1173e-05,  7.1803e-05,  7.1475e-06,  ...,  6.6470e-05,\n",
       "             5.7549e-04,  5.8511e-04],\n",
       "           [-1.0703e-05, -2.0310e-04,  4.7753e-05,  ..., -4.1055e-04,\n",
       "             4.2003e-04,  9.5495e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-8.3613e-04,  4.8069e-03, -5.6605e-04,  ..., -3.5902e-05,\n",
       "            -3.9974e-05, -1.2467e-03],\n",
       "           [ 4.8530e-04, -1.2294e-03, -4.9806e-05,  ...,  4.2752e-05,\n",
       "            -1.4377e-03,  1.5873e-03],\n",
       "           [-1.5453e-03, -6.5523e-04,  3.8012e-03,  ..., -2.8115e-05,\n",
       "            -5.1052e-04,  6.1261e-05],\n",
       "           ...,\n",
       "           [-3.7820e-05, -2.0527e-03, -1.8502e-03,  ..., -1.4115e-04,\n",
       "             1.3189e-04, -1.6758e-05],\n",
       "           [-1.0248e-04, -2.2667e-04, -7.1308e-09,  ..., -1.0971e-04,\n",
       "            -8.2576e-05, -6.8469e-04],\n",
       "           [-4.1549e-04,  3.0441e-04,  1.1727e-06,  ..., -5.0658e-04,\n",
       "             3.1748e-04,  1.2675e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.2013e-07,  2.2219e-04, -2.9289e-04,  ...,  5.1867e-06,\n",
       "             8.6616e-05, -1.6126e-04],\n",
       "           [-6.4813e-06, -3.2396e-03, -1.9114e-04,  ...,  9.0065e-04,\n",
       "            -6.7686e-04,  2.6983e-04],\n",
       "           [-5.3885e-04, -1.0555e-03, -3.2512e-04,  ..., -2.0311e-04,\n",
       "            -6.9293e-05, -2.9942e-04],\n",
       "           ...,\n",
       "           [-4.1234e-04, -1.5851e-04, -1.0735e-04,  ...,  6.5718e-04,\n",
       "             5.0380e-04, -6.9713e-04],\n",
       "           [-7.3693e-05,  2.4342e-04,  1.6253e-04,  ...,  3.6563e-04,\n",
       "             2.2337e-04,  1.8437e-04],\n",
       "           [ 2.0721e-04, -4.2982e-05,  1.9257e-05,  ..., -2.2971e-05,\n",
       "             5.4390e-05, -1.6954e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.7576e-04,  1.6266e-04,  2.6102e-04,  ..., -1.0072e-04,\n",
       "            -1.9757e-04, -4.8237e-04],\n",
       "           [ 6.1585e-04, -1.5643e-03,  2.7671e-04,  ...,  1.2369e-03,\n",
       "            -2.6045e-04, -3.7709e-03],\n",
       "           [ 5.9878e-04,  1.5670e-03,  1.5129e-03,  ..., -3.3281e-04,\n",
       "             1.9315e-04, -1.8886e-03],\n",
       "           ...,\n",
       "           [-2.0305e-04,  5.4381e-05, -4.7913e-05,  ..., -2.5702e-04,\n",
       "             5.2256e-05, -2.1372e-04],\n",
       "           [-3.5322e-05,  3.0276e-04,  1.4746e-04,  ...,  7.0925e-05,\n",
       "             1.5103e-03, -3.6245e-04],\n",
       "           [ 5.4781e-05, -6.4123e-05, -3.2044e-05,  ..., -1.6356e-04,\n",
       "             1.2849e-03,  1.3681e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-6.7547e-04,  2.2200e-03, -1.3256e-03,  ..., -9.3605e-05,\n",
       "            -7.9737e-05, -4.1743e-04],\n",
       "           [ 1.9313e-03,  3.5366e-04,  1.7779e-04,  ..., -3.6499e-04,\n",
       "            -3.3374e-04, -4.2791e-05],\n",
       "           [-2.3053e-04, -1.8214e-05,  2.0969e-05,  ...,  1.5286e-04,\n",
       "             1.9588e-05,  2.8933e-05],\n",
       "           ...,\n",
       "           [-5.1494e-04,  2.4088e-04,  2.2524e-04,  ...,  1.1033e-06,\n",
       "             1.9197e-04,  2.2515e-04],\n",
       "           [-1.7961e-04,  2.7685e-04,  8.2904e-06,  ...,  1.0097e-04,\n",
       "             2.4250e-04, -3.5816e-04],\n",
       "           [ 1.5266e-04,  1.3668e-04,  3.2253e-05,  ..., -4.0384e-04,\n",
       "             3.1895e-06, -8.6341e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.4018e-04,  4.4895e-03,  6.2575e-04,  ..., -1.3750e-04,\n",
       "            -4.6589e-04, -2.0033e-03],\n",
       "           [-1.9882e-03,  3.0921e-03,  1.5472e-04,  ..., -1.8965e-04,\n",
       "             2.9287e-03, -4.4138e-04],\n",
       "           [ 2.2095e-04,  8.2243e-04,  1.8302e-04,  ..., -1.0364e-03,\n",
       "             3.3382e-04,  2.0607e-04],\n",
       "           ...,\n",
       "           [-1.0754e-03,  1.0729e-04,  2.0358e-03,  ...,  1.0664e-03,\n",
       "            -7.5379e-04, -1.4103e-03],\n",
       "           [-2.6065e-04, -9.5827e-04,  5.3510e-04,  ..., -9.2615e-06,\n",
       "             1.3859e-03, -9.1131e-04],\n",
       "           [ 1.2607e-04, -5.6350e-04,  9.4537e-05,  ...,  5.2115e-04,\n",
       "             1.4523e-04, -2.0320e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.4929e-05,  1.1063e-03, -6.4769e-04,  ...,  1.1672e-05,\n",
       "            -3.1020e-04, -1.8270e-03],\n",
       "           [-3.6747e-04, -3.4757e-03,  4.4588e-05,  ..., -4.0132e-05,\n",
       "            -8.6766e-04,  3.8219e-03],\n",
       "           [-1.3419e-04,  3.0735e-04, -9.0165e-05,  ..., -7.7638e-04,\n",
       "            -3.4181e-04, -5.9878e-04],\n",
       "           ...,\n",
       "           [ 7.7856e-04, -4.8375e-04,  3.6832e-04,  ...,  2.8256e-04,\n",
       "             9.7273e-04,  3.9497e-04],\n",
       "           [-6.1652e-04,  4.5600e-05,  2.3974e-04,  ...,  2.2275e-05,\n",
       "             7.0818e-04,  7.7917e-04],\n",
       "           [-1.0562e-04,  7.0105e-05,  3.1225e-05,  ...,  2.2174e-04,\n",
       "            -7.1762e-04, -6.9205e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.3758e-04,  5.4492e-04, -2.2142e-04,  ..., -1.7129e-05,\n",
       "            -4.0221e-05, -3.3958e-04],\n",
       "           [ 3.9541e-04, -3.7262e-04, -5.1043e-04,  ..., -1.2267e-04,\n",
       "            -8.8539e-05,  3.8619e-04],\n",
       "           [-1.9026e-04, -2.9317e-04, -1.9502e-04,  ..., -4.4558e-05,\n",
       "            -2.2588e-05,  7.4898e-05],\n",
       "           ...,\n",
       "           [-4.3029e-04, -2.1039e-05,  4.1654e-04,  ...,  6.7086e-04,\n",
       "            -1.7351e-04,  1.9302e-04],\n",
       "           [ 1.6483e-04,  6.4461e-05,  1.2807e-07,  ...,  8.1710e-06,\n",
       "             1.1223e-05,  2.6248e-04],\n",
       "           [ 5.6216e-04, -2.1544e-06, -3.8817e-04,  ..., -1.0098e-04,\n",
       "            -4.2346e-05, -1.8837e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 1.0199e-03, -7.6915e-04, -1.0366e-03,  ...,  9.0936e-05,\n",
       "            -2.2311e-04, -3.5310e-04],\n",
       "           [ 5.0546e-04,  1.0788e-03,  4.0647e-03,  ..., -6.7841e-03,\n",
       "            -8.6521e-05,  4.5564e-03],\n",
       "           [-1.8543e-04,  2.4940e-03, -4.4309e-04,  ..., -4.0997e-04,\n",
       "            -1.1630e-02, -4.7073e-03],\n",
       "           ...,\n",
       "           [ 7.6462e-04, -3.1214e-04,  4.3251e-04,  ...,  2.7822e-04,\n",
       "             1.3471e-05, -2.4076e-06],\n",
       "           [-1.7302e-04,  2.6513e-04,  4.4325e-05,  ...,  4.0857e-05,\n",
       "             1.9407e-03, -3.9734e-05],\n",
       "           [ 2.7408e-04,  8.1671e-05, -1.2063e-04,  ...,  6.1925e-04,\n",
       "             1.2095e-03, -5.2631e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-5.2846e-05,  5.3844e-04, -3.6660e-04,  ..., -1.9177e-05,\n",
       "            -1.1760e-04, -3.3047e-04],\n",
       "           [-1.0293e-03,  2.5757e-04, -3.0919e-04,  ..., -7.2232e-05,\n",
       "            -9.9767e-04,  3.5301e-05],\n",
       "           [-1.2148e-04,  3.7954e-05,  6.2433e-05,  ..., -1.8870e-05,\n",
       "             1.2457e-04, -1.8700e-04],\n",
       "           ...,\n",
       "           [ 3.0661e-04, -5.2011e-04, -2.3029e-04,  ..., -3.0667e-04,\n",
       "            -3.0586e-04, -2.3247e-04],\n",
       "           [-7.4386e-05, -3.3783e-05,  2.0453e-05,  ...,  4.9594e-05,\n",
       "            -6.6623e-05, -2.7310e-04],\n",
       "           [ 2.7123e-04,  4.2637e-05, -2.7008e-05,  ...,  2.3914e-04,\n",
       "            -7.2693e-05, -3.2674e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.7943e-05, -2.2785e-04, -2.7216e-05,  ..., -5.3961e-06,\n",
       "            -1.2701e-05,  2.9119e-04],\n",
       "           [-1.1931e-04, -3.2887e-04,  4.0009e-05,  ...,  1.7462e-04,\n",
       "             1.0303e-04, -4.4816e-05],\n",
       "           [-5.2804e-05,  6.7101e-04, -3.2261e-04,  ...,  8.1966e-05,\n",
       "             1.9571e-05,  1.2987e-05],\n",
       "           ...,\n",
       "           [-7.7355e-05,  4.3055e-07, -4.0644e-04,  ...,  1.2047e-06,\n",
       "             5.4492e-06, -6.1020e-05],\n",
       "           [-7.3543e-05,  7.0463e-05,  3.4918e-05,  ...,  4.3623e-06,\n",
       "             3.4385e-04, -4.1888e-04],\n",
       "           [ 1.8291e-04, -5.2969e-06, -2.4638e-05,  ...,  3.5507e-05,\n",
       "             8.2908e-05, -3.1753e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.6008e-04,  3.6738e-04,  2.3856e-04,  ..., -5.9527e-06,\n",
       "            -3.8318e-06,  8.6077e-05],\n",
       "           [-2.0723e-04, -2.6882e-04,  3.8538e-04,  ...,  1.0308e-05,\n",
       "            -2.8381e-05,  5.9736e-05],\n",
       "           [ 1.0515e-04,  5.0110e-04, -1.9693e-04,  ...,  6.4633e-06,\n",
       "             1.9051e-05, -8.2509e-05],\n",
       "           ...,\n",
       "           [-4.4296e-05, -5.4860e-05, -2.3746e-04,  ...,  4.7188e-06,\n",
       "             2.8844e-05,  2.7116e-04],\n",
       "           [ 3.8696e-05, -6.9957e-06,  2.2946e-05,  ...,  1.6405e-06,\n",
       "             8.8913e-05, -2.9736e-04],\n",
       "           [ 1.0049e-04,  7.4562e-05, -1.0859e-04,  ..., -1.0779e-04,\n",
       "             5.4414e-06, -3.7215e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.9575e-04,  9.7953e-04, -1.9139e-04,  ..., -2.7474e-05,\n",
       "            -1.8554e-04, -4.7060e-04],\n",
       "           [ 6.6860e-04, -7.6204e-04,  1.9646e-04,  ...,  3.0623e-04,\n",
       "             5.4234e-05, -2.4293e-03],\n",
       "           [-5.1436e-05, -7.0177e-04,  2.1783e-04,  ...,  1.9450e-05,\n",
       "            -2.7517e-05,  2.4293e-04],\n",
       "           ...,\n",
       "           [ 6.1445e-06, -1.3176e-05, -1.4854e-05,  ..., -9.0997e-05,\n",
       "             1.7459e-04,  1.8473e-04],\n",
       "           [-1.0858e-04, -1.9685e-04, -1.4218e-04,  ..., -9.3990e-06,\n",
       "            -4.7837e-05, -2.1373e-04],\n",
       "           [ 2.4704e-05, -1.3080e-04,  1.5831e-04,  ...,  1.1110e-04,\n",
       "             5.3437e-05,  5.0191e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-2.7315e-04,  1.0541e-03,  5.9582e-05,  ...,  9.0405e-05,\n",
       "            -1.8879e-04, -9.2660e-04],\n",
       "           [ 1.4297e-03,  2.4044e-03,  4.2168e-03,  ..., -1.7595e-03,\n",
       "             5.7753e-04,  6.6965e-03],\n",
       "           [ 1.4405e-03,  3.4836e-04,  1.3091e-03,  ...,  9.3664e-05,\n",
       "            -6.3400e-03, -3.6308e-05],\n",
       "           ...,\n",
       "           [-3.0656e-03, -1.7067e-03, -6.5023e-04,  ...,  1.8764e-05,\n",
       "            -3.2127e-03,  1.8099e-03],\n",
       "           [ 4.2243e-05,  2.8434e-04, -9.3205e-05,  ..., -7.9051e-05,\n",
       "             1.1295e-03, -5.8047e-04],\n",
       "           [ 6.5429e-04,  7.7603e-07, -5.8172e-04,  ...,  5.1704e-04,\n",
       "            -4.7649e-05, -2.2613e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.9582e-04,  1.6160e-04,  2.7076e-06,  ..., -3.9594e-05,\n",
       "            -4.0328e-05, -2.4802e-03],\n",
       "           [-2.7377e-04, -3.6650e-04,  3.4628e-04,  ..., -2.7087e-04,\n",
       "             5.6451e-04, -2.6755e-03],\n",
       "           [ 3.6642e-04,  8.6987e-04, -3.7854e-03,  ..., -2.0258e-04,\n",
       "             7.5621e-05, -1.8442e-04],\n",
       "           ...,\n",
       "           [ 1.5040e-03,  5.0630e-03,  6.2583e-04,  ..., -1.3555e-03,\n",
       "            -1.2064e-03,  5.0958e-05],\n",
       "           [-3.5170e-05, -1.2444e-04, -3.1811e-04,  ..., -1.1928e-04,\n",
       "             7.3013e-04, -4.1547e-04],\n",
       "           [-1.4440e-04,  3.3011e-05, -8.8857e-05,  ...,  2.9787e-04,\n",
       "            -1.5620e-06,  1.7414e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 7.5046e-04, -1.1075e-03,  2.1432e-04,  ...,  2.1829e-04,\n",
       "            -4.2132e-05,  3.0014e-05],\n",
       "           [-1.3896e-05, -8.4548e-04, -7.5807e-05,  ..., -1.2078e-03,\n",
       "            -2.5569e-04,  3.2047e-04],\n",
       "           [ 2.4616e-03, -1.6083e-03, -1.8291e-04,  ..., -3.0355e-03,\n",
       "             8.1243e-04, -4.5292e-03],\n",
       "           ...,\n",
       "           [-3.9184e-03,  1.9858e-04,  4.9835e-03,  ..., -2.4035e-04,\n",
       "             5.5216e-04,  3.1367e-04],\n",
       "           [ 4.3609e-04,  1.2083e-03,  1.8451e-04,  ..., -1.5804e-03,\n",
       "             4.1857e-04, -1.0435e-03],\n",
       "           [-9.4696e-04, -1.8079e-04, -1.0607e-03,  ..., -1.8612e-03,\n",
       "            -6.7858e-05,  1.0971e-03]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.0647e-04,  3.9703e-04,  2.3919e-04,  ..., -1.7211e-05,\n",
       "             4.1110e-05, -1.0178e-04],\n",
       "           [ 9.1563e-05, -1.2250e-04,  2.4209e-05,  ..., -5.0447e-05,\n",
       "            -6.9237e-05, -3.2893e-04],\n",
       "           [-4.2111e-04, -4.0835e-05,  2.2921e-04,  ..., -4.6217e-05,\n",
       "            -2.2724e-04,  1.9555e-05],\n",
       "           ...,\n",
       "           [-7.7584e-05, -1.5527e-04,  3.3019e-05,  ...,  3.5840e-04,\n",
       "            -1.7409e-07,  1.7735e-04],\n",
       "           [ 5.5631e-05,  1.2939e-04,  4.3718e-06,  ...,  2.3312e-05,\n",
       "             8.3860e-05, -5.8246e-05],\n",
       "           [ 1.4959e-04,  3.1480e-07, -9.2988e-07,  ..., -5.7182e-05,\n",
       "            -4.1842e-05, -5.3022e-06]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 5.0466e-06,  4.7506e-04,  3.4882e-04,  ...,  6.5259e-06,\n",
       "            -2.4748e-05,  5.7829e-05],\n",
       "           [ 6.5333e-05, -1.1264e-03,  4.7832e-04,  ..., -1.4902e-05,\n",
       "            -1.0753e-03, -6.3311e-05],\n",
       "           [ 9.0917e-05, -2.1540e-04,  4.9297e-05,  ...,  5.1444e-06,\n",
       "            -1.5211e-03,  1.4598e-04],\n",
       "           ...,\n",
       "           [ 2.3739e-04, -5.0219e-05,  7.9194e-05,  ..., -5.4853e-05,\n",
       "            -1.5142e-07,  7.8897e-06],\n",
       "           [-9.2720e-06,  1.5946e-05, -3.8354e-05,  ..., -3.2871e-05,\n",
       "             8.9038e-05, -1.4504e-04],\n",
       "           [ 6.6922e-05,  6.1584e-05,  3.1897e-05,  ..., -1.3698e-05,\n",
       "             5.4435e-06,  1.0631e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.2008e-04,  1.7842e-03, -1.1884e-03,  ...,  1.2254e-05,\n",
       "            -2.2282e-04, -6.2734e-04],\n",
       "           [ 4.4561e-04,  3.2448e-04, -7.2075e-04,  ...,  1.3051e-03,\n",
       "            -3.0751e-03, -1.8361e-03],\n",
       "           [-1.1291e-04,  2.8454e-04, -2.2787e-04,  ..., -1.6734e-04,\n",
       "             8.4351e-05,  3.0225e-04],\n",
       "           ...,\n",
       "           [ 9.2516e-05,  2.2021e-04, -2.6223e-05,  ..., -2.3744e-04,\n",
       "             7.2281e-04, -2.4859e-04],\n",
       "           [-2.5768e-04,  2.4673e-04, -1.7267e-05,  ...,  2.6410e-05,\n",
       "            -3.8813e-04, -2.1103e-04],\n",
       "           [ 2.4939e-04, -7.4668e-05, -1.7229e-04,  ..., -2.5358e-04,\n",
       "            -2.4008e-07, -1.6325e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.4421e-04,  1.4695e-03, -1.4688e-03,  ..., -4.8468e-06,\n",
       "            -1.4495e-04, -1.1615e-03],\n",
       "           [-1.2547e-04,  3.8471e-04,  7.6173e-04,  ...,  2.2789e-04,\n",
       "            -1.0054e-03, -5.3286e-04],\n",
       "           [ 6.0375e-04,  5.8601e-04,  1.9773e-03,  ...,  1.6175e-04,\n",
       "             1.1044e-03,  4.0916e-04],\n",
       "           ...,\n",
       "           [ 1.2764e-03, -2.1571e-03,  1.1398e-03,  ...,  4.7994e-04,\n",
       "             2.0907e-04,  1.0037e-03],\n",
       "           [ 1.4244e-03,  1.7396e-04,  1.3231e-03,  ...,  2.5858e-05,\n",
       "             6.6871e-04,  2.6724e-03],\n",
       "           [-4.0309e-04, -1.4173e-04,  6.7927e-04,  ..., -3.9899e-03,\n",
       "            -6.5369e-04,  5.5350e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-1.0948e-05,  4.1073e-03, -7.8898e-04,  ..., -1.1653e-04,\n",
       "            -2.9324e-04, -2.3852e-03],\n",
       "           [ 3.5913e-03,  1.5856e-03,  5.3602e-04,  ..., -1.3493e-03,\n",
       "            -1.1212e-04,  2.7333e-03],\n",
       "           [-3.8575e-03,  1.0265e-03, -8.0905e-04,  ...,  4.7147e-03,\n",
       "             3.3210e-03,  1.9822e-03],\n",
       "           ...,\n",
       "           [ 1.2063e-02, -7.7444e-05, -4.4087e-03,  ..., -2.2806e-04,\n",
       "             2.9487e-03,  2.3918e-05],\n",
       "           [-9.1384e-03, -6.4601e-03, -1.6708e-03,  ...,  3.2416e-03,\n",
       "             4.1828e-04, -4.4119e-03],\n",
       "           [ 1.5466e-03, -8.5950e-04, -4.6320e-03,  ...,  6.2769e-03,\n",
       "             5.7680e-03, -2.2372e-03]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[-4.6711e-04,  2.8116e-03,  6.5106e-04,  ..., -9.9904e-05,\n",
       "             2.2760e-05, -3.7789e-04],\n",
       "           [ 6.4105e-03, -1.1760e-03,  1.5216e-04,  ...,  9.0419e-04,\n",
       "            -7.7416e-04, -6.1970e-05],\n",
       "           [ 1.5115e-03, -5.4173e-04, -2.0757e-05,  ..., -1.4822e-05,\n",
       "            -5.0855e-04, -6.1979e-03],\n",
       "           ...,\n",
       "           [-2.6634e-05,  1.9338e-04,  3.9385e-04,  ...,  2.7505e-05,\n",
       "             2.0255e-03, -2.6637e-05],\n",
       "           [-2.6662e-04,  5.8487e-04, -4.0877e-04,  ..., -6.5947e-05,\n",
       "            -4.0445e-05,  8.0081e-05],\n",
       "           [ 5.4008e-04, -3.1069e-04, -5.7353e-04,  ...,  1.6242e-03,\n",
       "             1.4827e-04, -6.2910e-05]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 2.5234e-05,  7.3507e-04,  6.6882e-04,  ..., -1.0433e-04,\n",
       "            -1.8918e-05, -9.2640e-04],\n",
       "           [-5.9680e-05, -9.1148e-04,  5.7711e-05,  ...,  8.9040e-05,\n",
       "            -2.4055e-04, -1.2859e-04],\n",
       "           [-1.8567e-04, -1.2513e-04,  2.4887e-05,  ...,  3.7318e-05,\n",
       "             1.1854e-04,  3.1181e-04],\n",
       "           ...,\n",
       "           [-1.5607e-04,  1.2573e-04,  7.1038e-04,  ..., -1.4321e-05,\n",
       "             2.5936e-03, -4.3610e-04],\n",
       "           [ 3.3584e-05, -2.4190e-04, -1.4459e-04,  ...,  4.2507e-08,\n",
       "             7.0364e-04,  6.8887e-05],\n",
       "           [ 5.0646e-04,  6.7390e-05, -3.8631e-04,  ..., -3.3773e-04,\n",
       "             1.1932e-05, -1.2722e-04]]], device='cuda:0', dtype=torch.float64,\n",
       "         grad_fn=<MulBackward0>)],\n",
       " 'conti_attr_list': [tensor([ 0.2840,  0.0259, -0.0395,  0.0779, -0.0405, -0.0070,  0.0611, -0.1699,\n",
       "           0.0087,  0.0431, -0.0611, -0.0612,  0.5147,  0.3289, -0.0270, -0.0347,\n",
       "          -0.0241, -0.1167, -0.1098, -0.0618, -0.0351,  0.0627,  0.0109, -0.1920,\n",
       "          -0.0073,  0.0216, -0.0535,  0.0160, -0.2031,  0.0157, -0.0474,  0.1474,\n",
       "          -0.0249, -0.0162, -0.0582, -0.0810,  0.2040,  0.0276,  0.0214, -0.0333,\n",
       "           0.0800,  0.1731,  0.1302,  0.0663,  0.0319,  0.0387,  0.0539,  0.0117,\n",
       "           0.1007,  0.1069,  0.0244, -0.3844], dtype=torch.float64),\n",
       "  tensor([ 0.0192, -0.0350,  0.0374,  0.1147, -0.4170,  0.1294,  0.4633,  0.2496,\n",
       "           0.0664, -0.1533, -0.0220,  0.1108, -0.0588, -0.0202, -0.0021, -0.0071,\n",
       "          -0.0695,  0.0224, -0.0710,  0.0202, -0.0237,  0.0138, -0.0187, -0.0729,\n",
       "          -0.0927, -0.3317, -0.1383, -0.0379, -0.0204,  0.0518, -0.0094,  0.0214,\n",
       "           0.0258,  0.0102,  0.0413, -0.0073,  0.0149, -0.0364,  0.0446,  0.0535,\n",
       "           0.1144,  0.1299,  0.0482,  0.1139,  0.0858,  0.1037,  0.0709,  0.0705,\n",
       "           0.0302, -0.0279, -0.0016, -0.0068,  0.0055, -0.0253,  0.0341,  0.0158,\n",
       "           0.0373,  0.0163,  0.0221,  0.0414, -0.0341,  0.0033,  0.0023, -0.0256,\n",
       "           0.0578,  0.0505,  0.0106,  0.0491,  0.0945], dtype=torch.float64),\n",
       "  tensor([-0.1667, -0.1750, -0.1299,  0.0269,  0.0601,  0.0701,  0.0734, -0.3130,\n",
       "          -0.0842, -0.0530,  0.1080,  0.0060,  0.1990, -0.0908,  0.2995,  0.3755,\n",
       "           0.0283,  0.0645, -0.4663, -0.0109, -0.0009,  0.0143, -0.1311,  0.0057,\n",
       "           0.0379,  0.0101, -0.0616, -0.1305,  0.1308,  0.0333,  0.0149,  0.0603,\n",
       "          -0.0676, -0.0988,  0.0502,  0.0308, -0.0152, -0.0695,  0.0762,  0.1077,\n",
       "           0.1078,  0.0442,  0.0408,  0.0249,  0.1109,  0.1413],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.2470,  0.0160, -0.0582, -0.2237, -0.0535, -0.0021,  0.0200,  0.0925,\n",
       "           0.0226, -0.0960,  0.1222,  0.3179,  0.0800,  0.0066, -0.5453,  0.0857,\n",
       "           0.0140,  0.0693,  0.1211,  0.0441,  0.1958,  0.0930,  0.0447,  0.0316,\n",
       "           0.0126,  0.0469, -0.0032,  0.0383, -0.0880, -0.0533,  0.0194,  0.0513,\n",
       "           0.0471, -0.0341,  0.0661,  0.0123, -0.0848,  0.0770, -0.0138,  0.0017,\n",
       "           0.0136,  0.0817,  0.0643,  0.5196], dtype=torch.float64),\n",
       "  tensor([-0.1352, -0.0044,  0.1199,  0.1279,  0.3796,  0.0690,  0.0608,  0.0152,\n",
       "           0.0043, -0.0066,  0.0039, -0.0422, -0.0470, -0.0053,  0.3194,  0.2170,\n",
       "           0.0763,  0.2299,  0.0833,  0.1006,  0.0680,  0.1914,  0.3408,  0.1541,\n",
       "           0.2854,  0.1822, -0.0497,  0.1528,  0.0342,  0.1635,  0.0726, -0.0133,\n",
       "           0.3065,  0.0304,  0.1570], dtype=torch.float64),\n",
       "  tensor([ 4.0527e-04,  9.2501e-02,  9.3217e-02, -2.5911e-01,  1.4960e-01,\n",
       "           8.2904e-02, -5.6716e-03, -2.1517e-02,  7.9439e-02,  3.4508e-01,\n",
       "           5.6496e-02, -7.1825e-02, -8.3474e-02,  5.1371e-01,  2.7981e-01,\n",
       "           9.1404e-02,  1.0758e-02, -1.6580e-03,  4.2070e-02,  9.9307e-02,\n",
       "          -5.1174e-02,  2.3187e-01,  7.9979e-02, -3.2047e-02, -8.0813e-02,\n",
       "          -1.3460e-01,  1.8723e-02,  1.3928e-01,  1.2995e-01, -3.5256e-02,\n",
       "           9.0615e-02,  8.3346e-02,  5.1457e-02,  1.1181e-01,  8.0123e-02,\n",
       "          -4.1949e-02,  3.7031e-02,  7.3949e-02, -4.1112e-03, -1.5565e-01,\n",
       "          -8.3859e-02,  1.0722e-01,  1.1173e-01, -1.0444e-02],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.1127,  0.0425, -0.0368, -0.0994, -0.0841,  0.0817,  0.0603,  0.7625,\n",
       "           0.0335,  0.0472, -0.0103,  0.0053,  0.0445, -0.2762,  0.0603, -0.0720,\n",
       "           0.0401, -0.0079,  0.1688, -0.0634, -0.0157,  0.0090, -0.0326, -0.0456,\n",
       "          -0.0138, -0.0343,  0.4093], dtype=torch.float64),\n",
       "  tensor([-0.2265,  0.4146, -0.0236, -0.3764,  0.3059,  0.0813,  0.0484,  0.0834,\n",
       "           0.0110,  0.2412,  0.2123,  0.0962, -0.2423,  0.0347, -0.0818, -0.1340,\n",
       "           0.0030,  0.0236, -0.0538, -0.2413,  0.1906], dtype=torch.float64),\n",
       "  tensor([ 0.2233,  0.2497, -0.0596, -0.0393, -0.0992,  0.1146, -0.1451,  0.2807,\n",
       "          -0.0472, -0.0264,  0.1686,  0.5092,  0.3025, -0.0464, -0.0728, -0.2230,\n",
       "           0.1137,  0.0875, -0.0391, -0.0697,  0.0970, -0.0833,  0.0532, -0.0229,\n",
       "           0.0858, -0.0258, -0.0589,  0.4000], dtype=torch.float64),\n",
       "  tensor([ 0.1379, -0.1506,  0.2918, -0.0029,  0.0624,  0.2530,  0.0480,  0.4741,\n",
       "           0.3296,  0.0977, -0.1990,  0.0737,  0.2000,  0.1946,  0.0632, -0.2533,\n",
       "           0.0279,  0.1256, -0.0534,  0.1549,  0.3392,  0.1874,  0.0055,  0.1237,\n",
       "          -0.0321,  0.1965], dtype=torch.float64),\n",
       "  tensor([ 2.2959e-01,  4.5153e-01,  1.2564e-01,  4.0764e-02,  1.9863e-01,\n",
       "          -5.8850e-02,  2.5757e-01, -1.7715e-01,  2.3384e-02, -1.3116e-01,\n",
       "          -3.9086e-02, -1.0784e-02,  1.8093e-01, -2.7490e-02,  4.7351e-02,\n",
       "           2.9409e-02, -6.1822e-02, -4.2260e-02, -4.7545e-02, -1.1679e-01,\n",
       "          -4.5656e-02,  1.7886e-02,  3.6339e-02, -9.9099e-03, -9.1595e-02,\n",
       "           2.8451e-01, -2.3797e-03, -7.8200e-02,  1.5251e-02, -5.2732e-03,\n",
       "          -7.8123e-02,  5.1345e-02,  1.2754e-01, -5.6630e-04,  9.1439e-02,\n",
       "          -1.4781e-02, -9.6117e-02,  9.8008e-02,  7.2309e-02,  6.5534e-02,\n",
       "          -9.6035e-03, -4.3756e-04, -6.7212e-02, -1.5501e-02, -6.8809e-02,\n",
       "          -6.4648e-03,  1.8314e-02, -6.7878e-02,  9.1945e-02],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 6.2101e-02, -1.2580e-01,  3.3339e-01, -2.6691e-01,  5.6001e-02,\n",
       "          -1.4770e-01, -1.6413e-02,  6.9668e-02, -5.9408e-03, -9.5813e-05,\n",
       "           2.6742e-02,  9.6789e-02,  4.4566e-02,  2.6139e-01,  1.7391e-01,\n",
       "           4.5838e-02,  1.2456e-02, -4.1419e-02,  7.4366e-02, -1.0616e-01,\n",
       "           3.5092e-02,  1.3733e-02, -2.1161e-01,  6.6769e-03,  4.4545e-02,\n",
       "          -3.7256e-02, -2.7968e-02,  9.8342e-03, -1.7538e-03,  6.6933e-02,\n",
       "          -3.9583e-02,  3.1196e-02,  9.4995e-02,  7.0003e-02, -2.8957e-01,\n",
       "           2.0580e-01, -2.7235e-01, -6.9253e-03,  7.0275e-02,  9.6899e-02,\n",
       "          -1.8701e-02, -3.1090e-02, -1.9001e-02, -7.0768e-03, -1.0455e-02,\n",
       "          -1.4779e-02,  1.2113e-01, -2.8117e-02, -2.2759e-02,  1.3808e-02,\n",
       "           4.0905e-02, -1.6701e-01,  2.9085e-02,  5.8449e-02,  8.4940e-02,\n",
       "           7.8194e-03, -1.8690e-02, -3.4964e-02,  1.2185e-02, -5.9763e-03,\n",
       "          -2.0162e-02,  2.3576e-02,  2.5973e-02, -9.2909e-03, -4.5816e-02,\n",
       "          -1.9641e-02, -5.2381e-02], dtype=torch.float64),\n",
       "  tensor([ 0.0918,  0.0394, -0.0162,  0.0102, -0.0381, -0.1178,  0.0257,  0.0239,\n",
       "          -0.0143, -0.0514,  0.8558,  0.1087,  0.1767,  0.1154, -0.0187,  0.0206,\n",
       "           0.0019, -0.0310,  0.0365, -0.0316, -0.0393,  0.0247, -0.0247,  0.0106,\n",
       "           0.0796,  0.0246,  0.0049,  0.0231, -0.0943, -0.0302,  0.0623, -0.0745,\n",
       "           0.1461,  0.0510, -0.0721, -0.0853, -0.0997, -0.0221,  0.0065,  0.0308,\n",
       "           0.0169, -0.0064,  0.0326, -0.0369,  0.1460], dtype=torch.float64),\n",
       "  tensor([ 0.0194, -0.1353,  0.0726,  0.0377, -0.0210,  0.0043,  0.0078, -0.0679,\n",
       "          -0.0393, -0.0952, -0.0042,  0.6750,  0.4620, -0.2061,  0.0107, -0.0314,\n",
       "          -0.0137,  0.0135, -0.0591, -0.0537, -0.0320,  0.0173,  0.0371,  0.1698,\n",
       "           0.0474,  0.0201, -0.1135,  0.0229,  0.0755, -0.0390,  0.0280, -0.0069,\n",
       "          -0.0443,  0.0189,  0.0091,  0.0099,  0.1130,  0.2263],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 2.4964e-01,  1.4252e-02, -8.0942e-02,  5.2945e-02, -3.5137e-02,\n",
       "           3.1200e-02,  9.2377e-02,  5.9336e-02,  2.7388e-02,  2.9436e-01,\n",
       "           9.9476e-03,  1.5625e-01, -4.7127e-02, -7.6096e-02, -5.3753e-02,\n",
       "           2.7684e-02, -4.7659e-02, -5.7724e-02,  1.0792e-01,  2.7733e-01,\n",
       "           6.3919e-02,  1.4930e-01,  3.3736e-01,  9.2265e-02,  6.3863e-02,\n",
       "          -4.9543e-04,  5.6531e-02, -8.1526e-02,  2.4192e-02, -2.0975e-01,\n",
       "           1.2208e-01,  4.3282e-02, -5.0729e-02,  3.9532e-02,  6.1882e-02,\n",
       "           1.9949e-02,  1.5410e-01,  9.5608e-02,  1.4791e-01,  3.2752e-02,\n",
       "           3.7278e-02,  8.2335e-02,  6.4435e-02, -1.1733e-02,  3.8287e-02,\n",
       "           2.0744e-01, -1.3072e-04,  3.9435e-02,  1.4055e-01,  7.0957e-02,\n",
       "          -1.8477e-02,  5.8563e-02, -1.4750e-02,  2.6989e-01],\n",
       "         dtype=torch.float64),\n",
       "  tensor([-0.0225, -0.0760,  0.2361,  0.0685, -0.0339,  0.2885,  0.1081,  0.0264,\n",
       "          -0.0064,  0.5296,  0.3272, -0.0655,  0.0474, -0.1735,  0.0182,  0.0089,\n",
       "          -0.0020,  0.1384,  0.0506, -0.1094,  0.1777,  0.0782,  0.0286, -0.0297,\n",
       "           0.0943,  0.0370,  0.0091, -0.0702, -0.0248,  0.0911,  0.0900, -0.0076,\n",
       "           0.1021, -0.0331, -0.0039,  0.0086,  0.0926, -0.0269,  0.0167,  0.0269,\n",
       "          -0.0523,  0.0156, -0.0384,  0.0669,  0.0155,  0.1088,  0.0394,  0.0368,\n",
       "           0.1008, -0.0336,  0.2578], dtype=torch.float64),\n",
       "  tensor([ 0.2738, -0.2909, -0.4096,  0.0570,  0.0048, -0.0192, -0.1916, -0.0582,\n",
       "           0.0312, -0.1778, -0.0151,  0.1717,  0.0899,  0.0925, -0.1061, -0.1176,\n",
       "           0.0141, -0.0034, -0.0014, -0.0504, -0.0511,  0.1286,  0.3017,  0.1266,\n",
       "          -0.0166,  0.1101,  0.0550,  0.1238,  0.0018, -0.0723, -0.0400, -0.1121,\n",
       "          -0.0136,  0.0728, -0.0935,  0.0945, -0.0999], dtype=torch.float64),\n",
       "  tensor([-0.0402, -0.0861,  0.5651,  0.1167,  0.0604,  0.0623,  0.1351,  0.0510,\n",
       "           0.0688, -0.0181, -0.0256, -0.0283,  0.0201,  0.0414, -0.0386,  0.1084,\n",
       "           0.0755,  0.4307,  0.3878,  0.0806,  0.0085,  0.0865,  0.0576, -0.0272,\n",
       "          -0.0140,  0.0376,  0.0400,  0.0476, -0.0331, -0.0281,  0.0554, -0.0936,\n",
       "           0.0140,  0.0512, -0.0549, -0.0112,  0.0501, -0.0009,  0.0294, -0.0121,\n",
       "          -0.0915, -0.0102,  0.0877,  0.0393,  0.0964,  0.0864, -0.0118,  0.3139],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.2099, -0.3623, -0.0615,  0.0434,  0.2203, -0.0430,  0.4777,  0.3637,\n",
       "          -0.0725,  0.0239,  0.0619, -0.0125, -0.1959, -0.0845,  0.2172,  0.0548,\n",
       "          -0.0672, -0.0597,  0.0317,  0.0375,  0.1380,  0.0863, -0.0820, -0.1593,\n",
       "          -0.0028,  0.0585,  0.0651,  0.0016,  0.0139,  0.0075, -0.2081, -0.0056,\n",
       "          -0.0740,  0.0250,  0.0848, -0.0078,  0.1619,  0.0282,  0.3094],\n",
       "         dtype=torch.float64),\n",
       "  tensor([-0.2286, -0.0293,  0.2587,  0.0106,  0.4359,  0.2197, -0.0912, -0.1839,\n",
       "          -0.0289,  0.0765,  0.0176,  0.3410,  0.2082, -0.1677, -0.1244,  0.0981,\n",
       "           0.0445,  0.0281,  0.1237,  0.0326,  0.0518,  0.0206, -0.0154,  0.1076,\n",
       "           0.0513,  0.0092,  0.0267,  0.0607, -0.0358, -0.0204,  0.0148,  0.0176,\n",
       "           0.0251, -0.0292,  0.0512,  0.1087,  0.1649, -0.0138, -0.0750, -0.1360,\n",
       "          -0.0186, -0.0491,  0.1635, -0.0157,  0.1326,  0.1959, -0.1712,  0.1025,\n",
       "           0.0756, -0.0836,  0.0876], dtype=torch.float64),\n",
       "  tensor([ 0.7354,  0.1096,  0.0127, -0.0187, -0.0468,  0.0431,  0.0030, -0.0735,\n",
       "          -0.0827,  0.0326,  0.0126,  0.0744,  0.2207,  0.1242,  0.0403,  0.0104,\n",
       "           0.0464, -0.0406,  0.0653, -0.0369, -0.0555,  0.0176,  0.1060,  0.0470,\n",
       "          -0.0301, -0.0529,  0.1518, -0.0721, -0.0251, -0.0663,  0.0395, -0.0142,\n",
       "          -0.0446, -0.0345,  0.0411,  0.0387,  0.0600,  0.0035, -0.0210,  0.3864],\n",
       "         dtype=torch.float64),\n",
       "  tensor([-0.1237,  0.0944,  0.0523, -0.0347, -0.2106, -0.1569, -0.2192,  0.0167,\n",
       "          -0.0859,  0.1346,  0.1134, -0.0303, -0.2262, -0.1424,  0.0014, -0.1352,\n",
       "          -0.0264,  0.0149, -0.3018, -0.1683, -0.0835, -0.1690, -0.0934, -0.0811,\n",
       "          -0.0902, -0.1925, -0.0799, -0.1813, -0.1111,  0.1011, -0.1759, -0.3022,\n",
       "           0.2845], dtype=torch.float64),\n",
       "  tensor([ 0.2990,  0.0759,  0.0366, -0.0403,  0.2230, -0.3770,  0.1521,  0.1131,\n",
       "           0.0453,  0.2886,  0.1321, -0.0005,  0.0983,  0.0535, -0.1313, -0.1004,\n",
       "          -0.0105,  0.0068,  0.0290,  0.0086,  0.0276,  0.0281, -0.1943, -0.1114,\n",
       "           0.0708,  0.1018,  0.0804, -0.0558, -0.0448,  0.0267,  0.0443,  0.1514,\n",
       "           0.0056,  0.0932,  0.0713,  0.0108, -0.0791,  0.0699,  0.0731,  0.0109,\n",
       "           0.2718], dtype=torch.float64),\n",
       "  tensor([-0.0272,  0.4951,  0.1285,  0.0451,  0.2198,  0.2025, -0.1457, -0.0095,\n",
       "           0.3953,  0.4047,  0.1776,  0.0586, -0.1114,  0.0425,  0.0656,  0.0593,\n",
       "           0.1648, -0.0106,  0.1539,  0.1164,  0.1600,  0.0134,  0.0382, -0.0143,\n",
       "          -0.1892,  0.0525,  0.1017,  0.0765,  0.1479,  0.0554,  0.0186,  0.0219,\n",
       "           0.0813,  0.0992, -0.0166,  0.1533], dtype=torch.float64),\n",
       "  tensor([-0.2057,  0.1332,  0.0483, -0.0112,  0.0325, -0.0341,  0.0233,  0.1219,\n",
       "           0.0861,  0.0651,  0.0438,  0.0616,  0.5027,  0.4220,  0.1576, -0.0069,\n",
       "           0.0100,  0.0371, -0.0500,  0.0508, -0.0175,  0.0644,  0.0310,  0.0016,\n",
       "           0.0763, -0.0632,  0.0261, -0.0170,  0.0445, -0.0044, -0.0148,  0.0053,\n",
       "           0.0194,  0.0253,  0.0666, -0.3408, -0.0321,  0.0078,  0.0325, -0.0322,\n",
       "           0.1613,  0.0738,  0.0721, -0.0045,  0.0172,  0.1202,  0.0241,  0.4552],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.0752,  0.0570,  0.0249, -0.0011, -0.1955, -0.0735, -0.0530,  0.1177,\n",
       "           0.0955,  0.0742,  0.1327,  0.0688,  0.0204,  0.1170,  0.0095,  0.1159,\n",
       "           0.0699,  0.0091,  0.4135,  0.0817, -0.1003,  0.0625, -0.0133,  0.0418,\n",
       "          -0.0696, -0.0370,  0.2511,  0.0146,  0.0916,  0.0933,  0.3663,  0.1444,\n",
       "          -0.0031, -0.0548, -0.0842,  0.0622,  0.1113,  0.1486,  0.2153, -0.0204,\n",
       "          -0.1891], dtype=torch.float64),\n",
       "  tensor([ 0.1030,  0.0269,  0.4436,  0.2488,  0.0679,  0.1240,  0.0034,  0.1279,\n",
       "           0.2334,  0.1410,  0.0248,  0.0612,  0.0488,  0.0438,  0.0591,  0.0340,\n",
       "           0.0480,  0.0464,  0.0374, -0.0094,  0.0363, -0.0045, -0.0058,  0.0417,\n",
       "           0.0106, -0.0275,  0.0193, -0.0189,  0.0124,  0.0815, -0.0243, -0.0435,\n",
       "           0.0552,  0.1064,  0.0681, -0.0288,  0.0289, -0.7134,  0.0365,  0.0555,\n",
       "           0.0709, -0.0096,  0.0099, -0.0177, -0.0087, -0.0500,  0.0835],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.3074,  0.4086,  0.0794,  0.1430,  0.0582, -0.2965, -0.0007,  0.2979,\n",
       "          -0.0163, -0.0870, -0.2753, -0.1266,  0.0700,  0.4109,  0.1293,  0.0113,\n",
       "           0.0776, -0.2274,  0.0342, -0.0809, -0.0537,  0.0427, -0.1246, -0.0314,\n",
       "           0.0494,  0.0872,  0.0469, -0.0287,  0.0491,  0.0776,  0.0064, -0.0487,\n",
       "           0.1360], dtype=torch.float64),\n",
       "  tensor([ 0.3822, -0.1377, -0.2015, -0.1075,  0.0447, -0.4169, -0.0941,  0.0618,\n",
       "           0.0118,  0.0595,  0.2242, -0.1075, -0.1000, -0.0524,  0.0537, -0.0302,\n",
       "          -0.1220, -0.1013,  0.0647, -0.1030,  0.0787,  0.0383,  0.0658, -0.0182,\n",
       "          -0.1079, -0.0284,  0.0556,  0.0715,  0.1343,  0.1810,  0.1174, -0.0164,\n",
       "          -0.0359,  0.0853,  0.2181,  0.1277,  0.0498, -0.0822, -0.0139, -0.1126,\n",
       "          -0.0874, -0.2440, -0.0528,  0.0888,  0.0749, -0.0924,  0.0222, -0.1569,\n",
       "           0.0193], dtype=torch.float64),\n",
       "  tensor([-0.0707, -0.0704,  0.2476, -0.1007,  0.0779,  0.0097, -0.0193,  0.0514,\n",
       "          -0.0444,  0.0486, -0.0749, -0.0987, -0.0903,  0.4918,  0.2281,  0.1623,\n",
       "          -0.0579,  0.1575,  0.0262, -0.0558,  0.5013, -0.0792, -0.0090,  0.0182,\n",
       "           0.0634,  0.2554, -0.0798,  0.0280,  0.0035, -0.0492,  0.0399,  0.0687,\n",
       "           0.0255,  0.0149,  0.0483,  0.0210, -0.1288,  0.1657],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.4713,  0.0879, -0.0900,  0.0623,  0.0508, -0.1953, -0.0583,  0.0487,\n",
       "          -0.0917,  0.1438,  0.3644,  0.4373,  0.0093, -0.1058, -0.1067,  0.0434,\n",
       "           0.0881,  0.0246, -0.1121,  0.0424, -0.0055, -0.0434,  0.1692,  0.0985,\n",
       "          -0.0123,  0.4289], dtype=torch.float64),\n",
       "  tensor([ 6.5292e-01,  3.8633e-02, -5.2661e-02,  7.2570e-02, -2.9338e-01,\n",
       "           5.6541e-02,  1.2743e-02, -4.2537e-03, -1.3465e-03,  1.9437e-01,\n",
       "           9.6558e-02,  4.6519e-02, -2.6834e-02,  1.5990e-01,  6.2808e-03,\n",
       "           1.2623e-01, -1.1395e-01,  7.7102e-03, -7.1774e-02,  9.1136e-02,\n",
       "           2.8042e-02,  4.9427e-02,  1.7291e-01,  2.3102e-02,  1.5937e-01,\n",
       "          -1.9336e-02,  4.7020e-02, -4.1239e-02, -6.3748e-02, -4.5479e-02,\n",
       "          -6.0006e-02, -5.5004e-02,  1.1649e-02, -1.2395e-02, -2.9764e-02,\n",
       "          -4.9681e-03,  2.3054e-02, -2.3344e-02,  3.1442e-04,  1.7278e-02,\n",
       "           1.0287e-01, -1.5856e-02,  1.0334e-01, -3.0510e-02,  1.9305e-01],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 1.8834e-01, -2.4774e-02,  3.3172e-01, -1.8898e-01,  5.4220e-03,\n",
       "           3.3534e-01, -1.0442e-01, -2.2999e-02, -2.3594e-02, -7.0556e-03,\n",
       "          -2.1820e-02,  2.0763e-02,  5.1542e-02,  3.0926e-02, -5.7189e-02,\n",
       "           8.8801e-02, -1.6827e-01,  2.5971e-02,  7.3590e-02,  3.8012e-01,\n",
       "          -3.8763e-02, -3.3139e-02, -3.1402e-02,  1.4159e-03, -1.4005e-02,\n",
       "          -1.2664e-01, -4.6808e-02,  2.2267e-02,  2.6133e-02,  2.1126e-04,\n",
       "           7.6000e-02,  3.6032e-02, -1.3873e-02,  2.9529e-02,  5.5841e-02,\n",
       "          -1.0745e-01, -8.1350e-02, -3.7776e-02, -7.3638e-02, -7.5610e-02,\n",
       "          -1.4236e-02, -6.4421e-03,  9.0953e-02, -4.5502e-02, -7.4790e-02,\n",
       "           9.8720e-03, -1.3277e-01,  1.6707e-01,  2.1213e-01,  6.5623e-02,\n",
       "          -4.2688e-02, -4.6266e-02, -2.9923e-02,  4.9056e-02,  4.5400e-02,\n",
       "           5.2720e-02, -3.7982e-02, -1.1191e-01,  7.4324e-03,  4.8762e-02,\n",
       "           9.6750e-02,  1.5124e-02,  6.3536e-02,  3.5251e-02,  1.0665e-01,\n",
       "           2.2016e-02,  1.1520e-01,  1.1358e-02,  2.2193e-02, -6.0682e-02,\n",
       "           2.9027e-02,  4.4254e-03, -4.7933e-02, -1.4719e-02,  3.7083e-02,\n",
       "           6.2539e-03, -2.3875e-03,  4.5472e-02, -6.4667e-02,  1.7965e-01],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 4.8881e-01,  6.5555e-03,  1.0112e-02, -1.4285e-01, -4.3108e-03,\n",
       "           1.3585e-02, -3.9543e-02,  4.7000e-04, -2.1330e-01, -2.1064e-02,\n",
       "           5.9600e-03,  7.6976e-02, -9.2489e-02,  1.4127e-01, -1.1961e-02,\n",
       "          -1.3823e-01,  2.9856e-01,  2.0708e-01, -5.0524e-02,  2.5208e-01,\n",
       "          -1.2055e-01, -2.4335e-01,  1.1389e-03,  2.5783e-01, -6.9658e-02,\n",
       "          -1.0452e-01, -6.8306e-02, -1.4837e-01,  5.0748e-02, -1.2650e-01,\n",
       "          -2.7506e-02, -7.9459e-02,  1.5574e-02,  4.2163e-02,  3.7328e-01],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.4923,  0.0230, -0.0118,  0.0492, -0.2070, -0.0483, -0.1433,  0.0101,\n",
       "          -0.0301,  0.0335,  0.3832,  0.3170,  0.0100, -0.0629,  0.0363, -0.0014,\n",
       "          -0.0377, -0.0125, -0.0144, -0.0037,  0.0069, -0.0357,  0.0635, -0.0140,\n",
       "          -0.0827, -0.0043, -0.0147,  0.0805, -0.0264,  0.0705, -0.0547,  0.0183,\n",
       "           0.1147, -0.0093, -0.0593,  0.1244, -0.0729,  0.1915,  0.0762, -0.0103,\n",
       "           0.0052, -0.0101,  0.0406, -0.1078, -0.0302,  0.0364, -0.0894, -0.0854,\n",
       "          -0.0213, -0.0879, -0.0335, -0.0090,  0.0017,  0.0122, -0.0081,  0.0278,\n",
       "           0.0026, -0.1783, -0.1450,  0.0781,  0.0604, -0.1436,  0.0781,  0.0678,\n",
       "          -0.0202,  0.0394, -0.0118,  0.1068, -0.0898,  0.1132],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.1967,  0.3358, -0.5492, -0.0484, -0.2672, -0.1420,  0.1136,  0.0380,\n",
       "          -0.0211,  0.2060,  0.1647,  0.1345,  0.0629, -0.0892, -0.0027, -0.0422,\n",
       "           0.2507, -0.2629,  0.0743,  0.0746,  0.0215,  0.0674,  0.0815,  0.0284,\n",
       "          -0.0606,  0.1941,  0.0015,  0.0262,  0.0100,  0.0155, -0.0239,  0.1489],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.0054, -0.0900,  0.0652,  0.2587, -0.3318,  0.2267, -0.0023,  0.0495,\n",
       "           0.0698,  0.2338,  0.1292,  0.0891, -0.0085, -0.1745,  0.0228, -0.0607,\n",
       "           0.0441,  0.0333,  0.0240,  0.2147,  0.0328, -0.0584,  0.0387, -0.0434,\n",
       "           0.2132,  0.1062,  0.0733,  0.1322, -0.0325,  0.0126,  0.0986,  0.0806,\n",
       "          -0.0032,  0.0572,  0.1417,  0.0742,  0.1575,  0.1093,  0.1490, -0.1938,\n",
       "           0.0079, -0.1074,  0.2976], dtype=torch.float64),\n",
       "  tensor([ 8.1456e-02,  5.1201e-01,  1.5136e-01,  1.1146e-01,  1.3353e-01,\n",
       "           2.1010e-02,  6.8137e-02,  6.8507e-02,  1.5223e-01,  1.3332e-01,\n",
       "          -6.2265e-02,  4.2109e-04,  1.7201e-01,  2.1402e-01,  4.6111e-02,\n",
       "           1.0796e-02,  4.0870e-01, -5.5928e-02, -4.3655e-02,  1.7187e-01,\n",
       "           2.3270e-01, -8.5719e-02, -4.5537e-02,  5.4950e-02, -3.2621e-02,\n",
       "          -3.2378e-02, -5.7554e-02,  3.0782e-02,  4.1629e-04, -7.5277e-03,\n",
       "           1.3519e-01,  8.4665e-03,  5.4917e-02,  7.3191e-02,  3.5118e-01],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.0898,  0.2295,  0.1167,  0.0989,  0.4904, -0.0731,  0.0265, -0.0266,\n",
       "           0.0761,  0.0084, -0.0075,  0.1304,  0.1892,  0.3549,  0.1655,  0.2247,\n",
       "          -0.0667,  0.2172,  0.0124, -0.0520,  0.1992,  0.1489,  0.1610, -0.0122,\n",
       "           0.0727, -0.0036,  0.1375], dtype=torch.float64),\n",
       "  tensor([ 0.3290,  0.2152, -0.0040,  0.0138, -0.2197, -0.0885,  0.0642, -0.1862,\n",
       "           0.0479, -0.0161,  0.3176, -0.0682, -0.0319,  0.0443, -0.1102, -0.1969,\n",
       "          -0.0563, -0.0458, -0.1757, -0.0595, -0.0581, -0.0889,  0.0873, -0.0571,\n",
       "          -0.1488, -0.0804, -0.0731, -0.0341,  0.0085,  0.0179,  0.0293, -0.3137,\n",
       "          -0.0289, -0.1711,  0.0282, -0.0403, -0.1907, -0.1281, -0.1492,  0.0339,\n",
       "          -0.1257, -0.0937, -0.0114,  0.0190,  0.0096,  0.1352, -0.0256, -0.0704,\n",
       "           0.0102,  0.4012], dtype=torch.float64),\n",
       "  tensor([ 0.1274, -0.0826, -0.5173, -0.1344,  0.0844,  0.0661, -0.1805, -0.0465,\n",
       "          -0.0473,  0.1863,  0.1671, -0.0088, -0.1029,  0.0108,  0.0575,  0.0169,\n",
       "          -0.0293, -0.0452,  0.0302, -0.0484,  0.0117, -0.0131, -0.0753, -0.0431,\n",
       "          -0.0148, -0.0492, -0.0272, -0.1707, -0.0386, -0.0505, -0.0531,  0.0349,\n",
       "           0.0302,  0.0512, -0.2311,  0.0555,  0.3374,  0.0652,  0.0161,  0.3290,\n",
       "          -0.0747, -0.2691,  0.1074,  0.0370, -0.0873, -0.1090, -0.1702],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.6096, -0.1007, -0.1174, -0.0233,  0.1616, -0.1619, -0.1633,  0.0837,\n",
       "           0.0534,  0.1987,  0.0250,  0.2174,  0.0187,  0.0185,  0.0610, -0.0091,\n",
       "           0.3213, -0.0345, -0.0062, -0.0572, -0.1171, -0.1182, -0.1454,  0.0224,\n",
       "          -0.0896, -0.0738,  0.0603, -0.1612,  0.1205, -0.0163, -0.1360,  0.1350,\n",
       "           0.0425, -0.0871,  0.0009, -0.1011,  0.1304, -0.0587,  0.0521, -0.0157,\n",
       "           0.0176,  0.0358,  0.0354,  0.0865], dtype=torch.float64),\n",
       "  tensor([-0.4969,  0.1074, -0.2108,  0.2020, -0.0956, -0.1721,  0.2258,  0.0146,\n",
       "          -0.0609,  0.1606,  0.0571, -0.0105,  0.0815,  0.0562, -0.1531,  0.4672,\n",
       "           0.0685,  0.0832,  0.0798,  0.0068, -0.1553,  0.0716,  0.0079,  0.0721,\n",
       "          -0.0556,  0.1454,  0.0024, -0.2324, -0.0740], dtype=torch.float64),\n",
       "  tensor([ 0.5808,  0.0383, -0.0285,  0.0208,  0.0700,  0.0325,  0.0662,  0.0183,\n",
       "           0.0013,  0.0434,  0.1035,  0.0658, -0.0124,  0.0921,  0.3903,  0.3483,\n",
       "           0.1945,  0.0080,  0.0783, -0.0881, -0.0765, -0.0644, -0.0329, -0.0291,\n",
       "          -0.0417,  0.0263,  0.0032,  0.0036,  0.0014, -0.0613,  0.0141,  0.0682,\n",
       "           0.0714, -0.0074,  0.0085, -0.0753, -0.0271,  0.0070,  0.0275, -0.0686,\n",
       "          -0.0059,  0.0460, -0.0101, -0.0107, -0.0394,  0.0534, -0.0700, -0.0290,\n",
       "           0.0762, -0.0384,  0.0021, -0.0520,  0.4374], dtype=torch.float64),\n",
       "  tensor([ 0.1311,  0.9117, -0.1416,  0.0170, -0.0611, -0.0367,  0.0091,  0.0231,\n",
       "           0.0189, -0.0191,  0.0139,  0.0125,  0.0534,  0.0281, -0.0183,  0.1119,\n",
       "           0.1659,  0.0831,  0.0371,  0.0059,  0.0419,  0.0065, -0.0370, -0.0196,\n",
       "           0.0747, -0.0152, -0.0103,  0.0211, -0.0387, -0.0282,  0.0149,  0.0284,\n",
       "           0.0100,  0.0095,  0.0157, -0.0196,  0.0078, -0.0016, -0.0041, -0.0158,\n",
       "          -0.0027, -0.0760, -0.0107, -0.0341, -0.0185,  0.0087, -0.0168,  0.0673],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 0.2406, -0.4784,  0.1780, -0.1842, -0.0702,  0.0872, -0.2531,  0.0178,\n",
       "          -0.1301, -0.0940,  0.0543, -0.1392, -0.1274, -0.3146, -0.0064,  0.1265,\n",
       "           0.0112,  0.0517, -0.0512, -0.1502, -0.0853,  0.1382,  0.0907, -0.2451,\n",
       "           0.1050], dtype=torch.float64),\n",
       "  tensor([ 0.1960,  0.1059, -0.0494, -0.0909, -0.1049,  0.0178, -0.0302,  0.5796,\n",
       "           0.2992,  0.2050, -0.0379,  0.0086, -0.0779, -0.0386, -0.0127, -0.0864,\n",
       "          -0.0838, -0.1851,  0.0106, -0.0522, -0.0109, -0.0284, -0.0719, -0.0655,\n",
       "           0.0411, -0.0056, -0.0240, -0.0439,  0.0098, -0.0382,  0.0169, -0.0207,\n",
       "           0.0190,  0.2372,  0.0303, -0.0323,  0.0552,  0.0435, -0.0379,  0.0124,\n",
       "           0.1060, -0.0008,  0.0501,  0.1192,  0.0808,  0.0580, -0.0148,  0.0125,\n",
       "           0.0950,  0.0816,  0.0907,  0.0768, -0.0453, -0.0107,  0.1680, -0.1486,\n",
       "          -0.0997, -0.2783], dtype=torch.float64),\n",
       "  tensor([ 0.2388, -0.0168,  0.0892,  0.0121, -0.0096,  0.0012,  0.0153,  0.0010,\n",
       "          -0.0256, -0.0473,  0.3994,  0.3457, -0.0312, -0.0487,  0.0239, -0.0022,\n",
       "          -0.1700, -0.0721, -0.0476, -0.0088,  0.0249, -0.0149, -0.0287,  0.0037,\n",
       "           0.0036,  0.0014, -0.0324, -0.0162, -0.0145, -0.0453,  0.0038, -0.0039,\n",
       "          -0.0097, -0.0138,  0.0058, -0.0022,  0.0253,  0.0215, -0.0209, -0.0465,\n",
       "          -0.0157,  0.0009,  0.0213,  0.0114,  0.0464,  0.0029,  0.0012,  0.0110,\n",
       "          -0.0094,  0.0161, -0.0170, -0.0315,  0.0075, -0.0580, -0.0127, -0.0022,\n",
       "          -0.0429, -0.0161,  0.0068,  0.0320,  0.1072,  0.1154,  0.0772,  0.0808,\n",
       "           0.0030, -0.0290, -0.0518, -0.0824, -0.0594, -0.5863,  0.3274],\n",
       "         dtype=torch.float64),\n",
       "  tensor([-1.9666e-01,  5.4284e-02, -7.3028e-02,  3.8330e-02, -2.3856e-01,\n",
       "          -4.5005e-01, -2.5867e-02,  1.2932e-02,  3.0038e-01,  1.8569e-01,\n",
       "          -1.0384e-01,  3.2095e-02,  8.0662e-03, -9.1042e-03, -2.5309e-02,\n",
       "          -4.2670e-03, -5.1384e-02,  1.5334e-01, -2.5176e-02,  5.8094e-03,\n",
       "          -2.8800e-02,  3.6207e-03, -2.6859e-01, -5.5939e-01, -2.3831e-02,\n",
       "           4.6567e-03,  3.9597e-02, -1.0057e-01,  4.3463e-02,  8.7109e-02,\n",
       "           8.9489e-03,  2.0144e-02, -7.6734e-03, -3.2716e-03,  5.6003e-03,\n",
       "           6.1915e-03,  1.9688e-02,  3.3464e-02, -1.4878e-03, -4.8736e-03,\n",
       "           4.5401e-02, -5.4579e-03,  2.6805e-02,  4.4317e-02,  1.7308e-02,\n",
       "           3.1914e-02,  2.8417e-03,  6.7044e-02,  9.2866e-02,  1.0313e-02,\n",
       "           9.5227e-03,  1.1631e-01,  3.2150e-02,  4.3423e-03, -4.8414e-03,\n",
       "           6.2234e-03,  2.5264e-02,  1.0370e-02, -4.1331e-02, -1.2642e-03,\n",
       "          -1.1641e-02, -7.6675e-04, -1.4864e-02, -5.4153e-02, -1.4707e-02,\n",
       "          -2.8597e-02, -1.5473e-02, -1.0251e-03, -1.6544e-02,  7.0087e-02,\n",
       "           2.5232e-02,  6.3341e-03,  3.6389e-03,  5.3946e-02,  3.5439e-02,\n",
       "           5.5205e-02,  2.5468e-02,  6.0091e-02, -8.1128e-03,  3.2909e-02,\n",
       "           4.4268e-02,  1.4788e-02,  3.7645e-04,  2.4470e-02,  1.6463e-01],\n",
       "         dtype=torch.float64),\n",
       "  tensor([ 1.7350e-01, -4.0283e-02,  1.5873e-02, -1.3070e-02, -1.6931e-03,\n",
       "          -2.7646e-02,  1.8703e-02,  2.0046e-02,  7.1348e-03, -1.7924e-01,\n",
       "           3.4843e-02, -1.8558e-02,  3.0540e-02,  7.3845e-02, -1.3895e-01,\n",
       "           4.1419e-01,  2.4256e-01, -1.6625e-02, -7.9738e-03, -1.7185e-02,\n",
       "           6.7262e-03, -9.1931e-03, -2.5789e-02, -1.3112e-02,  5.2038e-02,\n",
       "          -4.1392e-01,  1.0434e-02, -2.6314e-01,  1.4887e-02, -2.4840e-02,\n",
       "          -3.2332e-02,  1.7180e-02, -7.2408e-02, -1.9981e-02, -1.3476e-01,\n",
       "          -1.5211e-02,  1.9098e-02, -2.2527e-02, -1.0570e-02,  6.6367e-02,\n",
       "           1.7130e-04, -8.7008e-03, -1.8345e-02, -4.7829e-03, -9.9327e-03,\n",
       "           7.3042e-02,  1.5057e-02, -3.0578e-02, -9.6778e-03, -1.6275e-03,\n",
       "          -1.3784e-02,  1.3279e-02, -1.4001e-02, -7.8544e-03,  4.4184e-04,\n",
       "          -4.5492e-02, -2.5071e-02, -6.5384e-03, -1.5642e-02, -1.8110e-01,\n",
       "           3.3448e-02, -1.9177e-02, -1.3403e-01, -1.2968e-01, -2.0604e-02,\n",
       "          -1.5956e-01, -1.2555e-02,  5.8807e-02,  2.3921e-01,  9.7573e-02,\n",
       "           6.7479e-02,  2.1172e-02,  3.0560e-04,  9.7021e-02],\n",
       "         dtype=torch.float64)],\n",
       " 'raw_input_list': [['[CLS]',\n",
       "   'what',\n",
       "   'would',\n",
       "   'a',\n",
       "   'teacher',\n",
       "   'assess',\n",
       "   'the',\n",
       "   'levels',\n",
       "   'of',\n",
       "   'a',\n",
       "   'student',\n",
       "   'on?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'for',\n",
       "   'example,',\n",
       "   'an',\n",
       "   'experienced',\n",
       "   'teacher',\n",
       "   'and',\n",
       "   'parent',\n",
       "   'described',\n",
       "   'the',\n",
       "   'place',\n",
       "   'of',\n",
       "   'a',\n",
       "   'teacher',\n",
       "   'in',\n",
       "   'learning',\n",
       "   'as',\n",
       "   'follows:',\n",
       "   '\"the',\n",
       "   'real',\n",
       "   'bulk',\n",
       "   'of',\n",
       "   'learning',\n",
       "   'takes',\n",
       "   'place',\n",
       "   'in',\n",
       "   'self-study',\n",
       "   'and',\n",
       "   'problem',\n",
       "   'solving',\n",
       "   'with',\n",
       "   'a',\n",
       "   'lot',\n",
       "   'of',\n",
       "   'feedback',\n",
       "   'around',\n",
       "   'that',\n",
       "   'loop.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'company',\n",
       "   'created',\n",
       "   'doctor',\n",
       "   'who?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'who',\n",
       "   'character',\n",
       "   'by',\n",
       "   'bbc',\n",
       "   'television',\n",
       "   'in',\n",
       "   'the',\n",
       "   'early',\n",
       "   '1960s,',\n",
       "   'a',\n",
       "   'myriad',\n",
       "   'of',\n",
       "   'stories',\n",
       "   'have',\n",
       "   'been',\n",
       "   'published',\n",
       "   'about',\n",
       "   'doctor',\n",
       "   'who,',\n",
       "   'in',\n",
       "   'different',\n",
       "   'media:',\n",
       "   'apart',\n",
       "   'from',\n",
       "   'the',\n",
       "   'actual',\n",
       "   'television',\n",
       "   'episodes',\n",
       "   'that',\n",
       "   'continue',\n",
       "   'to',\n",
       "   'be',\n",
       "   'produced',\n",
       "   'by',\n",
       "   'the',\n",
       "   'bbc,',\n",
       "   'there',\n",
       "   'have',\n",
       "   'also',\n",
       "   'been',\n",
       "   'novels,',\n",
       "   'comics,',\n",
       "   'short',\n",
       "   'stories,',\n",
       "   'audio',\n",
       "   'books,',\n",
       "   'radio',\n",
       "   'plays,',\n",
       "   'interactive',\n",
       "   'video',\n",
       "   'games,',\n",
       "   'game',\n",
       "   'books,',\n",
       "   'webcasts,',\n",
       "   'dvd',\n",
       "   'extras,',\n",
       "   'and',\n",
       "   'even',\n",
       "   'stage',\n",
       "   'performances.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'was',\n",
       "   'the',\n",
       "   'name',\n",
       "   'of',\n",
       "   'the',\n",
       "   'media',\n",
       "   'day',\n",
       "   'event',\n",
       "   'for',\n",
       "   'super',\n",
       "   'bowl',\n",
       "   '50?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   \"game's\",\n",
       "   'media',\n",
       "   'day,',\n",
       "   'which',\n",
       "   'was',\n",
       "   'typically',\n",
       "   'held',\n",
       "   'on',\n",
       "   'the',\n",
       "   'tuesday',\n",
       "   'afternoon',\n",
       "   'prior',\n",
       "   'to',\n",
       "   'the',\n",
       "   'game,',\n",
       "   'was',\n",
       "   'moved',\n",
       "   'to',\n",
       "   'the',\n",
       "   'monday',\n",
       "   'evening',\n",
       "   'and',\n",
       "   're-branded',\n",
       "   'as',\n",
       "   'super',\n",
       "   'bowl',\n",
       "   'opening',\n",
       "   'night.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'how',\n",
       "   'many',\n",
       "   'doctor',\n",
       "   'who',\n",
       "   'soundtracks',\n",
       "   'have',\n",
       "   'been',\n",
       "   'released',\n",
       "   'since',\n",
       "   '2005?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   'fourth',\n",
       "   'was',\n",
       "   'released',\n",
       "   'on',\n",
       "   '4',\n",
       "   'october',\n",
       "   '2010',\n",
       "   'as',\n",
       "   'a',\n",
       "   'two',\n",
       "   'disc',\n",
       "   'special',\n",
       "   'edition',\n",
       "   'and',\n",
       "   'contained',\n",
       "   'music',\n",
       "   'from',\n",
       "   'the',\n",
       "   '20082010',\n",
       "   'specials',\n",
       "   '(the',\n",
       "   'next',\n",
       "   'doctor',\n",
       "   'to',\n",
       "   'end',\n",
       "   'of',\n",
       "   'time',\n",
       "   'part',\n",
       "   '2).',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'is',\n",
       "   'the',\n",
       "   'name',\n",
       "   'of',\n",
       "   'the',\n",
       "   \"country's\",\n",
       "   'longest',\n",
       "   'continuously',\n",
       "   'running',\n",
       "   'student',\n",
       "   'film',\n",
       "   'society?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'students',\n",
       "   'at',\n",
       "   'the',\n",
       "   'university',\n",
       "   'of',\n",
       "   'chicago',\n",
       "   'run',\n",
       "   'over',\n",
       "   '400',\n",
       "   'clubs',\n",
       "   'and',\n",
       "   'organizations',\n",
       "   'known',\n",
       "   'as',\n",
       "   'recognized',\n",
       "   'student',\n",
       "   'organizations',\n",
       "   '(rsos).',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'how',\n",
       "   'many',\n",
       "   'times',\n",
       "   'has',\n",
       "   'the',\n",
       "   'south',\n",
       "   'florida/miami',\n",
       "   'area',\n",
       "   'hosted',\n",
       "   'the',\n",
       "   'super',\n",
       "   'bowl?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   'south',\n",
       "   'florida/miami',\n",
       "   'area',\n",
       "   'has',\n",
       "   'previously',\n",
       "   'hosted',\n",
       "   'the',\n",
       "   'event',\n",
       "   '10',\n",
       "   'times',\n",
       "   '(tied',\n",
       "   'for',\n",
       "   'most',\n",
       "   'with',\n",
       "   'new',\n",
       "   'orleans),',\n",
       "   'with',\n",
       "   'the',\n",
       "   'most',\n",
       "   'recent',\n",
       "   'one',\n",
       "   'being',\n",
       "   'super',\n",
       "   'bowl',\n",
       "   'xliv',\n",
       "   'in',\n",
       "   '2010.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'is',\n",
       "   'different',\n",
       "   'about',\n",
       "   'paulinella',\n",
       "   'chromatophora?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'it',\n",
       "   'is',\n",
       "   'not',\n",
       "   'clear',\n",
       "   'whether',\n",
       "   'that',\n",
       "   'symbiont',\n",
       "   'is',\n",
       "   'closely',\n",
       "   'related',\n",
       "   'to',\n",
       "   'the',\n",
       "   'ancestral',\n",
       "   'chloroplast',\n",
       "   'of',\n",
       "   'other',\n",
       "   'eukaryotes.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'played',\n",
       "   'doctor',\n",
       "   'who',\n",
       "   'on',\n",
       "   'stage',\n",
       "   'in',\n",
       "   'the',\n",
       "   \"70's?\",\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'doctor',\n",
       "   'who',\n",
       "   'has',\n",
       "   'appeared',\n",
       "   'on',\n",
       "   'stage',\n",
       "   'numerous',\n",
       "   'times.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'do',\n",
       "   'clinical',\n",
       "   'pharmacists',\n",
       "   'work',\n",
       "   'with',\n",
       "   'much',\n",
       "   'of',\n",
       "   'the',\n",
       "   'time?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'clinical',\n",
       "   'pharmacists',\n",
       "   'often',\n",
       "   'collaborate',\n",
       "   'with',\n",
       "   'physicians',\n",
       "   'and',\n",
       "   'other',\n",
       "   'healthcare',\n",
       "   'professionals',\n",
       "   'to',\n",
       "   'improve',\n",
       "   'pharmaceutical',\n",
       "   'care.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'in',\n",
       "   'which',\n",
       "   'county',\n",
       "   'does',\n",
       "   'jacksonville',\n",
       "   'reside?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'it',\n",
       "   'is',\n",
       "   'the',\n",
       "   'county',\n",
       "   'seat',\n",
       "   'of',\n",
       "   'duval',\n",
       "   'county,',\n",
       "   'with',\n",
       "   'which',\n",
       "   'the',\n",
       "   'city',\n",
       "   'government',\n",
       "   'consolidated',\n",
       "   'in',\n",
       "   '1968.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'did',\n",
       "   'genghis',\n",
       "   'khan',\n",
       "   'charge',\n",
       "   'with',\n",
       "   'finding',\n",
       "   'and',\n",
       "   'punishing',\n",
       "   'the',\n",
       "   'shah?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'genghis',\n",
       "   'khan',\n",
       "   'ordered',\n",
       "   'the',\n",
       "   'wholesale',\n",
       "   'massacre',\n",
       "   'of',\n",
       "   'many',\n",
       "   'of',\n",
       "   'the',\n",
       "   'civilians,',\n",
       "   'enslaved',\n",
       "   'the',\n",
       "   'rest',\n",
       "   'of',\n",
       "   'the',\n",
       "   'population',\n",
       "   'and',\n",
       "   'executed',\n",
       "   'inalchuq',\n",
       "   'by',\n",
       "   'pouring',\n",
       "   'molten',\n",
       "   'silver',\n",
       "   'into',\n",
       "   'his',\n",
       "   'ears',\n",
       "   'and',\n",
       "   'eyes,',\n",
       "   'as',\n",
       "   'retribution',\n",
       "   'for',\n",
       "   'his',\n",
       "   'actions.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'entity',\n",
       "   'enforces',\n",
       "   'the',\n",
       "   'charter',\n",
       "   'of',\n",
       "   'fundamental',\n",
       "   'rights',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'union?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   'effect,',\n",
       "   'after',\n",
       "   'the',\n",
       "   'lisbon',\n",
       "   'treaty,',\n",
       "   'the',\n",
       "   'charter',\n",
       "   'and',\n",
       "   'the',\n",
       "   'convention',\n",
       "   'now',\n",
       "   'co-exist',\n",
       "   'under',\n",
       "   'european',\n",
       "   'union',\n",
       "   'law,',\n",
       "   'though',\n",
       "   'the',\n",
       "   'former',\n",
       "   'is',\n",
       "   'enforced',\n",
       "   'by',\n",
       "   'the',\n",
       "   'european',\n",
       "   'court',\n",
       "   'of',\n",
       "   'justice',\n",
       "   'in',\n",
       "   'relation',\n",
       "   'to',\n",
       "   'european',\n",
       "   'union',\n",
       "   'measures,',\n",
       "   'and',\n",
       "   'the',\n",
       "   'latter',\n",
       "   'by',\n",
       "   'the',\n",
       "   'european',\n",
       "   'court',\n",
       "   'of',\n",
       "   'human',\n",
       "   'rights',\n",
       "   'in',\n",
       "   'relation',\n",
       "   'to',\n",
       "   'measures',\n",
       "   'by',\n",
       "   'member',\n",
       "   'states.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'most',\n",
       "   'of',\n",
       "   'the',\n",
       "   \"museum's\",\n",
       "   'collection',\n",
       "   'had',\n",
       "   'been',\n",
       "   'returned',\n",
       "   'by',\n",
       "   'which',\n",
       "   'year?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'before',\n",
       "   'the',\n",
       "   'return',\n",
       "   'of',\n",
       "   'the',\n",
       "   'collections',\n",
       "   'after',\n",
       "   'the',\n",
       "   'war,',\n",
       "   'the',\n",
       "   'britain',\n",
       "   'can',\n",
       "   'make',\n",
       "   'it',\n",
       "   'exhibition',\n",
       "   'was',\n",
       "   'held',\n",
       "   'between',\n",
       "   'september',\n",
       "   'and',\n",
       "   'november',\n",
       "   '1946,',\n",
       "   'attracting',\n",
       "   'nearly',\n",
       "   'a',\n",
       "   'million',\n",
       "   'and',\n",
       "   'a',\n",
       "   'half',\n",
       "   'visitors.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'within',\n",
       "   'the',\n",
       "   '30',\n",
       "   'days',\n",
       "   'how',\n",
       "   'many',\n",
       "   'digiboxes',\n",
       "   'had',\n",
       "   'been',\n",
       "   'sold?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'within',\n",
       "   '30',\n",
       "   'days,',\n",
       "   'over',\n",
       "   '100,000',\n",
       "   'digiboxes',\n",
       "   'had',\n",
       "   'been',\n",
       "   'sold,',\n",
       "   'which',\n",
       "   'help',\n",
       "   'bolstered',\n",
       "   \"bskyb's\",\n",
       "   'decision',\n",
       "   'to',\n",
       "   'give',\n",
       "   'away',\n",
       "   'free',\n",
       "   'digiboxes',\n",
       "   'and',\n",
       "   'minidishes',\n",
       "   'from',\n",
       "   'may',\n",
       "   '1999.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'the',\n",
       "   'receptors',\n",
       "   'on',\n",
       "   'a',\n",
       "   'killer',\n",
       "   't',\n",
       "   'cell',\n",
       "   'must',\n",
       "   'bind',\n",
       "   'to',\n",
       "   'how',\n",
       "   'many',\n",
       "   'mhc:',\n",
       "   'antigen',\n",
       "   'complexes',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'activate',\n",
       "   'the',\n",
       "   'cell?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   'mhc:antigen',\n",
       "   'complex',\n",
       "   'is',\n",
       "   'also',\n",
       "   'recognized',\n",
       "   'by',\n",
       "   'the',\n",
       "   'helper',\n",
       "   \"cell's\",\n",
       "   'cd4',\n",
       "   'co-receptor,',\n",
       "   'which',\n",
       "   'recruits',\n",
       "   'molecules',\n",
       "   'inside',\n",
       "   'the',\n",
       "   't',\n",
       "   'cell',\n",
       "   '(e.g.,',\n",
       "   'lck)',\n",
       "   'that',\n",
       "   'are',\n",
       "   'responsible',\n",
       "   'for',\n",
       "   'the',\n",
       "   't',\n",
       "   \"cell's\",\n",
       "   'activation.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'how',\n",
       "   'much',\n",
       "   'did',\n",
       "   'westinghouse',\n",
       "   'pay',\n",
       "   'for',\n",
       "   \"tesla's\",\n",
       "   'designs?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   'july',\n",
       "   '1888,',\n",
       "   'brown',\n",
       "   'and',\n",
       "   'peck',\n",
       "   'negotiated',\n",
       "   'a',\n",
       "   'licensing',\n",
       "   'deal',\n",
       "   'with',\n",
       "   'george',\n",
       "   'westinghouse',\n",
       "   'for',\n",
       "   \"tesla's\",\n",
       "   'polyphase',\n",
       "   'induction',\n",
       "   'motor',\n",
       "   'and',\n",
       "   'transformer',\n",
       "   'designs',\n",
       "   'for',\n",
       "   '$60,000',\n",
       "   'in',\n",
       "   'cash',\n",
       "   'and',\n",
       "   'stock',\n",
       "   'and',\n",
       "   'a',\n",
       "   'royalty',\n",
       "   'of',\n",
       "   '$2.50',\n",
       "   'per',\n",
       "   'ac',\n",
       "   'horsepower',\n",
       "   'produced',\n",
       "   'by',\n",
       "   'each',\n",
       "   'motor.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'type',\n",
       "   'of',\n",
       "   'movies',\n",
       "   'were',\n",
       "   'produced',\n",
       "   'in',\n",
       "   \"jacksonville's\",\n",
       "   '30',\n",
       "   'studios?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'one',\n",
       "   'converted',\n",
       "   'movie',\n",
       "   'studio',\n",
       "   'site,',\n",
       "   'norman',\n",
       "   'studios,',\n",
       "   'remains',\n",
       "   'in',\n",
       "   'arlington;',\n",
       "   'it',\n",
       "   'has',\n",
       "   'been',\n",
       "   'converted',\n",
       "   'to',\n",
       "   'the',\n",
       "   'jacksonville',\n",
       "   'silent',\n",
       "   'film',\n",
       "   'museum',\n",
       "   'at',\n",
       "   'norman',\n",
       "   'studios.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'voyager',\n",
       "   'said',\n",
       "   'that',\n",
       "   'mombasa',\n",
       "   'was',\n",
       "   'a',\n",
       "   'great',\n",
       "   'harbour',\n",
       "   'and',\n",
       "   'moored',\n",
       "   'small',\n",
       "   'crafts',\n",
       "   'and',\n",
       "   'great',\n",
       "   'ships?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   'swahili',\n",
       "   'built',\n",
       "   'mombasa',\n",
       "   'into',\n",
       "   'a',\n",
       "   'major',\n",
       "   'port',\n",
       "   'city',\n",
       "   'and',\n",
       "   'established',\n",
       "   'trade',\n",
       "   'links',\n",
       "   'with',\n",
       "   'other',\n",
       "   'nearby',\n",
       "   'city-states,',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'commercial',\n",
       "   'centres',\n",
       "   'in',\n",
       "   'persia,',\n",
       "   'arabia,',\n",
       "   'and',\n",
       "   'even',\n",
       "   'india.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'where',\n",
       "   'is',\n",
       "   'energiprojekt',\n",
       "   'ab',\n",
       "   'based?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'although',\n",
       "   'the',\n",
       "   'reciprocating',\n",
       "   'steam',\n",
       "   'engine',\n",
       "   'is',\n",
       "   'no',\n",
       "   'longer',\n",
       "   'in',\n",
       "   'widespread',\n",
       "   'commercial',\n",
       "   'use,',\n",
       "   'various',\n",
       "   'companies',\n",
       "   'are',\n",
       "   'exploring',\n",
       "   'or',\n",
       "   'exploiting',\n",
       "   'the',\n",
       "   'potential',\n",
       "   'of',\n",
       "   'the',\n",
       "   'engine',\n",
       "   'as',\n",
       "   'an',\n",
       "   'alternative',\n",
       "   'to',\n",
       "   'internal',\n",
       "   'combustion',\n",
       "   'engines.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'other',\n",
       "   'than',\n",
       "   'tesla',\n",
       "   'did',\n",
       "   'westinghouse',\n",
       "   'consider',\n",
       "   'for',\n",
       "   'the',\n",
       "   'patents?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'westinghouse',\n",
       "   'looked',\n",
       "   'into',\n",
       "   'getting',\n",
       "   'a',\n",
       "   'patent',\n",
       "   'on',\n",
       "   'a',\n",
       "   'similar',\n",
       "   'commutator-less,',\n",
       "   'rotating',\n",
       "   'magnetic',\n",
       "   'field-based',\n",
       "   'induction',\n",
       "   'motor',\n",
       "   'presented',\n",
       "   'in',\n",
       "   'a',\n",
       "   'paper',\n",
       "   'in',\n",
       "   'march',\n",
       "   '1888',\n",
       "   'by',\n",
       "   'the',\n",
       "   'italian',\n",
       "   'physicist',\n",
       "   'galileo',\n",
       "   'ferraris,',\n",
       "   'but',\n",
       "   'decided',\n",
       "   \"tesla's\",\n",
       "   'patent',\n",
       "   'would',\n",
       "   'probably',\n",
       "   'control',\n",
       "   'the',\n",
       "   'market.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'separates',\n",
       "   'the',\n",
       "   'neuroimmune',\n",
       "   'system',\n",
       "   'and',\n",
       "   'peripheral',\n",
       "   'immune',\n",
       "   'system',\n",
       "   'in',\n",
       "   'humans?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   'humans,',\n",
       "   'the',\n",
       "   'bloodbrain',\n",
       "   'barrier,',\n",
       "   'bloodcerebrospinal',\n",
       "   'fluid',\n",
       "   'barrier,',\n",
       "   'and',\n",
       "   'similar',\n",
       "   'fluidbrain',\n",
       "   'barriers',\n",
       "   'separate',\n",
       "   'the',\n",
       "   'peripheral',\n",
       "   'immune',\n",
       "   'system',\n",
       "   'from',\n",
       "   'the',\n",
       "   'neuroimmune',\n",
       "   'system',\n",
       "   'which',\n",
       "   'protects',\n",
       "   'the',\n",
       "   'brain.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'did',\n",
       "   \"kublai's\",\n",
       "   'government',\n",
       "   'have',\n",
       "   'to',\n",
       "   'balance',\n",
       "   'between?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   \"kublai's\",\n",
       "   'government',\n",
       "   'after',\n",
       "   '1262',\n",
       "   'was',\n",
       "   'a',\n",
       "   'compromise',\n",
       "   'between',\n",
       "   'preserving',\n",
       "   'mongol',\n",
       "   'interests',\n",
       "   'in',\n",
       "   'china',\n",
       "   'and',\n",
       "   'satisfying',\n",
       "   'the',\n",
       "   'demands',\n",
       "   'of',\n",
       "   'his',\n",
       "   'chinese',\n",
       "   'subjects.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'did',\n",
       "   \"gasquet's\",\n",
       "   'book',\n",
       "   'blame',\n",
       "   'the',\n",
       "   'plague',\n",
       "   'on?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   'historian',\n",
       "   'francis',\n",
       "   'aidan',\n",
       "   'gasquet',\n",
       "   'wrote',\n",
       "   'about',\n",
       "   'the',\n",
       "   \"'great\",\n",
       "   \"pestilence'\",\n",
       "   'in',\n",
       "   '1893',\n",
       "   'and',\n",
       "   'suggested',\n",
       "   'that',\n",
       "   '\"it',\n",
       "   'would',\n",
       "   'appear',\n",
       "   'to',\n",
       "   'be',\n",
       "   'some',\n",
       "   'form',\n",
       "   'of',\n",
       "   'the',\n",
       "   'ordinary',\n",
       "   'eastern',\n",
       "   'or',\n",
       "   'bubonic',\n",
       "   'plague\".',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'shared',\n",
       "   'sideline',\n",
       "   'duties',\n",
       "   'with',\n",
       "   'evan',\n",
       "   'washburn?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   'the',\n",
       "   'united',\n",
       "   'states,',\n",
       "   'the',\n",
       "   'game',\n",
       "   'was',\n",
       "   'televised',\n",
       "   'by',\n",
       "   'cbs,',\n",
       "   'as',\n",
       "   'part',\n",
       "   'of',\n",
       "   'a',\n",
       "   'cycle',\n",
       "   'between',\n",
       "   'the',\n",
       "   'three',\n",
       "   'main',\n",
       "   'broadcast',\n",
       "   'television',\n",
       "   'partners',\n",
       "   'of',\n",
       "   'the',\n",
       "   'nfl.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'was',\n",
       "   'added',\n",
       "   'to',\n",
       "   'party',\n",
       "   'as',\n",
       "   'washington',\n",
       "   'went',\n",
       "   'on',\n",
       "   'the',\n",
       "   'way?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'washington',\n",
       "   'left',\n",
       "   'with',\n",
       "   'a',\n",
       "   'small',\n",
       "   'party,',\n",
       "   'picking',\n",
       "   'up',\n",
       "   'along',\n",
       "   'the',\n",
       "   'way',\n",
       "   'jacob',\n",
       "   'van',\n",
       "   'braam',\n",
       "   'as',\n",
       "   'an',\n",
       "   'interpreter;',\n",
       "   'christopher',\n",
       "   'gist,',\n",
       "   'a',\n",
       "   'company',\n",
       "   'surveyor',\n",
       "   'working',\n",
       "   'in',\n",
       "   'the',\n",
       "   'area;',\n",
       "   'and',\n",
       "   'a',\n",
       "   'few',\n",
       "   'mingo',\n",
       "   'led',\n",
       "   'by',\n",
       "   'tanaghrisson.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'did',\n",
       "   'queen',\n",
       "   'elizabeth',\n",
       "   'ii',\n",
       "   'open',\n",
       "   'in',\n",
       "   'newcastle',\n",
       "   'in',\n",
       "   '1981?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'it',\n",
       "   'was',\n",
       "   'opened',\n",
       "   'in',\n",
       "   'five',\n",
       "   'phases',\n",
       "   'between',\n",
       "   '1980',\n",
       "   'and',\n",
       "   '1984,',\n",
       "   'and',\n",
       "   'was',\n",
       "   \"britain's\",\n",
       "   'first',\n",
       "   'urban',\n",
       "   'light',\n",
       "   'rail',\n",
       "   'transit',\n",
       "   'system;',\n",
       "   'two',\n",
       "   'extensions',\n",
       "   'were',\n",
       "   'opened',\n",
       "   'in',\n",
       "   '1991',\n",
       "   'and',\n",
       "   '2002.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'writing',\n",
       "   'inspired',\n",
       "   'the',\n",
       "   'name',\n",
       "   'great',\n",
       "   'yuan?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'furthermore,',\n",
       "   'the',\n",
       "   'yuan',\n",
       "   'is',\n",
       "   'sometimes',\n",
       "   'known',\n",
       "   'as',\n",
       "   'the',\n",
       "   '\"empire',\n",
       "   'of',\n",
       "   'the',\n",
       "   'great',\n",
       "   'khan\"',\n",
       "   'or',\n",
       "   '\"khanate',\n",
       "   'of',\n",
       "   'the',\n",
       "   'great',\n",
       "   'khan\",',\n",
       "   'which',\n",
       "   'particularly',\n",
       "   'appeared',\n",
       "   'on',\n",
       "   'some',\n",
       "   'yuan',\n",
       "   'maps,',\n",
       "   'since',\n",
       "   'yuan',\n",
       "   'emperors',\n",
       "   'held',\n",
       "   'the',\n",
       "   'nominal',\n",
       "   'title',\n",
       "   'of',\n",
       "   'great',\n",
       "   'khan.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'happened',\n",
       "   'to',\n",
       "   'the',\n",
       "   'east',\n",
       "   'india',\n",
       "   'trading',\n",
       "   'company',\n",
       "   'in',\n",
       "   '1767?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   '1599',\n",
       "   'the',\n",
       "   'british',\n",
       "   'east',\n",
       "   'india',\n",
       "   'company',\n",
       "   'was',\n",
       "   'established',\n",
       "   'and',\n",
       "   'was',\n",
       "   'chartered',\n",
       "   'by',\n",
       "   'queen',\n",
       "   'elizabeth',\n",
       "   'in',\n",
       "   'the',\n",
       "   'following',\n",
       "   'year.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'the',\n",
       "   'principle',\n",
       "   'of',\n",
       "   'faunal',\n",
       "   'succession',\n",
       "   'was',\n",
       "   'developed',\n",
       "   '100',\n",
       "   'years',\n",
       "   'before',\n",
       "   'whose',\n",
       "   'theory',\n",
       "   'of',\n",
       "   'evolution?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'based',\n",
       "   'on',\n",
       "   'principles',\n",
       "   'laid',\n",
       "   'out',\n",
       "   'by',\n",
       "   'william',\n",
       "   'smith',\n",
       "   'almost',\n",
       "   'a',\n",
       "   'hundred',\n",
       "   'years',\n",
       "   'before',\n",
       "   'the',\n",
       "   'publication',\n",
       "   'of',\n",
       "   'charles',\n",
       "   \"darwin's\",\n",
       "   'theory',\n",
       "   'of',\n",
       "   'evolution,',\n",
       "   'the',\n",
       "   'principles',\n",
       "   'of',\n",
       "   'succession',\n",
       "   'were',\n",
       "   'developed',\n",
       "   'independently',\n",
       "   'of',\n",
       "   'evolutionary',\n",
       "   'thought.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'how',\n",
       "   'many',\n",
       "   'times',\n",
       "   'did',\n",
       "   'luther',\n",
       "   'preach',\n",
       "   'in',\n",
       "   'halle',\n",
       "   'in',\n",
       "   '1545',\n",
       "   'and',\n",
       "   '1546?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   '1545',\n",
       "   'and',\n",
       "   '1546',\n",
       "   'luther',\n",
       "   'preached',\n",
       "   'three',\n",
       "   'times',\n",
       "   'in',\n",
       "   'the',\n",
       "   'market',\n",
       "   'church',\n",
       "   'in',\n",
       "   'halle,',\n",
       "   'staying',\n",
       "   'with',\n",
       "   'his',\n",
       "   'friend',\n",
       "   'justus',\n",
       "   'jonas',\n",
       "   'during',\n",
       "   'christmas.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'part',\n",
       "   'of',\n",
       "   'the',\n",
       "   'rhine',\n",
       "   'flows',\n",
       "   'through',\n",
       "   'north',\n",
       "   'rhine-westphalia?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'here',\n",
       "   'the',\n",
       "   'rhine',\n",
       "   'flows',\n",
       "   'through',\n",
       "   'the',\n",
       "   'largest',\n",
       "   'conurbation',\n",
       "   'in',\n",
       "   'germany,',\n",
       "   'the',\n",
       "   'rhine-ruhr',\n",
       "   'region.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'is',\n",
       "   'the',\n",
       "   'most',\n",
       "   'important',\n",
       "   'thing',\n",
       "   'apicoplasts',\n",
       "   'do?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   'most',\n",
       "   'important',\n",
       "   'apicoplast',\n",
       "   'function',\n",
       "   'is',\n",
       "   'isopentenyl',\n",
       "   'pyrophosphate',\n",
       "   'synthesisin',\n",
       "   'fact,',\n",
       "   'apicomplexans',\n",
       "   'die',\n",
       "   'when',\n",
       "   'something',\n",
       "   'interferes',\n",
       "   'with',\n",
       "   'this',\n",
       "   'apicoplast',\n",
       "   'function,',\n",
       "   'and',\n",
       "   'when',\n",
       "   'apicomplexans',\n",
       "   'are',\n",
       "   'grown',\n",
       "   'in',\n",
       "   'an',\n",
       "   'isopentenyl',\n",
       "   'pyrophosphate-rich',\n",
       "   'medium,',\n",
       "   'they',\n",
       "   'dump',\n",
       "   'the',\n",
       "   'organelle.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'when',\n",
       "   'did',\n",
       "   'abc',\n",
       "   'begin',\n",
       "   'airing',\n",
       "   'dick',\n",
       "   \"clark's\",\n",
       "   'new',\n",
       "   \"year's\",\n",
       "   \"rockin'\",\n",
       "   'eve?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'since',\n",
       "   '1974,',\n",
       "   'abc',\n",
       "   'has',\n",
       "   'generally',\n",
       "   'aired',\n",
       "   'dick',\n",
       "   \"clark's\",\n",
       "   'new',\n",
       "   \"year's\",\n",
       "   \"rockin'\",\n",
       "   'eve',\n",
       "   'on',\n",
       "   'new',\n",
       "   \"year's\",\n",
       "   'eve',\n",
       "   '(hosted',\n",
       "   'first',\n",
       "   'by',\n",
       "   'its',\n",
       "   'creator',\n",
       "   'dick',\n",
       "   'clark,',\n",
       "   'and',\n",
       "   'later',\n",
       "   'by',\n",
       "   'his',\n",
       "   'successor',\n",
       "   'ryan',\n",
       "   'seacrest);',\n",
       "   'the',\n",
       "   'only',\n",
       "   'exception',\n",
       "   'was',\n",
       "   'in',\n",
       "   '1999,',\n",
       "   'when',\n",
       "   'abc',\n",
       "   'put',\n",
       "   'it',\n",
       "   'on',\n",
       "   'a',\n",
       "   'one-year',\n",
       "   'hiatus',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'coverage',\n",
       "   'of',\n",
       "   'the',\n",
       "   'international',\n",
       "   'millennium',\n",
       "   'festivities,',\n",
       "   'though',\n",
       "   \"clark's\",\n",
       "   'traditional',\n",
       "   'countdown',\n",
       "   'from',\n",
       "   'times',\n",
       "   'square',\n",
       "   'was',\n",
       "   'still',\n",
       "   'featured',\n",
       "   'within',\n",
       "   'the',\n",
       "   'coverage.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'the',\n",
       "   'kronenberg',\n",
       "   'palace',\n",
       "   'had',\n",
       "   'been',\n",
       "   'an',\n",
       "   'exceptional',\n",
       "   'example',\n",
       "   'of',\n",
       "   'what',\n",
       "   'type',\n",
       "   'of',\n",
       "   'architecture?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'despite',\n",
       "   'that',\n",
       "   'the',\n",
       "   'warsaw',\n",
       "   'university',\n",
       "   'of',\n",
       "   'technology',\n",
       "   'building',\n",
       "   '(18991902)',\n",
       "   'is',\n",
       "   'the',\n",
       "   'most',\n",
       "   'interesting',\n",
       "   'of',\n",
       "   'the',\n",
       "   'late',\n",
       "   '19th-century',\n",
       "   'architecture.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'was',\n",
       "   'the',\n",
       "   'definition',\n",
       "   'of',\n",
       "   'professionals,',\n",
       "   'for',\n",
       "   'this',\n",
       "   'study?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'it',\n",
       "   'is',\n",
       "   'important',\n",
       "   'to',\n",
       "   'note,',\n",
       "   'however,',\n",
       "   'that',\n",
       "   'the',\n",
       "   'british',\n",
       "   'study',\n",
       "   'referenced',\n",
       "   'above',\n",
       "   'is',\n",
       "   'the',\n",
       "   'only',\n",
       "   'one',\n",
       "   'of',\n",
       "   'its',\n",
       "   'kind',\n",
       "   'and',\n",
       "   'consisted',\n",
       "   'of',\n",
       "   '\"a',\n",
       "   'random',\n",
       "   '...',\n",
       "   'probability',\n",
       "   'sample',\n",
       "   'of',\n",
       "   '2,869',\n",
       "   'young',\n",
       "   'people',\n",
       "   'between',\n",
       "   'the',\n",
       "   'ages',\n",
       "   'of',\n",
       "   '18',\n",
       "   'and',\n",
       "   '24',\n",
       "   'in',\n",
       "   'a',\n",
       "   'computer-assisted',\n",
       "   'study\"',\n",
       "   'and',\n",
       "   'that',\n",
       "   'the',\n",
       "   'questions',\n",
       "   'referred',\n",
       "   'to',\n",
       "   '\"sexual',\n",
       "   'abuse',\n",
       "   'with',\n",
       "   'a',\n",
       "   'professional,\"',\n",
       "   'not',\n",
       "   'necessarily',\n",
       "   'a',\n",
       "   'teacher.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'bassett',\n",
       "   'focuses',\n",
       "   'on',\n",
       "   'what',\n",
       "   'to',\n",
       "   'illustrate',\n",
       "   'his',\n",
       "   'idea?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'to',\n",
       "   'better',\n",
       "   'illustrate',\n",
       "   'this',\n",
       "   'idea,',\n",
       "   'bassett',\n",
       "   'focuses',\n",
       "   'his',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'the',\n",
       "   'role',\n",
       "   'of',\n",
       "   'nineteenth-century',\n",
       "   'maps',\n",
       "   'during',\n",
       "   'the',\n",
       "   '\"scramble',\n",
       "   'for',\n",
       "   'africa\".',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'where',\n",
       "   'did',\n",
       "   'tesla',\n",
       "   'believe',\n",
       "   'his',\n",
       "   'talents',\n",
       "   'came',\n",
       "   'from?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   \"tesla's\",\n",
       "   'mother,',\n",
       "   'uka',\n",
       "   'tesla',\n",
       "   '(nee',\n",
       "   'mandic),',\n",
       "   'whose',\n",
       "   'father',\n",
       "   'was',\n",
       "   'also',\n",
       "   'an',\n",
       "   'orthodox',\n",
       "   'priest,:10',\n",
       "   'had',\n",
       "   'a',\n",
       "   'talent',\n",
       "   'for',\n",
       "   'making',\n",
       "   'home',\n",
       "   'craft',\n",
       "   'tools,',\n",
       "   'mechanical',\n",
       "   'appliances,',\n",
       "   'and',\n",
       "   'the',\n",
       "   'ability',\n",
       "   'to',\n",
       "   'memorize',\n",
       "   'serbian',\n",
       "   'epic',\n",
       "   'poems.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'was',\n",
       "   'given',\n",
       "   'the',\n",
       "   'esteemed',\n",
       "   'status',\n",
       "   'of',\n",
       "   'mvp',\n",
       "   'for',\n",
       "   'super',\n",
       "   'bowl',\n",
       "   '50?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'denver',\n",
       "   'linebacker',\n",
       "   'von',\n",
       "   'miller',\n",
       "   'was',\n",
       "   'named',\n",
       "   'super',\n",
       "   'bowl',\n",
       "   'mvp,',\n",
       "   'recording',\n",
       "   'five',\n",
       "   'solo',\n",
       "   'tackles,',\n",
       "   '2',\n",
       "   'sacks,',\n",
       "   'and',\n",
       "   'two',\n",
       "   'forced',\n",
       "   'fumbles.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'was',\n",
       "   'the',\n",
       "   'percentage',\n",
       "   'of',\n",
       "   'black',\n",
       "   'or',\n",
       "   'african-americans',\n",
       "   'living',\n",
       "   'in',\n",
       "   'the',\n",
       "   'city?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'hispanic',\n",
       "   'or',\n",
       "   'latino',\n",
       "   'of',\n",
       "   'any',\n",
       "   'race',\n",
       "   'were',\n",
       "   '39.9%',\n",
       "   'of',\n",
       "   'the',\n",
       "   'population.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'was',\n",
       "   'the',\n",
       "   'result',\n",
       "   'of',\n",
       "   'the',\n",
       "   '2007',\n",
       "   'election?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'with',\n",
       "   'international',\n",
       "   'criminal',\n",
       "   'court',\n",
       "   'trial',\n",
       "   'dates',\n",
       "   'in',\n",
       "   '2013',\n",
       "   'for',\n",
       "   'both',\n",
       "   'president',\n",
       "   'kenyatta',\n",
       "   'and',\n",
       "   'deputy',\n",
       "   'president',\n",
       "   'william',\n",
       "   'ruto',\n",
       "   'related',\n",
       "   'to',\n",
       "   'the',\n",
       "   '2007',\n",
       "   'election',\n",
       "   'aftermath,',\n",
       "   'us',\n",
       "   'president',\n",
       "   'barack',\n",
       "   'obama',\n",
       "   'chose',\n",
       "   'not',\n",
       "   'to',\n",
       "   'visit',\n",
       "   'the',\n",
       "   'country',\n",
       "   'during',\n",
       "   'his',\n",
       "   'mid-2013',\n",
       "   'african',\n",
       "   'trip.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'equation',\n",
       "   'currently',\n",
       "   'decribes',\n",
       "   'the',\n",
       "   'physics',\n",
       "   'of',\n",
       "   'force.',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'the',\n",
       "   'notion',\n",
       "   '\"force\"',\n",
       "   'keeps',\n",
       "   'its',\n",
       "   'meaning',\n",
       "   'in',\n",
       "   'quantum',\n",
       "   'mechanics,',\n",
       "   'though',\n",
       "   'one',\n",
       "   'is',\n",
       "   'now',\n",
       "   'dealing',\n",
       "   'with',\n",
       "   'operators',\n",
       "   'instead',\n",
       "   'of',\n",
       "   'classical',\n",
       "   'variables',\n",
       "   'and',\n",
       "   'though',\n",
       "   'the',\n",
       "   'physics',\n",
       "   'is',\n",
       "   'now',\n",
       "   'described',\n",
       "   'by',\n",
       "   'the',\n",
       "   'schrodinger',\n",
       "   'equation',\n",
       "   'instead',\n",
       "   'of',\n",
       "   'newtonian',\n",
       "   'equations.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'did',\n",
       "   'iqbal',\n",
       "   'fear',\n",
       "   'would',\n",
       "   'weaken',\n",
       "   'the',\n",
       "   'spiritual',\n",
       "   'foundations',\n",
       "   'of',\n",
       "   'islam',\n",
       "   'and',\n",
       "   'muslim',\n",
       "   'society?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   'his',\n",
       "   'travels',\n",
       "   'to',\n",
       "   'egypt,',\n",
       "   'afghanistan,',\n",
       "   'palestine',\n",
       "   'and',\n",
       "   'syria,',\n",
       "   'he',\n",
       "   'promoted',\n",
       "   'ideas',\n",
       "   'of',\n",
       "   'greater',\n",
       "   'islamic',\n",
       "   'political',\n",
       "   'co-operation',\n",
       "   'and',\n",
       "   'unity,',\n",
       "   'calling',\n",
       "   'for',\n",
       "   'the',\n",
       "   'shedding',\n",
       "   'of',\n",
       "   'nationalist',\n",
       "   'differences.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'in',\n",
       "   'what',\n",
       "   'meeting',\n",
       "   'did',\n",
       "   'shirley',\n",
       "   'lay',\n",
       "   'out',\n",
       "   'plans',\n",
       "   'for',\n",
       "   '1756?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'at',\n",
       "   'a',\n",
       "   'meeting',\n",
       "   'in',\n",
       "   'albany',\n",
       "   'in',\n",
       "   'december',\n",
       "   '1755,',\n",
       "   'he',\n",
       "   'laid',\n",
       "   'out',\n",
       "   'his',\n",
       "   'plans',\n",
       "   'for',\n",
       "   '1756.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'in',\n",
       "   'autoimmune',\n",
       "   'disorders,',\n",
       "   'the',\n",
       "   'immune',\n",
       "   'system',\n",
       "   \"doesn't\",\n",
       "   'distinguish',\n",
       "   'between',\n",
       "   'what',\n",
       "   'types',\n",
       "   'of',\n",
       "   'cells?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'functions',\n",
       "   'of',\n",
       "   'specialized',\n",
       "   'cells',\n",
       "   '(located',\n",
       "   'in',\n",
       "   'the',\n",
       "   'thymus',\n",
       "   'and',\n",
       "   'bone',\n",
       "   'marrow)',\n",
       "   'is',\n",
       "   'to',\n",
       "   'present',\n",
       "   'young',\n",
       "   'lymphocytes',\n",
       "   'with',\n",
       "   'self',\n",
       "   'antigens',\n",
       "   'produced',\n",
       "   'throughout',\n",
       "   'the',\n",
       "   'body',\n",
       "   'and',\n",
       "   'to',\n",
       "   'eliminate',\n",
       "   'those',\n",
       "   'cells',\n",
       "   'that',\n",
       "   'recognize',\n",
       "   'self-antigens,',\n",
       "   'preventing',\n",
       "   'autoimmunity.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'which',\n",
       "   'husband',\n",
       "   'and',\n",
       "   'wife',\n",
       "   'modern',\n",
       "   'furniture',\n",
       "   'design',\n",
       "   'team',\n",
       "   'are',\n",
       "   'represented',\n",
       "   'in',\n",
       "   'the',\n",
       "   'v&a',\n",
       "   'furniture',\n",
       "   'collection?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'finest',\n",
       "   'pieces',\n",
       "   'of',\n",
       "   'continental',\n",
       "   'furniture',\n",
       "   'in',\n",
       "   'the',\n",
       "   'collection',\n",
       "   'is',\n",
       "   'the',\n",
       "   'rococo',\n",
       "   'augustus',\n",
       "   'rex',\n",
       "   'bureau',\n",
       "   'cabinet',\n",
       "   'dated',\n",
       "   'c1750',\n",
       "   'from',\n",
       "   'germany,',\n",
       "   'with',\n",
       "   'especially',\n",
       "   'fine',\n",
       "   'marquetry',\n",
       "   'and',\n",
       "   'ormolu',\n",
       "   'mounts.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'why',\n",
       "   'did',\n",
       "   'oil',\n",
       "   'start',\n",
       "   'getting',\n",
       "   'priced',\n",
       "   'in',\n",
       "   'terms',\n",
       "   'of',\n",
       "   'gold?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'because',\n",
       "   'oil',\n",
       "   'was',\n",
       "   'priced',\n",
       "   'in',\n",
       "   'dollars,',\n",
       "   'oil',\n",
       "   \"producers'\",\n",
       "   'real',\n",
       "   'income',\n",
       "   'decreased.',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'who',\n",
       "   'did',\n",
       "   'britain',\n",
       "   'exploit',\n",
       "   'in',\n",
       "   'india?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'although',\n",
       "   'a',\n",
       "   'substantial',\n",
       "   'number',\n",
       "   'of',\n",
       "   'colonies',\n",
       "   'had',\n",
       "   'been',\n",
       "   'designed',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'economic',\n",
       "   'profit',\n",
       "   'and',\n",
       "   'to',\n",
       "   'ship',\n",
       "   'resources',\n",
       "   'to',\n",
       "   'home',\n",
       "   'ports',\n",
       "   'in',\n",
       "   'the',\n",
       "   'seventeenth',\n",
       "   'and',\n",
       "   'eighteenth',\n",
       "   'centuries,',\n",
       "   'fieldhouse',\n",
       "   'suggests',\n",
       "   'that',\n",
       "   'in',\n",
       "   'the',\n",
       "   'nineteenth',\n",
       "   'and',\n",
       "   'twentieth',\n",
       "   'centuries',\n",
       "   'in',\n",
       "   'places',\n",
       "   'such',\n",
       "   'as',\n",
       "   'africa',\n",
       "   'and',\n",
       "   'asia,',\n",
       "   'this',\n",
       "   'idea',\n",
       "   'is',\n",
       "   'not',\n",
       "   'necessarily',\n",
       "   'valid:',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'popular',\n",
       "   'environmentalist',\n",
       "   'is',\n",
       "   'also',\n",
       "   'a',\n",
       "   'university',\n",
       "   'alumni',\n",
       "   'member?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'in',\n",
       "   'science,',\n",
       "   'alumni',\n",
       "   'include',\n",
       "   'astronomers',\n",
       "   'carl',\n",
       "   'sagan,',\n",
       "   'a',\n",
       "   'prominent',\n",
       "   'contributor',\n",
       "   'to',\n",
       "   'the',\n",
       "   'scientific',\n",
       "   'research',\n",
       "   'of',\n",
       "   'extraterrestrial',\n",
       "   'life,',\n",
       "   'and',\n",
       "   'edwin',\n",
       "   'hubble,',\n",
       "   'known',\n",
       "   'for',\n",
       "   '\"hubble\\'s',\n",
       "   'law\",',\n",
       "   'nasa',\n",
       "   'astronaut',\n",
       "   'john',\n",
       "   'm.',\n",
       "   'grunsfeld,',\n",
       "   'geneticist',\n",
       "   'james',\n",
       "   'watson,',\n",
       "   'best',\n",
       "   'known',\n",
       "   'as',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'co-discoverers',\n",
       "   'of',\n",
       "   'the',\n",
       "   'structure',\n",
       "   'of',\n",
       "   'dna,',\n",
       "   'experimental',\n",
       "   'physicist',\n",
       "   'luis',\n",
       "   'alvarez,',\n",
       "   'popular',\n",
       "   'environmentalist',\n",
       "   'david',\n",
       "   'suzuki,',\n",
       "   'balloonist',\n",
       "   'jeannette',\n",
       "   'piccard,',\n",
       "   'biologists',\n",
       "   'ernest',\n",
       "   'everett',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'what',\n",
       "   'publication',\n",
       "   'did',\n",
       "   'philip',\n",
       "   'howard',\n",
       "   'work',\n",
       "   'for?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'responding',\n",
       "   'to',\n",
       "   'the',\n",
       "   'findings',\n",
       "   'of',\n",
       "   'the',\n",
       "   'survey',\n",
       "   'in',\n",
       "   'the',\n",
       "   'times',\n",
       "   'newspaper,',\n",
       "   'journalist',\n",
       "   'philip',\n",
       "   'howard',\n",
       "   'maintained',\n",
       "   'that,',\n",
       "   '\"to',\n",
       "   'compare',\n",
       "   'the',\n",
       "   'violence',\n",
       "   'of',\n",
       "   'dr',\n",
       "   'who,',\n",
       "   'sired',\n",
       "   'by',\n",
       "   'a',\n",
       "   'horse-laugh',\n",
       "   'out',\n",
       "   'of',\n",
       "   'a',\n",
       "   'nightmare,',\n",
       "   'with',\n",
       "   'the',\n",
       "   'more',\n",
       "   'realistic',\n",
       "   'violence',\n",
       "   'of',\n",
       "   'other',\n",
       "   'television',\n",
       "   'series,',\n",
       "   'where',\n",
       "   'actors',\n",
       "   'who',\n",
       "   'look',\n",
       "   'like',\n",
       "   'human',\n",
       "   'beings',\n",
       "   'bleed',\n",
       "   'paint',\n",
       "   'that',\n",
       "   'looks',\n",
       "   'like',\n",
       "   'blood,',\n",
       "   'is',\n",
       "   'like',\n",
       "   'comparing',\n",
       "   'monopoly',\n",
       "   'with',\n",
       "   'the',\n",
       "   'property',\n",
       "   'market',\n",
       "   'in',\n",
       "   'london:',\n",
       "   'both',\n",
       "   'are',\n",
       "   'fantasies,',\n",
       "   'but',\n",
       "   'one',\n",
       "   'is',\n",
       "   'meant',\n",
       "   'to',\n",
       "   'be',\n",
       "   'taken',\n",
       "   'seriously.\"',\n",
       "   '[SEP]'],\n",
       "  ['[CLS]',\n",
       "   'if',\n",
       "   'the',\n",
       "   'apparant',\n",
       "   'force',\n",
       "   'of',\n",
       "   'two',\n",
       "   'fermions',\n",
       "   'is',\n",
       "   'attractive,',\n",
       "   'what',\n",
       "   'is',\n",
       "   'the',\n",
       "   'spin',\n",
       "   'function?',\n",
       "   '[SEP]',\n",
       "   '[SEP]',\n",
       "   'if',\n",
       "   'two',\n",
       "   'identical',\n",
       "   'fermions',\n",
       "   '(e.g.',\n",
       "   'electrons)',\n",
       "   'have',\n",
       "   'a',\n",
       "   'symmetric',\n",
       "   'spin',\n",
       "   'function',\n",
       "   '(e.g.',\n",
       "   'parallel',\n",
       "   'spins)',\n",
       "   'the',\n",
       "   'spatial',\n",
       "   'variables',\n",
       "   'must',\n",
       "   'be',\n",
       "   'antisymmetric',\n",
       "   '(i.e.',\n",
       "   'they',\n",
       "   'exclude',\n",
       "   'each',\n",
       "   'other',\n",
       "   'from',\n",
       "   'their',\n",
       "   'places',\n",
       "   'much',\n",
       "   'as',\n",
       "   'if',\n",
       "   'there',\n",
       "   'was',\n",
       "   'a',\n",
       "   'repulsive',\n",
       "   'force),',\n",
       "   'and',\n",
       "   'vice',\n",
       "   'versa,',\n",
       "   'i.e.',\n",
       "   'for',\n",
       "   'antiparallel',\n",
       "   'spins',\n",
       "   'the',\n",
       "   'position',\n",
       "   'variables',\n",
       "   'must',\n",
       "   'be',\n",
       "   'symmetric',\n",
       "   '(i.e.',\n",
       "   'the',\n",
       "   'apparent',\n",
       "   'force',\n",
       "   'must',\n",
       "   'be',\n",
       "   'attractive).',\n",
       "   '[SEP]']]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_info(idxs, qnli_data_raw, targets, model_out_list, raw_attr_list, conti_attr_list, raw_input_list, \n",
    "          fname=f'../MethodOutputs/{file_name_base}_out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db9de1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_HTML(cur_file_name=f'{file_name_base}_QNLI_BERT.ipynb',\n",
    "              out_file_name=f'{file_name_base}_QNLI_BERT.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
