{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733ed20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58deb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    try:\n",
    "        return log_loss(y_true, y_pred, labels=[0,1])\n",
    "    except ValueError:\n",
    "        print(f'y_true {y_true}, {len(y_true)}')\n",
    "        print(f'y_pred {y_pred}, {len(y_pred)}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fcc493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(l1, l2):  #expected two lists of words or two sets of words\n",
    "    l1, l2 = set(l1), set(l2)\n",
    "    intersection = l1.intersection(l2)\n",
    "    union = l1.union(l2)\n",
    "    try:\n",
    "        return float(len(intersection)) / len(union)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9307b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_out_and_results_files(frame_name):\n",
    "    frame_out_name = f'{frame_name}_out.pkl'\n",
    "    frame_res_name =  f'{frame_name}_results.csv'\n",
    "    frame_out = {}\n",
    "    with open(f'{frame_out_name}', 'rb') as f:\n",
    "        frame_out = pickle.load(f)\n",
    "    frame_res = pd.read_csv(f'{frame_res_name}')\n",
    "#     print(f'{frame_res.columns}')\n",
    "    return frame_out, frame_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292850cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process frame_out's information to be used by calc score\n",
    "def get_organized_frame_out(frame_out):\n",
    "    #get features (words)\n",
    "    organized = {'feats_pos': [], 'feats_neg': [], 'N_Chunks': []}\n",
    "    for i, (ri, ca) in enumerate(zip(frame_out['raw_input_list'], frame_out['conti_attr_list'])):\n",
    "        #subout beginnining <s> and end </s> tokens for ['BEGIN'] and ['END']\n",
    "        raw_input_i = ['[BEGIN]'if f == '<s>' else '[END]' if f == '</s>' else f for f in ri]\n",
    "        attr_appearance_cutoff = 5e-2\n",
    "        ca = ca.to(torch.float32)\n",
    "        \n",
    "        #filtering out by zeroing non-appearing features\n",
    "        ca_i = torch.where(torch.abs(ca) < attr_appearance_cutoff, torch.zeros(1), ca) \n",
    "        \n",
    "        #get positive and negative features\n",
    "        ca_i_pos = torch.where(ca_i > 0, ca_i, torch.zeros(1))\n",
    "        ca_i_neg = torch.where(ca_i < 0, ca_i, torch.zeros(1))\n",
    "        \n",
    "        try:\n",
    "            #get idx of pos/neg identified feature\n",
    "            ca_i_pos_idx = torch.nonzero(ca_i_pos).squeeze().numpy()\n",
    "            ca_i_neg_idx = torch.nonzero(ca_i_neg).squeeze().numpy() \n",
    "            #don't account for empty ''s or empty arrays\n",
    "            features_pos = [raw_input_i[idx] for idx in ca_i_pos_idx if ri[idx] != ''] \n",
    "            features_neg = [raw_input_i[idx] for idx in ca_i_neg_idx if ri[idx] != ''] \n",
    "        except TypeError: #TypeError: iteration over a 0-d array\n",
    "            #         print(f'i: {i}')\n",
    "            #         print(f'{ri}')\n",
    "            #         print(f'i: {raw_input_i}')\n",
    "            #         print(f'{ca}')\n",
    "            #         print(f'{ca_i}')\n",
    "            #         print(f'{ca_i_idx}')\n",
    "            #         print(f'features frame {features_frame}')\n",
    "            #         print(f'N_Chunks {N_Chunks}')\n",
    "            #         print(organized)\n",
    "            features_pos = []\n",
    "            features_neg = []\n",
    "            \n",
    "\n",
    "        \n",
    "            \n",
    "        organized['feats_pos'].append(features_pos)\n",
    "        organized['feats_neg'].append(features_neg)\n",
    "        \n",
    "        N_cs = len(features_pos) + len(features_neg)\n",
    "        organized['N_Chunks'].append(N_cs)\n",
    "        \n",
    "#      \n",
    "    frame_out.update(organized)\n",
    "    return frame_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3418be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process frame_res to be used by calc_score\n",
    "def get_organized_frame_res(frame_res):\n",
    "    organized = {'yh': [], 'non_neg': [], 'non_pos': [], 'should_neg': [], 'should_pos': [], 'trust_numbers': []}\n",
    "    for x in frame_res.groupby('Input.qnli_number'):  # edit this groupby thing for different tasks\n",
    "        task_answers = x[1]['Answer.taskAnswers']\n",
    "        entailments, trusts = [], []\n",
    "        non_red_ins, non_green_ins, fiat_red_ins, fiat_green_ins = [], [], [], []\n",
    "        for answer_string in task_answers:  # edit this portion below to adapt\n",
    "            json_obj = json.loads(answer_string)[0]\n",
    "#             print(f'json obj {json_obj}')\n",
    "            entailment = 1 if json_obj['entailment_radio']['1'] else 0\n",
    "            entailments.append(entailment)\n",
    "            trust = json_obj['trust_number'] if 'trust_number' in json_obj else 0         \n",
    "            trusts.append(trust)\n",
    "            \n",
    "            if 'non_red_in' in json_obj.keys():\n",
    "                non_red_ins.append([k.strip() for k in json_obj['non_red_in'].split(',')])\n",
    "            else:\n",
    "                non_red_ins.append([])\n",
    "\n",
    "            if 'non_green_in' in json_obj.keys():\n",
    "                non_green_ins.append([k.strip() for k in json_obj['non_green_in'].split(',')])\n",
    "            else:\n",
    "                non_green_ins.append([])\n",
    "\n",
    "            if 'fiat_red_in' in json_obj.keys():\n",
    "                fiat_red_ins.append([k.strip() for k in json_obj['fiat_red_in'].split(',')])\n",
    "            else:\n",
    "                fiat_red_ins.append([])\n",
    "\n",
    "            if 'fiat_green_in' in json_obj.keys():\n",
    "                fiat_green_ins.append([k.strip() for k in json_obj['fiat_green_in'].split(',')])\n",
    "            else:\n",
    "                fiat_green_ins.append([])\n",
    "\n",
    "        organized['yh'].append(entailments)\n",
    "        organized['non_neg'].append(non_red_ins)\n",
    "        organized['non_pos'].append(non_green_ins)\n",
    "        organized['should_neg'].append(fiat_red_ins)\n",
    "        organized['should_pos'].append(fiat_green_ins)\n",
    "        organized['trust_numbers'].append(trusts)\n",
    "        \n",
    "    # see agreement rate of turks:\n",
    "    # assume 3 annotators\n",
    "        \n",
    "    annotation_triples = []\n",
    "    for i, y_res in enumerate(organized['yh'], start=1):\n",
    "        if len(y_res) != 3:\n",
    "            print(f'i:{i}, don\\'t have 3 answers')\n",
    "            continue\n",
    "        a1 = ('a1', str(i), y_res[0])\n",
    "        a2 = ('a2', str(i), y_res[1])\n",
    "        a3 = ('a3', str(i), y_res[2])\n",
    "        annotation_triples.append(a1)\n",
    "        annotation_triples.append(a2)\n",
    "        annotation_triples.append(a3)      \n",
    "    annotation_task = AnnotationTask(annotation_triples)\n",
    "    average_ao = annotation_task.avg_Ao()\n",
    "    return organized, average_ao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f258d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metr1_simulatability(yh, yg, ym, beta_1=1, beta_2=1):    \n",
    "    loss_func = cross_entropy_loss\n",
    "    l_yhyg = loss_func(yg, yh)\n",
    "    l_yhym = loss_func(ym, yh)\n",
    "    denom = beta_1 * l_yhyg + beta_2 * l_yhym + 1\n",
    "    comp1 = (1/denom)\n",
    "    return comp1\n",
    "\n",
    "def metr1_wrapper(org_frame_out, org_frame_results):\n",
    "    metr1s = []\n",
    "    \n",
    "    yhs = org_frame_results['yh']\n",
    "    yms = np.round(org_frame_out['model_out_list'])\n",
    "    ygs = org_frame_out['targets']\n",
    "    yh_0, yh_1, yh_2 = [], [], [] #assume 3 annotaters\n",
    "    for yh in yhs:\n",
    "        if len(yh) != 3:\n",
    "            print(f'len not 3 {yh}')\n",
    "            continue \n",
    "        yh_0.append(yh[0])\n",
    "        yh_1.append(yh[1])\n",
    "        yh_2.append(yh[2])\n",
    "    \n",
    "    annotations = [yh_0, yh_1, yh_2]\n",
    "    for yh in annotations:\n",
    "        metr1s.append(calc_metr1_simulatability(yh, ygs, yms))\n",
    "#     print(np.mean(metr1s), metr1s)\n",
    "    return np.mean(metr1s), metr1s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae6a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metr2_fidelity(feat_f_pos, feat_f_neg, feat_h_pos, feat_h_neg):\n",
    "    jaccard_pos = jaccard_similarity(feat_f_pos, feat_h_pos)\n",
    "    jaccard_neg = jaccard_similarity(feat_f_neg, feat_h_neg)\n",
    "    fidelity = np.mean([jaccard_pos, jaccard_neg])\n",
    "#     print(f'pos jaccard {jaccard_pos}')\n",
    "#     print(f'neg jaccard {jaccard_neg}')\n",
    "#     print(f'fidelity {fidelity}')\n",
    "    return fidelity \n",
    "\n",
    "def metr2_wrapper(org_frame_out, org_frame_results):\n",
    "    metr2s = []\n",
    "    for ff_pos, ff_neg, fh_nn_all, fh_np_all, fh_sn_all, fh_sp_all\\\n",
    "                            in zip(org_frame_out['feats_pos'], \n",
    "                               org_frame_out['feats_neg'], \n",
    "                               org_frame_results['non_neg'],\n",
    "                               org_frame_results['non_pos'],\n",
    "                               org_frame_results['should_neg'],\n",
    "                               org_frame_results['should_pos']):\n",
    "        metr2_annos = []\n",
    "        for fh_nn, fh_np, fh_sn, fh_sp\\\n",
    "                            in zip(fh_nn_all, fh_np_all, fh_sn_all, fh_sp_all):\n",
    "            fh_neg = set(ff_neg).difference(fh_nn).union(fh_sn)\n",
    "            fh_pos = set(ff_pos).difference(fh_nn).union(fh_sp)\n",
    "            \n",
    "            metr2_an = calc_metr2_fidelity(ff_pos, ff_neg, fh_pos, fh_neg)\n",
    "            metr2_annos.append(metr2_an)\n",
    "        metr2s.append(metr2_annos)\n",
    "    metr2s = np.array(metr2s)\n",
    "    metr2s_average = metr2s.mean(axis=1).mean(axis=0)\n",
    "    return metr2s_average, metr2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "354db9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metr3_complexity(N_c):\n",
    "    if N_c == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/(np.log(N_c)+1)\n",
    "\n",
    "def metr3_wrapper(org_frame_out, org_frame_results):\n",
    "    metr3s = []\n",
    "    N_Chunks = org_frame_out['N_Chunks']\n",
    "    for N_c in N_Chunks:\n",
    "        metr3 = calc_metr3_complexity(N_c)\n",
    "        metr3s.append(metr3)\n",
    "    metr3s_average = np.mean(metr3s)\n",
    "#     print(metr3s_average, metr3s)\n",
    "    return metr3s_average, metr3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26997907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_m1m2m3(org_frame_out, org_frame_results):\n",
    "    m1, metric1s = metr1_wrapper(org_frame_out, org_frame_results)\n",
    "    m2, metric2s = metr2_wrapper(org_frame_out, org_frame_results)\n",
    "    m3, metric3s = metr3_wrapper(org_frame_out, org_frame_results)\n",
    "    return (m1, metric1s), (m2, metric2s), (m3, metric3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8c3fd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trust_corr_wrapper(metric1s, metric2s, metric3s, trust_numbers, frame_name):\n",
    "#     print(f'metric 1: {metric1s}')\n",
    "#     print(f'metric 2: {len(metric2s)}')    \n",
    "#     print(f'metric 3: {len(metric3s)}')   \n",
    "    \n",
    "    trust1, trust2, trust3 = [], [], []\n",
    "    for (t1, t2, t3) in trust_numbers:\n",
    "        trust1.append(float(t1))\n",
    "        trust2.append(float(t2))\n",
    "        trust3.append(float(t3))\n",
    "    \n",
    "    trusts = [np.mean([float(t1), float(t2), float(t3)]) for (t1, t2, t3) in trust_numbers]\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    print(f'asdf')\n",
    "#     df['m1'] = metric1s\n",
    "    anno1_score = metric1s[0] + np.mean([k[0] for k in metric2s]) + np.mean(metric3s)\n",
    "    anno2_score = metric1s[1] + np.mean([k[1] for k in metric2s]) + np.mean(metric3s)\n",
    "    anno3_score = metric1s[2] + np.mean([k[2] for k in metric2s]) + np.mean(metric3s)\n",
    "    print(f'anno1_score {anno1_score}')\n",
    "    df['anno1_score'] = anno1_score\n",
    "    print(f'df {df}')\n",
    "#     df['anno2_score'] = anno2_score\n",
    "#     df['anno3_score'] = anno3_score\n",
    "    \n",
    "    print(df.corr())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f4e5244",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def score_wrapper(frame_names=['deeplift', 'guided_backprop', 'input_x_gradients', 'integrated_gradients', 'kernel_shap', 'lime']):\n",
    "    avg_aos = []\n",
    "    m1s, m2s, m3s = [], [], []\n",
    "    for frame_name in frame_names:\n",
    "        print(f'Framework processed: {frame_name}')\n",
    "        frame_out, frame_results = load_out_and_results_files(frame_name)\n",
    "        org_frame_results, avg_ao = get_organized_frame_res(frame_results)\n",
    "        avg_aos.append(avg_ao)\n",
    "        org_frame_out = get_organized_frame_out(frame_out)    \n",
    "        (m1, metric1s), (m2, metric2s), (m3, metric3s) = calc_m1m2m3(org_frame_out, org_frame_results)\n",
    "        m1s.append(m1)\n",
    "        m2s.append(m2)\n",
    "        m3s.append(m3)\n",
    "        print(f'm1: {m1:4f}, m2: {m2:4f}, m3: {m3:4f}')\n",
    "        trust_numbers = org_frame_results['trust_numbers']\n",
    "        trust_corr_wrapper(metric1s, metric2s, metric3s, trust_numbers, frame_name)\n",
    "    print(f'average average agreement {np.mean(avg_aos):.2f}')\n",
    "    return frame_names, m1s, m2s, m3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8436936c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework processed: deeplift\n",
      "m1: 0.057034, m2: 0.859048, m3: 0.235495\n",
      "anno1_score 1.1554967961118152\n",
      "df Empty DataFrame\n",
      "Columns: [Trust Rating, anno1_score]\n",
      "Index: []\n",
      "              Trust Rating  anno1_score\n",
      "Trust Rating           NaN          NaN\n",
      "anno1_score            NaN          NaN\n",
      "Framework processed: guided_backprop\n",
      "m1: 0.054720, m2: 0.866368, m3: 0.228977\n",
      "anno1_score 1.2197431344474752\n",
      "df Empty DataFrame\n",
      "Columns: [Trust Rating, anno1_score]\n",
      "Index: []\n",
      "              Trust Rating  anno1_score\n",
      "Trust Rating           NaN          NaN\n",
      "anno1_score            NaN          NaN\n",
      "Framework processed: input_x_gradients\n",
      "m1: 0.050538, m2: 0.915632, m3: 0.235495\n",
      "anno1_score 1.2636325649696492\n",
      "df Empty DataFrame\n",
      "Columns: [Trust Rating, anno1_score]\n",
      "Index: []\n",
      "              Trust Rating  anno1_score\n",
      "Trust Rating           NaN          NaN\n",
      "anno1_score            NaN          NaN\n",
      "Framework processed: integrated_gradients\n",
      "m1: 0.050538, m2: 0.793053, m3: 0.232914\n",
      "anno1_score 1.1810513287931435\n",
      "df Empty DataFrame\n",
      "Columns: [Trust Rating, anno1_score]\n",
      "Index: []\n",
      "              Trust Rating  anno1_score\n",
      "Trust Rating           NaN          NaN\n",
      "anno1_score            NaN          NaN\n",
      "Framework processed: kernel_shap\n",
      "m1: 0.059342, m2: 0.854397, m3: 0.230393\n",
      "anno1_score 1.2216603332797258\n",
      "df Empty DataFrame\n",
      "Columns: [Trust Rating, anno1_score]\n",
      "Index: []\n",
      "              Trust Rating  anno1_score\n",
      "Trust Rating           NaN          NaN\n",
      "anno1_score            NaN          NaN\n",
      "Framework processed: lime\n",
      "m1: 0.054025, m2: 0.850321, m3: 0.216652\n",
      "anno1_score 1.0589989996727387\n",
      "df Empty DataFrame\n",
      "Columns: [Trust Rating, anno1_score]\n",
      "Index: []\n",
      "              Trust Rating  anno1_score\n",
      "Trust Rating           NaN          NaN\n",
      "anno1_score            NaN          NaN\n",
      "average average agreement 0.65\n"
     ]
    }
   ],
   "source": [
    "frame_names, m1s, m2s, m3s = score_wrapper(['deeplift', 'guided_backprop', 'input_x_gradients', 'integrated_gradients', 'kernel_shap', 'lime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_1_3_alpha(frame_names, m1s, m2s, m3s):\n",
    "    for f, m1, m2, m3 in zip(frame_names, m1s, m2s, m3s):\n",
    "        m1_13 = m1 * (1/3)\n",
    "        m2_13 = m2 * (1/3)\n",
    "        m3_13 = m3 * (1/3)\n",
    "        score_13 = m1_13 + m2_13 + m3_13\n",
    "        print(F\"\\\\textbf{{f}} & {m1_13:.4f} & {m2_13:.4f} & {m3_13:.4f} & {score_13:.4f} \\\\\\\\ {f}\")\n",
    "\n",
    "see_1_3_alpha(frame_names, m1s, m2s, m3s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alpha_combos():\n",
    "    import itertools\n",
    "    alphas = np.arange(0, 11, step=1, dtype=np.uint8)\n",
    "    all_combos = [(a/10,b/10,c/10) for (a,b,c) in itertools.product(alphas, alphas, alphas) if np.sum([a,b,c]) == 10]\n",
    "    all_combos.reverse()\n",
    "    return all_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1646be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_combos = generate_alpha_combos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_alphas(frame_names, m1s, m2s, m3s, alpha_combos):\n",
    "    scores_list = [] #indexed by the alpha combinations\n",
    "    for combo in alpha_combos:\n",
    "        a1, a2, a3 = combo[0], combo[1], combo[2]\n",
    "        scores_for_fs = []\n",
    "        for f, m1, m2, m3 in zip(frame_names, m1s, m2s, m3s):\n",
    "            a1m1, a2m2, a3m3 = a1 * m1, a2 * m2, a3 * m3\n",
    "            score = a1m1 + a2m2 + a3m3\n",
    "            scores_for_fs.append(score)      \n",
    "        scores_list.append([frame_names, scores_for_fs])\n",
    "    return scores_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = apply_alphas(frame_names, m1s, m2s, m3s, alpha_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e56a5b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalize_score_list(scores_list):\n",
    "    normalized_scores_list = []\n",
    "    from sklearn.preprocessing import normalize\n",
    "    def min_max_norm(scores):\n",
    "        norm_scores = (scores-np.min(scores))/(np.max(scores)-np.min(scores))\n",
    "        return norm_scores\n",
    "    \n",
    "    for (frame_names, scores) in scores_list:\n",
    "        scores = np.array(scores)\n",
    "#         normalized_scores = normalize([scores])[0]\n",
    "#         normalized_scores = scores / np.sum(scores)\n",
    "        normalized_scores = min_max_norm(scores)\n",
    "#         print(f'after normalizing: {normalized_scores}\\n-------------------------------------')\n",
    "        normalized_scores_list.append([frame_names, normalized_scores])\n",
    "    return normalized_scores_list\n",
    "\n",
    "# scores_list = normalize_score_list(scores_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d94f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_scores(scores_list, alpha_combos):\n",
    "    frame_names = scores_list[0][0]\n",
    "    frame_scores = {}\n",
    "    for i in range(len(frame_names)):\n",
    "        frame_name = frame_names[i]\n",
    "        frame_scores[frame_name] = [scores_list[j][1][i] for j in range(len(scores_list))]\n",
    "    \n",
    "   \n",
    "    \n",
    "    #sort frames based on average scores\n",
    "    average_scores_dict = {}\n",
    "    for frame, frame_scores_list in frame_scores.items():\n",
    "        print(f'Average score for {frame}: {np.mean(frame_scores_list)}, std: {np.std(frame_scores_list)}')\n",
    "        average_scores_dict[frame] = np.mean(frame_scores_list)\n",
    "    sorted_average_scores_list =[[k, v] for k, v in sorted(average_scores_dict.items(), \n",
    "                                                          key=lambda item: item[1], reverse=True)]\n",
    "    print(f'sorted average scores {sorted_average_scores_list}')\n",
    "\n",
    "    ordered_frame_names = [k for [k,v] in sorted_average_scores_list]\n",
    "    #sort based on top frame\n",
    "    top_frame_name = ordered_frame_names[0]\n",
    "    print(f'top_frame_name {top_frame_name}')\n",
    "    sorted_idxs = np.argsort(frame_scores[top_frame_name])\n",
    "    sorted_frame_scores = {}\n",
    "    for frame in frame_names:\n",
    "        sorted_frame_scores[frame] = np.array(frame_scores[frame])[sorted_idxs]\n",
    "    \n",
    "    permuted_alphas = np.array(np.array(alpha_combos)[sorted_idxs])\n",
    "    a1_idx = np.where(permuted_alphas[:,0]==1)[0][0]\n",
    "    a2_idx = np.where(permuted_alphas[:,1]==1)[0][0]\n",
    "    a3_idx = np.where(permuted_alphas[:,2]==1)[0][0]\n",
    "    \n",
    "    \n",
    "    return sorted_frame_scores, a1_idx, a2_idx, a3_idx, ordered_frame_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_scores, a1_idx, a2_idx, a3_idx, ordered_frame_names = get_frame_scores(scores_list, alpha_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_graph_info(frame_scores, a1_idx, a2_idx, a3_idx, ordered_frame_names, task_name='QNLI'):\n",
    "    out_dict = {\n",
    "        'task_name': task_name,\n",
    "        'frame_scores': frame_scores, \n",
    "        'a1_idx': a1_idx, \n",
    "        'a2_idx': a2_idx,\n",
    "        'a3_idx': a3_idx,\n",
    "        'ordered_frame_names': ordered_frame_names\n",
    "    }\n",
    "    out_file_name = f'{task_name}_graph.pkl'\n",
    "    with open(out_file_name, 'wb') as f:\n",
    "        pickle.dump(out_dict, f)\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_graph_info(frame_scores, a1_idx, a2_idx, a3_idx, ordered_frame_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64460e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef329485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05175f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de230e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
