{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806fb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, pickle, torch\n",
    "sys.path.insert(0, '../../Models')\n",
    "sys.path.insert(0, '../../Utils')\n",
    "sys.path.insert(0, '../../Preprocess')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preload_models import get_sst2_tok_n_model\n",
    "from _utils import sample_random_glue_sst2, get_continuation_mapping, \\\n",
    "                    get_continuous_attributions, get_continuous_raw_inputs, \\\n",
    "                    collect_info_for_metric, save_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd38a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf84a84dc7f4f82b1455d129e9964e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3b24abff24d1d8c0.arrow\n",
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5960909ab3834668.arrow\n"
     ]
    }
   ],
   "source": [
    "sst2_data_raw, targets, idxs = sample_random_glue_sst2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90c47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_sst2_tok_n_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17be71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some containers to save some info\n",
    "model_out_list, raw_attr_list, conti_attr_list, raw_input_list = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d272625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8067da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime = Lime(forward_func=model.forward)\n",
    "IG = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_record(raw_review, target):\n",
    "    #tokenizer operations\n",
    "    tokenized = tokenizer(raw_review, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mapping = tokenized['offset_mapping']\n",
    "    conti_map = get_continuation_mapping(offset_mapping)\n",
    "    input_ids = torch.tensor(tokenized['input_ids']).unsqueeze(0)\n",
    "    detokenized = [t.replace('Ä ', '') for t in tokenizer.convert_ids_to_tokens(input_ids[0])]\n",
    "    \n",
    "    #feeding input forward \n",
    "    input_emb = model.get_embeddings(input_ids)\n",
    "    pred_prob = model(input_emb).item()\n",
    "    \n",
    "    #categorizing results\n",
    "    pred_class = 'Pos' if pred_prob > 0.5 else 'Neg' \n",
    "    true_class = 'Pos' if target > 0.5 else 'Neg' \n",
    "    \n",
    "    #attribution algorithm working\n",
    "    attribution, delta = IG.attribute(input_emb, return_convergence_delta=True)\n",
    "    word_attributions = attribution.squeeze(0).sum(dim=1)\n",
    "    word_attributions /= torch.norm(word_attributions)\n",
    "    attr_score = torch.sum(word_attributions)\n",
    "    attr_class = 'Pos' if attr_score > 0.5 else 'Neg'\n",
    "    convergence_score = delta\n",
    "    \n",
    "    \n",
    "    #re-organizing tensors and arrays because words get split down\n",
    "    conti_attr = get_continuous_attributions(conti_map, word_attributions)\n",
    "    raw_input = get_continuous_raw_inputs(conti_map, detokenized)\n",
    "\n",
    "#     print(f'word attributions {word_attributions}')\n",
    "#     print(f'pred_prob {pred_prob}')\n",
    "#     print(f'pred_class {pred_class}')\n",
    "#     print(f'true_class {true_class}')\n",
    "#     print(f'attribution {attribution}')\n",
    "#     print(f'attr_class {attr_class}')\n",
    "#     print(f'attr_score {attr_score}')\n",
    "#     print(f'raw_input {raw_input}')\n",
    "\n",
    "        \n",
    "#     collect info for metrics later\n",
    "    collect_info_for_metric(model_out_list, pred_prob, raw_attr_list, attribution, conti_attr_list, conti_attr, raw_input_list, raw_input)\n",
    "        \n",
    "    \n",
    "    visual_record = visualization.VisualizationDataRecord(word_attributions=word_attributions,\n",
    "                                                         pred_prob=pred_prob,\n",
    "                                                         pred_class=pred_class,\n",
    "                                                         true_class=true_class,\n",
    "                                                         attr_class=attr_class,\n",
    "                                                         attr_score=attr_score,\n",
    "                                                         raw_input=raw_input,\n",
    "                                                         convergence_score=convergence_score)\n",
    "        \n",
    "        \n",
    "    return visual_record\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efa6df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (datum_raw, target) in enumerate(zip(sst2_data_raw, targets), start=1):\n",
    "    print(f'Raw review: {datum_raw}')\n",
    "    print(f'GT target: {target}')\n",
    "    visual_record=generate_record(datum_raw, target)\n",
    "    print(visualization.visualize_text([visual_record]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6344a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info(idxs, sst2_data_raw, targets, model_out_list, raw_attr_list, conti_attr_list, raw_input_list, fname='deep_lift_out.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
