{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "import torch\n",
    "sys.path.insert(0, '../../Models')\n",
    "sys.path.insert(0, '../../Utils')\n",
    "from _utils import  sample_random_glue_stsb, collect_info_for_metric, save_info, \\\n",
    "                    get_continuation_mapping, get_continuous_attributions, get_continuous_raw_inputs,\\\n",
    "                    attr_normalizing_func\n",
    "from preload_models import get_stsb_tokenizer_n_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "stsb_data_raw, targets, idxs = sample_random_glue_stsb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_stsb_tokenizer_n_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2232ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some containers to save some info\n",
    "model_out_list, raw_attr_list, conti_attr_list, raw_input_list = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Lime\n",
    "from captum._utils.models.linear_model import SkLearnLasso\n",
    "from captum.attr import visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime = Lime(model, interpretable_model=SkLearnLasso(alpha=0.0003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85598a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_record(raw_datum, target): #raw_datum expected to be a tuple/list of 2 sentences\n",
    "    #tokenizer operations\n",
    "    tokenized = tokenizer(raw_datum, truncation=True, return_offsets_mapping=True)\n",
    "    offset_mappings = tokenized['offset_mapping']\n",
    "    #concatenate the two offset_mappings together because they are fed in together\n",
    "    conti_map = get_continuation_mapping(offset_mappings[0]) + get_continuation_mapping(offset_mappings[1])\n",
    "    #change the first input_id of the second sentence to be the last input_id of the 1st sentence (i.e. an [END] token))\n",
    "    tokenized_input_ids = tokenized['input_ids'][0] + \\\n",
    "                        [tokenized['input_ids'][1][i] if i != 0 else tokenized['input_ids'][0][-1] \\\n",
    "                         for i in range(len(tokenized['input_ids'][1]))]\n",
    "    input_ids = torch.tensor(tokenized_input_ids).unsqueeze(0)\n",
    "    detokenized = [t.replace('Ä ', '') for t in tokenizer.convert_ids_to_tokens(input_ids[0])]\n",
    "    \n",
    "    #feeding input forward \n",
    "    input_emb = model.get_embeddings(input_ids)\n",
    "    pred_prob = model(input_emb).item()\n",
    "    print(f'pred_prob {pred_prob*5}')\n",
    "\n",
    "     #categorizing results\n",
    "    pred_class = 'Similar' if pred_prob > 0.5 else 'Not Similar' \n",
    "    true_class = 'Similar' if target > 2.5 else 'Not Similar' \n",
    "    \n",
    "    #attribution algorithm working\n",
    "    attri_start_time = time.time()\n",
    "    attribution = lime.attribute(input_emb, n_samples=5000, show_progress=True, perturbations_per_eval=5000)\n",
    "    print(f'attribution took {time.time() - attri_start_time:2f} seconds')\n",
    "    \n",
    "    attribution[torch.isnan(attribution)] = 0\n",
    "    word_attributions = attribution.squeeze(0).sum(dim=1)\n",
    "#     word_attributions = attr_normalizing_func(word_attributions)\n",
    "    word_attributions /= torch.norm(word_attributions)\n",
    "    attr_score = torch.sum(word_attributions)\n",
    "    attr_class = 'Similar' if attr_score > 0.5 else 'Not Similar'\n",
    "    convergence_score = None\n",
    "    \n",
    "    \n",
    "#     #re-organizing tensors and arrays because words get split down\n",
    "    conti_attr = get_continuous_attributions(conti_map, word_attributions)\n",
    "    raw_input = get_continuous_raw_inputs(conti_map, detokenized)\n",
    "\n",
    "#     print(f'word attributions {word_attributions}')\n",
    "#     print(f'pred_prob {pred_prob}')\n",
    "#     print(f'pred_class {pred_class}')\n",
    "#     print(f'true_class {true_class}')\n",
    "#     print(f'attribution {attribution}')\n",
    "#     print(f'attr_class {attr_class}')\n",
    "#     print(f'attr_score {attr_score}')\n",
    "#     print(f'raw_input {raw_input}')\n",
    "\n",
    "        \n",
    "#   collect info for metrics later\n",
    "    collect_info_for_metric(model_out_list, pred_prob, raw_attr_list, attribution, conti_attr_list, conti_attr, raw_input_list, raw_input)\n",
    "        \n",
    "    \n",
    "    visual_record = visualization.VisualizationDataRecord(word_attributions=conti_attr,\n",
    "                                                         pred_prob=pred_prob,\n",
    "                                                         pred_class=pred_class,\n",
    "                                                         true_class=true_class,\n",
    "                                                         attr_class=attr_class,\n",
    "                                                         attr_score=attr_score,\n",
    "                                                         raw_input=raw_input,\n",
    "                                                         convergence_score=convergence_score)\n",
    "        \n",
    "        \n",
    "    return visual_record\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0321c07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (datum_raw, target) in enumerate(zip(stsb_data_raw, targets), start=1):\n",
    "    example_1 = \"Here are many samples: Pbase.com Out of the 16k photos there, I'm sure some are with a d700.\"\n",
    "    example_2 = \"It's a 1:1 lens, so that means that the size of the subject will be the same size on the sensor.\"\n",
    "    datum_raw, target = [example_1, example_1], 1\n",
    "    print(f'Raw review: {datum_raw}') #datum expected to be a list of 2 sentences\n",
    "    print(f'GT target: {target}')\n",
    "    visual_record=generate_record(datum_raw, target)\n",
    "    print(visualization.visualize_text([visual_record]))   \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7681361",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_info(idxs, stsb_data_raw, targets, model_out_list, raw_attr_list, conti_attr_list, raw_input_list, fname='lime_out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ca5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
