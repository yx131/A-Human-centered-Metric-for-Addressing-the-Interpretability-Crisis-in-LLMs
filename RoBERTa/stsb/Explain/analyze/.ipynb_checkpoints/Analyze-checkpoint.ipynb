{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733ed20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4949403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mi_loss(y_true, y_pred):\n",
    "    return 1 - normalized_mutual_info_score(y_true, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58deb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    try:\n",
    "        return log_loss(y_true, y_pred, labels=[0,1])\n",
    "    except ValueError:\n",
    "        print(f'y_true {y_true}')\n",
    "        print(f'y_pred {y_pred}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fcc493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(l1, l2):  #expected two lists of words or two sets of words\n",
    "    l1, l2 = set(l1), set(l2)\n",
    "    intersection = l1.intersection(l2)\n",
    "    union = l1.union(l2)\n",
    "    try:\n",
    "        return float(len(intersection)) / len(union)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9307b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_out_and_results_files(frame_name):\n",
    "    frame_out_name = f'{frame_name}_out.pkl'\n",
    "    frame_res_name =  f'{frame_name}_results.csv'\n",
    "    frame_out = {}\n",
    "    with open(f'{frame_out_name}', 'rb') as f:\n",
    "        frame_out = pickle.load(f)\n",
    "    frame_res = pd.read_csv(f'{frame_res_name}')\n",
    "#     print(f'{frame_res.columns}')\n",
    "    return frame_out, frame_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292850cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process frame_out's information to be used by calc score\n",
    "def get_organized_frame_out(frame_out):\n",
    "    #get features (words)\n",
    "    organized = {'feats_pos': [], 'feats_neg': [], 'N_Chunks': []}\n",
    "    for i, (ri, ca) in enumerate(zip(frame_out['raw_input_list'], frame_out['conti_attr_list'])):\n",
    "        #subout beginnining <s> and end </s> tokens for ['BEGIN'] and ['END']\n",
    "        raw_input_i = ['[BEGIN]'if f == '<s>' else '[END]' if f == '</s>' else f for f in ri]\n",
    "        attr_appearance_cutoff = 5e-2\n",
    "        ca = ca.to(torch.float32)\n",
    "        \n",
    "        #filtering out by zeroing non-appearing features\n",
    "        ca_i = torch.where(torch.abs(ca) < attr_appearance_cutoff, torch.zeros(1), ca) \n",
    "        \n",
    "        #get positive and negative features\n",
    "        ca_i_pos = torch.where(ca_i > 0, ca_i, torch.zeros(1))\n",
    "        ca_i_neg = torch.where(ca_i < 0, ca_i, torch.zeros(1))\n",
    "        \n",
    "        try:\n",
    "            #get idx of pos/neg identified feature\n",
    "            ca_i_pos_idx = torch.nonzero(ca_i_pos).squeeze().numpy()\n",
    "            ca_i_neg_idx = torch.nonzero(ca_i_neg).squeeze().numpy() \n",
    "            #don't account for empty ''s or empty arrays\n",
    "            features_pos = [raw_input_i[idx] for idx in ca_i_pos_idx if ri[idx] != ''] \n",
    "            features_neg = [raw_input_i[idx] for idx in ca_i_neg_idx if ri[idx] != ''] \n",
    "        except TypeError: #TypeError: iteration over a 0-d array\n",
    "            #         print(f'i: {i}')\n",
    "            #         print(f'{ri}')\n",
    "            #         print(f'i: {raw_input_i}')\n",
    "            #         print(f'{ca}')\n",
    "            #         print(f'{ca_i}')\n",
    "            #         print(f'{ca_i_idx}')\n",
    "            #         print(f'features frame {features_frame}')\n",
    "            #         print(f'N_Chunks {N_Chunks}')\n",
    "            #         print(organized)\n",
    "            features_pos = []\n",
    "            features_neg = []\n",
    "            \n",
    "\n",
    "        \n",
    "            \n",
    "        organized['feats_pos'].append(features_pos)\n",
    "        organized['feats_neg'].append(features_neg)\n",
    "        \n",
    "        N_cs = len(features_pos) + len(features_neg)\n",
    "        organized['N_Chunks'].append(N_cs)\n",
    "        \n",
    "#      \n",
    "    frame_out.update(organized)\n",
    "    return frame_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3418be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process frame_res to be used by calc_score\n",
    "def get_organized_frame_res(frame_res):\n",
    "    organized = {'yh': [], 'non_neg': [], 'non_pos': [], 'should_neg': [], 'should_pos': []}\n",
    "    for x in frame_res.groupby('Input.sst2_number'):  # edit this groupby thing for different tasks\n",
    "        task_answers = x[1]['Answer.taskAnswers']\n",
    "        sentiments = []\n",
    "        non_red_ins, non_green_ins, fiat_red_ins, fiat_green_ins = [], [], [], []\n",
    "        for answer_string in task_answers:  # edit this portion below to adapt\n",
    "            json_obj = json.loads(answer_string)[0]\n",
    "            senti = 1 if json_obj['sentiment_radio']['1'] else 0\n",
    "            sentiments.append(senti)\n",
    "\n",
    "            if 'non_red_in' in json_obj.keys():\n",
    "                non_red_ins.append([k.strip() for k in json_obj['non_red_in'].split(',')])\n",
    "            else:\n",
    "                non_red_ins.append([])\n",
    "\n",
    "            if 'non_green_in' in json_obj.keys():\n",
    "                non_green_ins.append([k.strip() for k in json_obj['non_green_in'].split(',')])\n",
    "            else:\n",
    "                non_green_ins.append([])\n",
    "\n",
    "            if 'fiat_red_in' in json_obj.keys():\n",
    "                fiat_red_ins.append([k.strip() for k in json_obj['fiat_red_in'].split(',')])\n",
    "            else:\n",
    "                fiat_red_ins.append([])\n",
    "\n",
    "            if 'fiat_green_in' in json_obj.keys():\n",
    "                fiat_green_ins.append([k.strip() for k in json_obj['fiat_green_in'].split(',')])\n",
    "            else:\n",
    "                fiat_green_ins.append([])\n",
    "\n",
    "        organized['yh'].append(sentiments)\n",
    "        organized['non_neg'].append(non_red_ins)\n",
    "        organized['non_pos'].append(non_green_ins)\n",
    "        organized['should_neg'].append(fiat_red_ins)\n",
    "        organized['should_pos'].append(fiat_green_ins)\n",
    "\n",
    "    # see agreement rate of turks:\n",
    "    # assume 3 annotators\n",
    "        \n",
    "    annotation_triples = []\n",
    "    for i, senti in enumerate(organized['yh'], start=1):\n",
    "        a1 = ('a1', str(i), senti[0])\n",
    "        a2 = ('a2', str(i), senti[1])\n",
    "        a3 = ('a3', str(i), senti[2])\n",
    "        annotation_triples.append(a1)\n",
    "        annotation_triples.append(a2)\n",
    "        annotation_triples.append(a3)      \n",
    "    annotation_task = AnnotationTask(annotation_triples)\n",
    "    average_ao = annotation_task.avg_Ao()\n",
    "\n",
    "    return organized, average_ao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f258d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metr1_simulatability(yh, yg, ym, beta_1=1, beta_2=1):    \n",
    "    loss_func = cross_entropy_loss\n",
    "#     loss_func = normalized_mi_loss\n",
    "    l_yhyg = loss_func(yh, yg)\n",
    "    l_yhym = loss_func(yh, ym)\n",
    "    \n",
    "    denom = beta_1 * l_yhyg + beta_2 * l_yhym + 1\n",
    "    comp1 = (1/denom)\n",
    "    return comp1\n",
    "\n",
    "def metr1_wrapper(org_frame_out, org_frame_results):\n",
    "    metr1s = []\n",
    "    \n",
    "    yhs = org_frame_results['yh']\n",
    "    yms = np.round(org_frame_out['model_out_list'])\n",
    "    ygs = org_frame_out['targets']\n",
    "\n",
    "    yh_0, yh_1, yh_2 = [], [], [] #assume 3 annotaters\n",
    "    for yh in yhs:\n",
    "        yh_0.append(yh[0])\n",
    "        yh_1.append(yh[1])\n",
    "        yh_2.append(yh[2])\n",
    "    \n",
    "    annotations = [yh_0, yh_1, yh_2]\n",
    "    for yh in annotations:\n",
    "        metr1s.append(calc_metr1_simulatability(yh, ygs, yms))\n",
    "#     print(np.mean(metr1s), metr1s)\n",
    "    return np.mean(metr1s), metr1s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae6a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metr2_fidelity(feat_f_pos, feat_f_neg, feat_h_pos, feat_h_neg):\n",
    "    jaccard_pos = jaccard_similarity(feat_f_pos, feat_h_pos)\n",
    "    jaccard_neg = jaccard_similarity(feat_f_neg, feat_h_neg)\n",
    "    fidelity = np.mean([jaccard_pos, jaccard_neg])\n",
    "#     print(f'pos jaccard {jaccard_pos}')\n",
    "#     print(f'neg jaccard {jaccard_neg}')\n",
    "#     print(f'fidelity {fidelity}')\n",
    "    return fidelity \n",
    "\n",
    "def metr2_wrapper(org_frame_out, org_frame_results):\n",
    "    metr2s = []\n",
    "    for ff_pos, ff_neg, fh_nn_all, fh_np_all, fh_sn_all, fh_sp_all\\\n",
    "                            in zip(org_frame_out['feats_pos'], \n",
    "                               org_frame_out['feats_neg'], \n",
    "                               org_frame_results['non_neg'],\n",
    "                               org_frame_results['non_pos'],\n",
    "                               org_frame_results['should_neg'],\n",
    "                               org_frame_results['should_pos']):\n",
    "        metr2_annos = []\n",
    "        for fh_nn, fh_np, fh_sn, fh_sp\\\n",
    "                            in zip(fh_nn_all, fh_np_all, fh_sn_all, fh_sp_all):\n",
    "            fh_neg = set(ff_neg).difference(fh_nn).union(fh_sn)\n",
    "            fh_pos = set(ff_pos).difference(fh_nn).union(fh_sp)\n",
    "            \n",
    "            metr2_an = calc_metr2_fidelity(ff_pos, ff_neg, fh_pos, fh_neg)\n",
    "            metr2_annos.append(metr2_an)\n",
    "        metr2s.append(metr2_annos)\n",
    "    metr2s = np.array(metr2s)\n",
    "    metr2s_average = metr2s.mean(axis=1).mean(axis=0)\n",
    "    return metr2s_average, metr2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354db9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metr3_complexity(N_c):\n",
    "    if N_c == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/np.log(N_c)\n",
    "\n",
    "def metr3_wrapper(org_frame_out, org_frame_results):\n",
    "    metr3s = []\n",
    "    N_Chunks = org_frame_out['N_Chunks']\n",
    "    for N_c in N_Chunks:\n",
    "        metr3 = calc_metr3_complexity(N_c)\n",
    "        metr3s.append(metr3)\n",
    "    metr3s_average = np.mean(metr3s)\n",
    "#     print(metr3s_average, metr3s)\n",
    "    return metr3s_average, metr3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26997907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_m1m2m3(org_frame_out, org_frame_results):\n",
    "    m1, metric1s = metr1_wrapper(org_frame_out, org_frame_results)\n",
    "    m2, metric2s = metr2_wrapper(org_frame_out, org_frame_results)\n",
    "    m3, metric3s = metr3_wrapper(org_frame_out, org_frame_results)\n",
    "    return m1, m2, m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f4e5244",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def score_wrapper(frame_names=['deep_lift', 'guided_backprop', 'input_x_gradients', 'integrated_gradients', 'kernel_shap', 'lime']):\n",
    "    avg_aos = []\n",
    "    m1s, m2s, m3s = [], [], []\n",
    "    for frame_name in frame_names:\n",
    "        print(f'Framework processed: {frame_name}')\n",
    "        frame_out, frame_results = load_out_and_results_files(frame_name)\n",
    "        org_frame_results, avg_ao = get_organized_frame_res(frame_results)\n",
    "        avg_aos.append(avg_ao)\n",
    "        org_frame_out = get_organized_frame_out(frame_out)    \n",
    "        m1, m2, m3 = calc_m1m2m3(org_frame_out, org_frame_results)\n",
    "        m1s.append(m1)\n",
    "        m2s.append(m2)\n",
    "        m3s.append(m3)\n",
    "        print(f'm1: {m1:4f}, m2: {m2:4f}, m3: {m3:4f}')\n",
    "    print(f'average average agreement {np.mean(avg_aos):.2f}')\n",
    "    return frame_names, m1s, m2s, m3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8436936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework processed: deep_lift\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'deep_lift_out.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_286141/159656398.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mframe_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm3s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deep_lift'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'guided_backprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_x_gradients'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'integrated_gradients'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kernel_shap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_286141/4236761539.py\u001b[0m in \u001b[0;36mscore_wrapper\u001b[0;34m(frame_names)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mframe_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Framework processed: {frame_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mframe_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_out_and_results_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0morg_frame_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_ao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_organized_frame_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mavg_aos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_ao\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_286141/1076930125.py\u001b[0m in \u001b[0;36mload_out_and_results_files\u001b[0;34m(frame_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mframe_res_name\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34mf'{frame_name}_results.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mframe_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{frame_out_name}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mframe_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mframe_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{frame_res_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'deep_lift_out.pkl'"
     ]
    }
   ],
   "source": [
    "frame_names, m1s, m2s, m3s = score_wrapper(['deep_lift', 'guided_backprop', 'input_x_gradients', 'integrated_gradients', 'kernel_shap', 'lime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alpha_combos():\n",
    "    import itertools\n",
    "    alphas = np.arange(0, 11, step=1, dtype=np.uint8)\n",
    "    all_combos = [(a/10,b/10,c/10) for (a,b,c) in itertools.product(alphas, alphas, alphas) if np.sum([a,b,c]) == 10]\n",
    "    return all_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1646be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_combos = generate_alpha_combos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_alphas(frame_names, m1s, m2s, m3s, alpha_combos):\n",
    "    score_dict = {}\n",
    "    for f, m1, m2, m3 in zip(frame_names, m1s, m2s, m3s):\n",
    "        score_dict[f] = []\n",
    "        for combo in alpha_combos:\n",
    "            a1, a2, a3 = combo[0], combo[1], combo[2]\n",
    "            score = a1 * m1 + a2 * m2 + a3 * m3\n",
    "            score_dict[f].append([score, a1, a2, a3])\n",
    "    return score_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = apply_alphas(frame_names, m1s, m2s, m3s, alpha_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a327a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_scores(score_dict):\n",
    "    for f, (score, m1, m2, m3) in score_dict.items():\n",
    "        x = np.linspace(-1, 1, 50)\n",
    "        print(x)\n",
    "        y = 2*x + 1\n",
    "\n",
    "        plt.plot(x, y)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4c7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
